{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "711a631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\tensorflow_hub\\__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n",
      "c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "Num GPUs Available:  1\n",
      "GPUs disponibles : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Version TF : 2.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, SimpleRNN, Dense, LSTM, Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim.downloader as gensim_downloader\n",
    "import gensim\n",
    "import multiprocessing\n",
    "from mlflow import MlflowClient\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from Source.preprocess_data import *  ## import all functions from preprocess_data.py\n",
    "from Source.postprocess_data import * ## import all functions from postprocess_data.py\n",
    "from Source.utils import *  ## import all functions from utils.py\n",
    "import nltk\n",
    "import optuna\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import TweetTokenizer, WordPunctTokenizer, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, LancasterStemmer, SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "nw = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "\n",
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")\n",
    "os.environ[\"TF_KERAS\"]='1'\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPUs disponibles :\", tf.config.list_physical_devices(\"GPU\"))\n",
    "print(\"Version TF :\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7e458c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 160000 rows\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/AI+Engineer/Project+7%C2%A0-+D%C3%A9tectez+les+Bad+Buzz+gr%C3%A2ce+au+Deep+Learning/sentiment140.zip',\n",
    "                header=None,\n",
    "                compression='zip',\n",
    "                encoding='cp1252')\n",
    "\n",
    "df.columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "sample_df, _ = train_test_split(df, test_size=0.9, random_state=42, stratify=df['target'])\n",
    "sample_df = sample_df.reset_index(drop=True)\n",
    "print(f\"Sample size: {sample_df.shape[0]} rows\")\n",
    "# On ne garde que les colonnes 'target' et 'text'\n",
    "sample_df = sample_df[['target', 'text']]\n",
    "sample_df[\"target\"] = sample_df[\"target\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "sample_df.to_csv('Data/raw_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd610a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X_raw = sample_df['text']\n",
    "y = sample_df['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_raw, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72b03b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_list()\n",
    "X_val = X_val.to_list()\n",
    "y_train = y_train.to_list()\n",
    "y_val = y_val.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339133d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2dd8378",
   "metadata": {},
   "source": [
    "# Universal sentence encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "292657e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MLflow experiment...\n"
     ]
    }
   ],
   "source": [
    "# Création de l'étude Optuna et optimisation\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"USE_models_experiment\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"USE_models_experiment\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Essai avec Universal sentence encoder \"\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"RNN_types-pretrained-embeddings\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6219c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices=false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e71a94ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "lr = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46bdf418",
   "metadata": {},
   "outputs": [],
   "source": [
    "use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "y_val   = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "# Pré-calcul\n",
    "# Pré-calcul embeddings sur CPU\n",
    "def use_layer_experiment(dense_size):\n",
    "     with mlflow.start_run():\n",
    "        mlflow.log_params(params={\n",
    "            'num_words':1,               \n",
    "            'max_len': 1,\n",
    "            'min_count': 1,\n",
    "            'stemmer': 'None', \n",
    "            'latent_dim': 'None', \n",
    "            'rnn_size': 'None', \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr,\n",
    "            'embedding_name':'None',\n",
    "            'rnn_layer_name':'USE embedding' \n",
    "        })\n",
    "        model_savepath = \"./Models/USE_model_exp.h5\"\n",
    "\n",
    "\n",
    "        with tf.device(\"/CPU:0\"):\n",
    "            X_train_emb = use(X_train)\n",
    "            X_val_emb = use(X_val)\n",
    "\n",
    "        \n",
    "\n",
    "    # Classifieur simple\n",
    "        model = Sequential([\n",
    "            layers.Input(shape=(512,)),\n",
    "            layers.Dense(dense_size, activation='relu', kernel_regularizer=regularizers.L2(l2=1e-4)),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(dense_size, activation='relu', kernel_regularizer=regularizers.L2(l2=1e-4)),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "\n",
    "        ## Callbacks\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "        ## Compilation\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "        #Entrainement\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(X_train_emb, y_train, epochs=epochs, batch_size=64, validation_data=(X_val_emb,y_val), callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "                # Prédictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(X_val_emb)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "        \n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des métriques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8164dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with USE layer\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5043 - accuracy: 0.7734 - val_loss: 0.4720 - val_accuracy: 0.7862 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4794 - accuracy: 0.7864 - val_loss: 0.4670 - val_accuracy: 0.7888 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4746 - accuracy: 0.7893 - val_loss: 0.4648 - val_accuracy: 0.7911 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4718 - accuracy: 0.7916 - val_loss: 0.4647 - val_accuracy: 0.7914 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4708 - accuracy: 0.7926 - val_loss: 0.4648 - val_accuracy: 0.7926 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4698 - accuracy: 0.7936 - val_loss: 0.4649 - val_accuracy: 0.7922 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4585 - accuracy: 0.8019 - val_loss: 0.4592 - val_accuracy: 0.7961 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4546 - accuracy: 0.8031 - val_loss: 0.4593 - val_accuracy: 0.7952 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4508 - accuracy: 0.8056 - val_loss: 0.4590 - val_accuracy: 0.7953 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4491 - accuracy: 0.8056 - val_loss: 0.4583 - val_accuracy: 0.7973 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4486 - accuracy: 0.8063 - val_loss: 0.4577 - val_accuracy: 0.7968 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4461 - accuracy: 0.8088 - val_loss: 0.4592 - val_accuracy: 0.7947 - lr: 5.0000e-04\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4445 - accuracy: 0.8085 - val_loss: 0.4591 - val_accuracy: 0.7966 - lr: 5.0000e-04\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4354 - accuracy: 0.8148 - val_loss: 0.4587 - val_accuracy: 0.7961 - lr: 2.5000e-04\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4308 - accuracy: 0.8176 - val_loss: 0.4590 - val_accuracy: 0.7966 - lr: 2.5000e-04\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4252 - accuracy: 0.8214 - val_loss: 0.4590 - val_accuracy: 0.7968 - lr: 1.2500e-04\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4212 - accuracy: 0.8230 - val_loss: 0.4595 - val_accuracy: 0.7958 - lr: 1.2500e-04\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4193 - accuracy: 0.8253 - val_loss: 0.4589 - val_accuracy: 0.7956 - lr: 6.2500e-05\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4170 - accuracy: 0.8275 - val_loss: 0.4589 - val_accuracy: 0.7963 - lr: 6.2500e-05\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4165 - accuracy: 0.8267 - val_loss: 0.4591 - val_accuracy: 0.7962 - lr: 3.1250e-05\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4132 - accuracy: 0.8291 - val_loss: 0.4594 - val_accuracy: 0.7957 - lr: 3.1250e-05\n",
      "1000/1000 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 16:03:51 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/22 16:03:51 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp0bq86xxk\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp0bq86xxk\\model\\data\\model\\assets\n",
      "2025/09/22 16:04:00 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running test with USE layer\")\n",
    "use_layer_experiment(dense_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e13ab8",
   "metadata": {},
   "source": [
    "# Embedding MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c288b712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([128000, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name, from_pt=True)\n",
    "\n",
    "def get_embeddings(texts, batch_size=128, max_length=128):\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        tokens = tokenizer(batch_texts, padding=True, truncation=True,\n",
    "                           max_length=max_length, return_tensors=\"tf\")\n",
    "        outputs = model(**tokens)\n",
    "\n",
    "        \n",
    "        mean_embed = tf.reduce_mean(outputs.last_hidden_state, axis=1)  # pooling moyen\n",
    "        # print(mean_embed.shape)\n",
    "        max_embed = tf.reduce_max(outputs.last_hidden_state, axis=1) # pooling max\n",
    "        # print(max_embed.shape)\n",
    "        batch_emb = tf.concat([mean_embed, max_embed], axis=1)\n",
    "        # print(batch_emb.shape)\n",
    "\n",
    "        all_embeddings.append(batch_emb)\n",
    "        # print(i)\n",
    "    return tf.concat(all_embeddings, axis=0)\n",
    "\n",
    "with tf.device(\"/CPU:0\"):\n",
    "            X_train_emb = get_embeddings(X_train) \n",
    "            X_val_emb = get_embeddings(X_val)\n",
    "\n",
    "\n",
    "X_train_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eab877ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = np.array(y_train).astype(\"float32\")\n",
    "y_val   = np.array(y_val).astype(\"float32\")\n",
    "lr = 1e-4\n",
    "epochs = 200\n",
    "def minilm_layer_experiment(dense_size=128):\n",
    "     with mlflow.start_run():\n",
    "        mlflow.log_params(params={\n",
    "            'num_words':1,               \n",
    "            'max_len': 1,\n",
    "            'min_count': 1,\n",
    "            'stemmer': 'None', \n",
    "            'latent_dim': 'None', \n",
    "            'rnn_size': 'None', \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr,\n",
    "            'embedding_name':'None',\n",
    "            'rnn_layer_name':'MiniLM embedding' \n",
    "        })\n",
    "        model_savepath = \"./Models/MiniLM_model_exp.h5\"\n",
    "\n",
    "        # Charger le modèle DistilBERT optimisé pour embeddings\n",
    "        # model_name = \"sentence-transformers/distilbert-base-nli-stsb-mean-tokens\"\n",
    "        # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # model = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # def get_embeddings(texts, batch_size=32, max_length=128):\n",
    "        #     all_embeddings = []\n",
    "        #     for i in range(0, len(texts), batch_size):\n",
    "        #         batch_texts = texts[i:i+batch_size]\n",
    "        #         tokens = tokenizer(batch_texts, padding=True, truncation=True,\n",
    "        #                    max_length=max_length, return_tensors=\"tf\")\n",
    "        #         outputs = model(**tokens)\n",
    "        #         mean_embed = tf.reduce_mean(outputs.last_hidden_state, axis=1)  # pooling moyen\n",
    "        #         max_embed = tf.reduce_max(outputs.last_hidden_state, axis=1) # pooling max\n",
    "        #         batch_emb = tf.concat([mean_embed, max_embed], axis=1)\n",
    "        #         all_embeddings.append(batch_emb)\n",
    "\n",
    "        #     return tf.concat(all_embeddings, axis=0)\n",
    "\n",
    "        # with tf.device(\"/CPU:0\"):\n",
    "        #     X_train_emb = get_embeddings(X_train)  \n",
    "        #     X_val_emb = get_embeddings(X_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        model = Sequential([\n",
    "            layers.Input(shape=(768,)),\n",
    "            layers.Dense(dense_size*2, activation='relu', kernel_regularizer=regularizers.L2(l2=1e-3)),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(dense_size, activation='relu', kernel_regularizer=regularizers.L2(l2=1e-3) ),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        ## Callbacks\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=True, mode='max')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=50)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "        ## Compilation\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "        #Entrainement\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(X_train_emb, y_train, epochs=epochs, batch_size=64, validation_data=(X_val_emb,y_val), callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "                # Prédictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(X_val_emb)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "        \n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des métriques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.tensorflow.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d58eae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with MiniLM layer\n",
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.9217 - accuracy: 0.7046 - val_loss: 0.7439 - val_accuracy: 0.7577 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.7012 - accuracy: 0.7514 - val_loss: 0.6380 - val_accuracy: 0.7636 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6208 - accuracy: 0.7590 - val_loss: 0.5830 - val_accuracy: 0.7676 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.5800 - accuracy: 0.7622 - val_loss: 0.5567 - val_accuracy: 0.7693 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.5571 - accuracy: 0.7653 - val_loss: 0.5397 - val_accuracy: 0.7713 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.5415 - accuracy: 0.7685 - val_loss: 0.5288 - val_accuracy: 0.7711 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.5314 - accuracy: 0.7712 - val_loss: 0.5197 - val_accuracy: 0.7750 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5235 - accuracy: 0.7708 - val_loss: 0.5142 - val_accuracy: 0.7738 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.5159 - accuracy: 0.7739 - val_loss: 0.5076 - val_accuracy: 0.7762 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5111 - accuracy: 0.7746 - val_loss: 0.5030 - val_accuracy: 0.7775 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.5068 - accuracy: 0.7765 - val_loss: 0.5017 - val_accuracy: 0.7748 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.5029 - accuracy: 0.7777 - val_loss: 0.5018 - val_accuracy: 0.7756 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4976 - accuracy: 0.7794 - val_loss: 0.4974 - val_accuracy: 0.7771 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4958 - accuracy: 0.7805 - val_loss: 0.4938 - val_accuracy: 0.7778 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4935 - accuracy: 0.7816 - val_loss: 0.4913 - val_accuracy: 0.7800 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4910 - accuracy: 0.7833 - val_loss: 0.4897 - val_accuracy: 0.7798 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4884 - accuracy: 0.7835 - val_loss: 0.4903 - val_accuracy: 0.7796 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4862 - accuracy: 0.7839 - val_loss: 0.4881 - val_accuracy: 0.7814 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4847 - accuracy: 0.7850 - val_loss: 0.4890 - val_accuracy: 0.7788 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4838 - accuracy: 0.7852 - val_loss: 0.4834 - val_accuracy: 0.7837 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4814 - accuracy: 0.7867 - val_loss: 0.4868 - val_accuracy: 0.7805 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4795 - accuracy: 0.7871 - val_loss: 0.4880 - val_accuracy: 0.7797 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4785 - accuracy: 0.7880 - val_loss: 0.4859 - val_accuracy: 0.7805 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4786 - accuracy: 0.7895 - val_loss: 0.4864 - val_accuracy: 0.7800 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4759 - accuracy: 0.7895 - val_loss: 0.4826 - val_accuracy: 0.7820 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 9s 4ms/step - loss: 0.4754 - accuracy: 0.7892 - val_loss: 0.4808 - val_accuracy: 0.7821 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4742 - accuracy: 0.7897 - val_loss: 0.4817 - val_accuracy: 0.7835 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4738 - accuracy: 0.7902 - val_loss: 0.4818 - val_accuracy: 0.7827 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4732 - accuracy: 0.7908 - val_loss: 0.4820 - val_accuracy: 0.7837 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4715 - accuracy: 0.7913 - val_loss: 0.4836 - val_accuracy: 0.7819 - lr: 1.0000e-04\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4718 - accuracy: 0.7922 - val_loss: 0.4798 - val_accuracy: 0.7847 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4712 - accuracy: 0.7908 - val_loss: 0.4806 - val_accuracy: 0.7859 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4704 - accuracy: 0.7925 - val_loss: 0.4810 - val_accuracy: 0.7835 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4692 - accuracy: 0.7943 - val_loss: 0.4789 - val_accuracy: 0.7827 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4680 - accuracy: 0.7941 - val_loss: 0.4826 - val_accuracy: 0.7793 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4687 - accuracy: 0.7926 - val_loss: 0.4796 - val_accuracy: 0.7843 - lr: 1.0000e-04\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4674 - accuracy: 0.7937 - val_loss: 0.4814 - val_accuracy: 0.7822 - lr: 1.0000e-04\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4674 - accuracy: 0.7939 - val_loss: 0.4793 - val_accuracy: 0.7865 - lr: 1.0000e-04\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4671 - accuracy: 0.7940 - val_loss: 0.4800 - val_accuracy: 0.7852 - lr: 1.0000e-04\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4647 - accuracy: 0.7959 - val_loss: 0.4832 - val_accuracy: 0.7814 - lr: 1.0000e-04\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4666 - accuracy: 0.7951 - val_loss: 0.4805 - val_accuracy: 0.7841 - lr: 1.0000e-04\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4637 - accuracy: 0.7961 - val_loss: 0.4822 - val_accuracy: 0.7824 - lr: 1.0000e-04\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4632 - accuracy: 0.7968 - val_loss: 0.4819 - val_accuracy: 0.7843 - lr: 1.0000e-04\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4639 - accuracy: 0.7980 - val_loss: 0.4826 - val_accuracy: 0.7847 - lr: 1.0000e-04\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4630 - accuracy: 0.7969 - val_loss: 0.4772 - val_accuracy: 0.7871 - lr: 1.0000e-04\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4628 - accuracy: 0.7969 - val_loss: 0.4791 - val_accuracy: 0.7853 - lr: 1.0000e-04\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4617 - accuracy: 0.7989 - val_loss: 0.4827 - val_accuracy: 0.7815 - lr: 1.0000e-04\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4613 - accuracy: 0.7980 - val_loss: 0.4807 - val_accuracy: 0.7848 - lr: 1.0000e-04\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4609 - accuracy: 0.7978 - val_loss: 0.4801 - val_accuracy: 0.7819 - lr: 1.0000e-04\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4604 - accuracy: 0.8001 - val_loss: 0.4804 - val_accuracy: 0.7870 - lr: 1.0000e-04\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4609 - accuracy: 0.7984 - val_loss: 0.4840 - val_accuracy: 0.7842 - lr: 1.0000e-04\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4595 - accuracy: 0.7997 - val_loss: 0.4815 - val_accuracy: 0.7857 - lr: 1.0000e-04\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4586 - accuracy: 0.8002 - val_loss: 0.4822 - val_accuracy: 0.7843 - lr: 1.0000e-04\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4584 - accuracy: 0.8009 - val_loss: 0.5011 - val_accuracy: 0.7726 - lr: 1.0000e-04\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4585 - accuracy: 0.8012 - val_loss: 0.4880 - val_accuracy: 0.7807 - lr: 1.0000e-04\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4574 - accuracy: 0.8009 - val_loss: 0.4794 - val_accuracy: 0.7852 - lr: 1.0000e-04\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4577 - accuracy: 0.8023 - val_loss: 0.4823 - val_accuracy: 0.7851 - lr: 1.0000e-04\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4580 - accuracy: 0.8013 - val_loss: 0.4857 - val_accuracy: 0.7837 - lr: 1.0000e-04\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4571 - accuracy: 0.8017 - val_loss: 0.4805 - val_accuracy: 0.7858 - lr: 1.0000e-04\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4564 - accuracy: 0.8033 - val_loss: 0.4808 - val_accuracy: 0.7839 - lr: 1.0000e-04\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4556 - accuracy: 0.8033 - val_loss: 0.4887 - val_accuracy: 0.7793 - lr: 1.0000e-04\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4561 - accuracy: 0.8018 - val_loss: 0.4871 - val_accuracy: 0.7811 - lr: 1.0000e-04\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4551 - accuracy: 0.8036 - val_loss: 0.4814 - val_accuracy: 0.7834 - lr: 1.0000e-04\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4543 - accuracy: 0.8028 - val_loss: 0.4918 - val_accuracy: 0.7765 - lr: 1.0000e-04\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4555 - accuracy: 0.8039 - val_loss: 0.4965 - val_accuracy: 0.7737 - lr: 1.0000e-04\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4404 - accuracy: 0.8139 - val_loss: 0.4843 - val_accuracy: 0.7845 - lr: 5.0000e-05\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4371 - accuracy: 0.8162 - val_loss: 0.4812 - val_accuracy: 0.7865 - lr: 5.0000e-05\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4357 - accuracy: 0.8157 - val_loss: 0.4823 - val_accuracy: 0.7857 - lr: 5.0000e-05\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4332 - accuracy: 0.8175 - val_loss: 0.4859 - val_accuracy: 0.7822 - lr: 5.0000e-05\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4331 - accuracy: 0.8179 - val_loss: 0.4846 - val_accuracy: 0.7852 - lr: 5.0000e-05\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4314 - accuracy: 0.8198 - val_loss: 0.4876 - val_accuracy: 0.7832 - lr: 5.0000e-05\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4310 - accuracy: 0.8194 - val_loss: 0.4839 - val_accuracy: 0.7842 - lr: 5.0000e-05\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4320 - accuracy: 0.8193 - val_loss: 0.4836 - val_accuracy: 0.7839 - lr: 5.0000e-05\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4294 - accuracy: 0.8205 - val_loss: 0.4830 - val_accuracy: 0.7842 - lr: 5.0000e-05\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4291 - accuracy: 0.8204 - val_loss: 0.4836 - val_accuracy: 0.7834 - lr: 5.0000e-05\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4274 - accuracy: 0.8211 - val_loss: 0.4842 - val_accuracy: 0.7836 - lr: 5.0000e-05\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4277 - accuracy: 0.8214 - val_loss: 0.4835 - val_accuracy: 0.7847 - lr: 5.0000e-05\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4274 - accuracy: 0.8223 - val_loss: 0.4844 - val_accuracy: 0.7847 - lr: 5.0000e-05\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4245 - accuracy: 0.8228 - val_loss: 0.4858 - val_accuracy: 0.7842 - lr: 5.0000e-05\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4248 - accuracy: 0.8228 - val_loss: 0.4886 - val_accuracy: 0.7817 - lr: 5.0000e-05\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4251 - accuracy: 0.8238 - val_loss: 0.4877 - val_accuracy: 0.7843 - lr: 5.0000e-05\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4240 - accuracy: 0.8245 - val_loss: 0.4864 - val_accuracy: 0.7838 - lr: 5.0000e-05\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4229 - accuracy: 0.8239 - val_loss: 0.4854 - val_accuracy: 0.7843 - lr: 5.0000e-05\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4226 - accuracy: 0.8248 - val_loss: 0.4872 - val_accuracy: 0.7822 - lr: 5.0000e-05\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4226 - accuracy: 0.8232 - val_loss: 0.4859 - val_accuracy: 0.7854 - lr: 5.0000e-05\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4127 - accuracy: 0.8315 - val_loss: 0.4887 - val_accuracy: 0.7831 - lr: 2.5000e-05\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4102 - accuracy: 0.8314 - val_loss: 0.4862 - val_accuracy: 0.7842 - lr: 2.5000e-05\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4108 - accuracy: 0.8321 - val_loss: 0.4879 - val_accuracy: 0.7832 - lr: 2.5000e-05\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4093 - accuracy: 0.8333 - val_loss: 0.4890 - val_accuracy: 0.7835 - lr: 2.5000e-05\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4085 - accuracy: 0.8321 - val_loss: 0.4913 - val_accuracy: 0.7841 - lr: 2.5000e-05\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4075 - accuracy: 0.8331 - val_loss: 0.4891 - val_accuracy: 0.7817 - lr: 2.5000e-05\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4066 - accuracy: 0.8342 - val_loss: 0.4899 - val_accuracy: 0.7838 - lr: 2.5000e-05\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4066 - accuracy: 0.8342 - val_loss: 0.4898 - val_accuracy: 0.7848 - lr: 2.5000e-05\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4065 - accuracy: 0.8354 - val_loss: 0.4928 - val_accuracy: 0.7835 - lr: 2.5000e-05\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4044 - accuracy: 0.8358 - val_loss: 0.4895 - val_accuracy: 0.7845 - lr: 2.5000e-05\n",
      "1000/1000 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 18:05:43 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/22 18:05:43 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpa95f3lnk\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpa95f3lnk\\model\\data\\model\\assets\n",
      "2025/09/22 18:05:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running test with MiniLM layer\")\n",
    "minilm_layer_experiment(dense_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ab751",
   "metadata": {},
   "source": [
    "# Utilisation de DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fe5b279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bassm\\.cache\\huggingface\\hub\\models--sentence-transformers--distilbert-base-nli-stsb-mean-tokens. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFDistilBertModel.\n",
      "\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([128000, 1536])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Charger le modèle DistilBERT optimisé pour embeddings\n",
    "model_name = \"sentence-transformers/distilbert-base-nli-stsb-mean-tokens\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name, from_pt=True)\n",
    "\n",
    "\n",
    "with tf.device(\"/CPU:0\"):\n",
    "            X_train_emb = get_embeddings(X_train) \n",
    "            X_val_emb = get_embeddings(X_val)\n",
    "\n",
    "\n",
    "X_train_emb.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9dee886",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb2 = X_train_emb \n",
    "X_val_emb2 = X_val_emb\n",
    "y_train = np.array(y_train).astype(\"float32\")\n",
    "y_val   = np.array(y_val).astype(\"float32\")\n",
    "lr = 1e-4\n",
    "epochs = 200\n",
    "def db_nli_stsb_layer_experiment(dense_size=128):\n",
    "     with mlflow.start_run():\n",
    "        mlflow.log_params(params={\n",
    "            'num_words':1,               \n",
    "            'max_len': 1,\n",
    "            'min_count': 1,\n",
    "            'stemmer': 'None', \n",
    "            'latent_dim': 'None', \n",
    "            'rnn_size': 'None', \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr,\n",
    "            'embedding_name':'None',\n",
    "            'rnn_layer_name':'DistilBERT-nli-stsb' \n",
    "        })\n",
    "        model_savepath = \"./Models/DB-NLI-STSB_model_exp.h5\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        model = Sequential([\n",
    "            layers.Input(shape=(1536,)),\n",
    "            layers.Dense(dense_size*2, activation='relu', kernel_regularizer=regularizers.L2(l2=1e-3)),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(dense_size, activation='relu', kernel_regularizer=regularizers.L2(l2=1e-3) ),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        ## Callbacks\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=True, mode='max')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=50)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "        ## Compilation\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "        #Entrainement\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(X_train_emb, y_train, epochs=epochs, batch_size=64, validation_data=(X_val_emb,y_val), callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "                # Prédictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(X_val_emb)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "        \n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des métriques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.tensorflow.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2e83c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with MiniLM layer\n",
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.9854 - accuracy: 0.7566 - val_loss: 0.8217 - val_accuracy: 0.7834 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.7406 - accuracy: 0.7787 - val_loss: 0.6548 - val_accuracy: 0.7859 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.6166 - accuracy: 0.7859 - val_loss: 0.5726 - val_accuracy: 0.7899 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5574 - accuracy: 0.7891 - val_loss: 0.5316 - val_accuracy: 0.7918 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5255 - accuracy: 0.7932 - val_loss: 0.5104 - val_accuracy: 0.7935 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5071 - accuracy: 0.7946 - val_loss: 0.5052 - val_accuracy: 0.7886 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4932 - accuracy: 0.7965 - val_loss: 0.4852 - val_accuracy: 0.7955 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4838 - accuracy: 0.7971 - val_loss: 0.4776 - val_accuracy: 0.7954 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4769 - accuracy: 0.7993 - val_loss: 0.4754 - val_accuracy: 0.7957 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4713 - accuracy: 0.8002 - val_loss: 0.4713 - val_accuracy: 0.7947 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4668 - accuracy: 0.8000 - val_loss: 0.4649 - val_accuracy: 0.7985 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4628 - accuracy: 0.8022 - val_loss: 0.4632 - val_accuracy: 0.7981 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4601 - accuracy: 0.8018 - val_loss: 0.4618 - val_accuracy: 0.7973 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4590 - accuracy: 0.8018 - val_loss: 0.4613 - val_accuracy: 0.7969 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4570 - accuracy: 0.8023 - val_loss: 0.4591 - val_accuracy: 0.8001 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4553 - accuracy: 0.8034 - val_loss: 0.4574 - val_accuracy: 0.7989 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4531 - accuracy: 0.8051 - val_loss: 0.4573 - val_accuracy: 0.7995 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4525 - accuracy: 0.8037 - val_loss: 0.4565 - val_accuracy: 0.8005 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4516 - accuracy: 0.8056 - val_loss: 0.4574 - val_accuracy: 0.7991 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4505 - accuracy: 0.8055 - val_loss: 0.4589 - val_accuracy: 0.7958 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4495 - accuracy: 0.8056 - val_loss: 0.4626 - val_accuracy: 0.7939 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4488 - accuracy: 0.8053 - val_loss: 0.4589 - val_accuracy: 0.7991 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4481 - accuracy: 0.8072 - val_loss: 0.4568 - val_accuracy: 0.7981 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4468 - accuracy: 0.8076 - val_loss: 0.4570 - val_accuracy: 0.7982 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4476 - accuracy: 0.8076 - val_loss: 0.4584 - val_accuracy: 0.7971 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4462 - accuracy: 0.8073 - val_loss: 0.4569 - val_accuracy: 0.7981 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4453 - accuracy: 0.8077 - val_loss: 0.4554 - val_accuracy: 0.7995 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4458 - accuracy: 0.8083 - val_loss: 0.4564 - val_accuracy: 0.7978 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4445 - accuracy: 0.8092 - val_loss: 0.4549 - val_accuracy: 0.8002 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4452 - accuracy: 0.8100 - val_loss: 0.4563 - val_accuracy: 0.7985 - lr: 1.0000e-04\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4445 - accuracy: 0.8085 - val_loss: 0.4532 - val_accuracy: 0.8014 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4436 - accuracy: 0.8101 - val_loss: 0.4549 - val_accuracy: 0.7986 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4441 - accuracy: 0.8103 - val_loss: 0.4574 - val_accuracy: 0.8008 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4419 - accuracy: 0.8102 - val_loss: 0.4560 - val_accuracy: 0.7987 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4421 - accuracy: 0.8114 - val_loss: 0.4533 - val_accuracy: 0.8021 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4417 - accuracy: 0.8108 - val_loss: 0.4546 - val_accuracy: 0.8004 - lr: 1.0000e-04\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4415 - accuracy: 0.8113 - val_loss: 0.4555 - val_accuracy: 0.8029 - lr: 1.0000e-04\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4418 - accuracy: 0.8122 - val_loss: 0.4562 - val_accuracy: 0.7997 - lr: 1.0000e-04\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4408 - accuracy: 0.8126 - val_loss: 0.4570 - val_accuracy: 0.8002 - lr: 1.0000e-04\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4411 - accuracy: 0.8118 - val_loss: 0.4575 - val_accuracy: 0.8006 - lr: 1.0000e-04\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4409 - accuracy: 0.8113 - val_loss: 0.4559 - val_accuracy: 0.8006 - lr: 1.0000e-04\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4395 - accuracy: 0.8127 - val_loss: 0.4550 - val_accuracy: 0.8002 - lr: 1.0000e-04\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4404 - accuracy: 0.8120 - val_loss: 0.4573 - val_accuracy: 0.8018 - lr: 1.0000e-04\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4400 - accuracy: 0.8122 - val_loss: 0.4576 - val_accuracy: 0.8016 - lr: 1.0000e-04\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 9s 4ms/step - loss: 0.4400 - accuracy: 0.8127 - val_loss: 0.4611 - val_accuracy: 0.8002 - lr: 1.0000e-04\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4394 - accuracy: 0.8134 - val_loss: 0.4728 - val_accuracy: 0.7892 - lr: 1.0000e-04\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 9s 4ms/step - loss: 0.4392 - accuracy: 0.8126 - val_loss: 0.4582 - val_accuracy: 0.8019 - lr: 1.0000e-04\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4381 - accuracy: 0.8142 - val_loss: 0.4578 - val_accuracy: 0.7993 - lr: 1.0000e-04\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4386 - accuracy: 0.8140 - val_loss: 0.4585 - val_accuracy: 0.8002 - lr: 1.0000e-04\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4386 - accuracy: 0.8146 - val_loss: 0.4578 - val_accuracy: 0.7994 - lr: 1.0000e-04\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4373 - accuracy: 0.8153 - val_loss: 0.4598 - val_accuracy: 0.8017 - lr: 1.0000e-04\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4255 - accuracy: 0.8223 - val_loss: 0.4569 - val_accuracy: 0.7997 - lr: 5.0000e-05\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4211 - accuracy: 0.8253 - val_loss: 0.4555 - val_accuracy: 0.8019 - lr: 5.0000e-05\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4174 - accuracy: 0.8271 - val_loss: 0.4576 - val_accuracy: 0.8019 - lr: 5.0000e-05\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4172 - accuracy: 0.8266 - val_loss: 0.4545 - val_accuracy: 0.8018 - lr: 5.0000e-05\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4145 - accuracy: 0.8277 - val_loss: 0.4607 - val_accuracy: 0.8007 - lr: 5.0000e-05\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4116 - accuracy: 0.8296 - val_loss: 0.4572 - val_accuracy: 0.8013 - lr: 5.0000e-05\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4107 - accuracy: 0.8302 - val_loss: 0.4588 - val_accuracy: 0.8016 - lr: 5.0000e-05\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4103 - accuracy: 0.8307 - val_loss: 0.4557 - val_accuracy: 0.8032 - lr: 5.0000e-05\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4090 - accuracy: 0.8315 - val_loss: 0.4565 - val_accuracy: 0.8017 - lr: 5.0000e-05\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4070 - accuracy: 0.8325 - val_loss: 0.4587 - val_accuracy: 0.8017 - lr: 5.0000e-05\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4069 - accuracy: 0.8325 - val_loss: 0.4574 - val_accuracy: 0.8027 - lr: 5.0000e-05\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4059 - accuracy: 0.8328 - val_loss: 0.4593 - val_accuracy: 0.8009 - lr: 5.0000e-05\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4052 - accuracy: 0.8342 - val_loss: 0.4631 - val_accuracy: 0.8011 - lr: 5.0000e-05\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4041 - accuracy: 0.8341 - val_loss: 0.4616 - val_accuracy: 0.8009 - lr: 5.0000e-05\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4036 - accuracy: 0.8353 - val_loss: 0.4603 - val_accuracy: 0.8012 - lr: 5.0000e-05\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4018 - accuracy: 0.8357 - val_loss: 0.4611 - val_accuracy: 0.7993 - lr: 5.0000e-05\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4006 - accuracy: 0.8371 - val_loss: 0.4601 - val_accuracy: 0.8006 - lr: 5.0000e-05\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4012 - accuracy: 0.8360 - val_loss: 0.4610 - val_accuracy: 0.8019 - lr: 5.0000e-05\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4009 - accuracy: 0.8371 - val_loss: 0.4595 - val_accuracy: 0.8007 - lr: 5.0000e-05\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.3979 - accuracy: 0.8385 - val_loss: 0.4645 - val_accuracy: 0.8005 - lr: 5.0000e-05\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.3882 - accuracy: 0.8434 - val_loss: 0.4623 - val_accuracy: 0.8006 - lr: 2.5000e-05\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.3845 - accuracy: 0.8458 - val_loss: 0.4657 - val_accuracy: 0.8013 - lr: 2.5000e-05\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.3818 - accuracy: 0.8477 - val_loss: 0.4672 - val_accuracy: 0.8016 - lr: 2.5000e-05\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.3819 - accuracy: 0.8482 - val_loss: 0.4655 - val_accuracy: 0.8020 - lr: 2.5000e-05\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.3795 - accuracy: 0.8499 - val_loss: 0.4639 - val_accuracy: 0.8008 - lr: 2.5000e-05\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.3781 - accuracy: 0.8498 - val_loss: 0.4681 - val_accuracy: 0.8023 - lr: 2.5000e-05\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.3777 - accuracy: 0.8501 - val_loss: 0.4683 - val_accuracy: 0.8001 - lr: 2.5000e-05\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.3763 - accuracy: 0.8521 - val_loss: 0.4723 - val_accuracy: 0.7995 - lr: 2.5000e-05\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.3752 - accuracy: 0.8520 - val_loss: 0.4693 - val_accuracy: 0.8012 - lr: 2.5000e-05\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.3746 - accuracy: 0.8525 - val_loss: 0.4684 - val_accuracy: 0.8012 - lr: 2.5000e-05\n",
      "1000/1000 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 20:21:06 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/22 20:21:06 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmphsrv39wf\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmphsrv39wf\\model\\data\\model\\assets\n",
      "2025/09/22 20:21:16 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running test with MiniLM layer\")\n",
    "db_nli_stsb_layer_experiment(dense_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68175e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env_P7_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
