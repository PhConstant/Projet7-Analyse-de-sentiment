{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c17c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\tensorflow_hub\\__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n",
      "c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "Num GPUs Available:  1\n",
      "GPUs disponibles : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Version TF : 2.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, SimpleRNN, Dense, LSTM, Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim.downloader as gensim_downloader\n",
    "import gensim\n",
    "import multiprocessing\n",
    "from mlflow import MlflowClient\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from Source.preprocess_data import *  ## import all functions from preprocess_data.py\n",
    "from Source.postprocess_data import * ## import all functions from postprocess_data.py\n",
    "from Source.utils import *  ## import all functions from utils.py\n",
    "import nltk\n",
    "import optuna\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import TweetTokenizer, WordPunctTokenizer, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, LancasterStemmer, SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "nw = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "\n",
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")\n",
    "os.environ[\"TF_KERAS\"]='1'\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPUs disponibles :\", tf.config.list_physical_devices(\"GPU\"))\n",
    "print(\"Version TF :\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "969e2c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 160000 rows\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/AI+Engineer/Project+7%C2%A0-+D%C3%A9tectez+les+Bad+Buzz+gr%C3%A2ce+au+Deep+Learning/sentiment140.zip',\n",
    "                header=None,\n",
    "                compression='zip',\n",
    "                encoding='cp1252')\n",
    "\n",
    "df.columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "sample_df, _ = train_test_split(df, test_size=0.9, random_state=42, stratify=df['target'])\n",
    "sample_df = sample_df.reset_index(drop=True)\n",
    "print(f\"Sample size: {sample_df.shape[0]} rows\")\n",
    "# On ne garde que les colonnes 'target' et 'text'\n",
    "sample_df = sample_df[['target', 'text']]\n",
    "sample_df[\"target\"] = sample_df[\"target\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "sample_df.to_csv('Data/raw_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2583e99",
   "metadata": {},
   "source": [
    "# Séparation train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8a5c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X_raw = sample_df['text']\n",
    "y = sample_df['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_raw, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5697acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f95011c",
   "metadata": {},
   "source": [
    "# Préparation de l'experience de base (baseline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fbc2cd",
   "metadata": {},
   "source": [
    "## Pré-traitement des dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "478f1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 20000\n",
    "max_len = 10\n",
    "min_count = 3\n",
    "\n",
    "X_sentence_train, tokenizer, sentences_train = preprocess_data_embedding(X_raw=X_train, \n",
    "                                                        stem_lem_func=PorterStemmer().stem,\n",
    "                                                        tokenizer=None, \n",
    "                                                        stop_words=stopwords.words('english'), \n",
    "                                                        min_count=min_count,\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words, \n",
    "                                                        return_sentences=True) \n",
    "X_sentence_val = preprocess_data_embedding(X_raw=X_val, \n",
    "                                                        stem_lem_func=PorterStemmer().stem,\n",
    "                                                        tokenizer=tokenizer, \n",
    "                                                        stop_words=stopwords.words('english'), \n",
    "                                                        min_count=1, # mincount = 1 car on est sur le jeu de validation\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef45eb84",
   "metadata": {},
   "source": [
    "# Création d'un embedding de base custom pour notre modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12bc38bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build & train Word2Vec model ...\n",
      "Vocabulary size: 13547\n",
      "Word2Vec trained\n",
      "Coverage: 99.99%\n",
      "Mean norm: 7.0292816\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "latent_dim = 50\n",
    "print(\"Build & train Word2Vec model ...\")\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences_train, \n",
    "    vector_size=latent_dim,  # dimension de l’espace latent\n",
    "    window=5,         # taille du contexte\n",
    "    min_count=min_count,      # ignorer les mots trop rares\n",
    "    workers=4,        # parallélisme CPU\n",
    "    sg=0,              # 1 = skip-gram, 0 = CBOW\n",
    "    epochs=100\n",
    ")\n",
    "\n",
    "\n",
    "model_vectors = w2v_model.wv\n",
    "w2v_words = model_vectors.index_to_key\n",
    "print(\"Vocabulary size: %i\" % len(w2v_words))\n",
    "print(\"Word2Vec trained\")\n",
    "\n",
    "found = sum(1 for w in tokenizer.word_index if w in w2v_model.wv)\n",
    "coverage = found / len(tokenizer.word_index)\n",
    "print(f\"Coverage: {coverage*100:.2f}%\")\n",
    "\n",
    "vectors = np.array([w2v_model.wv[w] for w in tokenizer.word_index if w in w2v_model.wv])\n",
    "print(\"Mean norm:\", np.mean(np.linalg.norm(vectors, axis=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba88b04d",
   "metadata": {},
   "source": [
    "## Création de la matrice d'embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "664fbb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (13549, 50)\n",
      "Words found in pretrained embeddings: 13547/13549 (99.99%)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, vocab_size = build_embedding_matrix(tokenizer=tokenizer,\n",
    "                                          embedding_model=model_vectors, \n",
    "                                          latent_dim=latent_dim\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0441bc8",
   "metadata": {},
   "source": [
    "## Création du modèle simple avec RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec0b1894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 10, 50)            677450    \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 64)                7360      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 684,875\n",
      "Trainable params: 7,425\n",
      "Non-trainable params: 677,450\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model =  build_base_RNN(vocab_size=vocab_size, \n",
    "                        latent_dim=latent_dim,\n",
    "                        input_length=max_len, \n",
    "                        embedding_matrix=embedding_matrix,\n",
    "                        rnn_size = 64)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326f6b4a",
   "metadata": {},
   "source": [
    "## Callbacks pour l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./Models/baselineRNN.h5\", monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-5)\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "\n",
    "callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716708dd",
   "metadata": {},
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a2eabb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    history = model.fit(X_sentence_train, y_train, epochs=50, batch_size=64, validation_data=(X_sentence_val,y_val), callbacks=callbacks_list, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d23f7",
   "metadata": {},
   "source": [
    "## Post-traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29e8dacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 3s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.73446875,\n",
       " 'F1_negatif': 0.7334107238101214,\n",
       " 'F1_positif': 0.7355184113051327,\n",
       " 'Recall_negatif': 0.7305,\n",
       " 'Recall_positif': 0.7384375,\n",
       " 'Precision_negatif': 0.7363447363447363,\n",
       " 'Precision_positif': 0.7326223104111118,\n",
       " 'ROC_AUC': 0.815024501953125}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_sentence_val)\n",
    "y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d820a8",
   "metadata": {},
   "source": [
    "## Liste des hyper-paramètres à optimiser \n",
    "\n",
    "\n",
    "- Prétraitement : \n",
    "    - Stemming ou Lemmatisation\n",
    "    - Taille du vocabulaire (num_words)\n",
    "    - Nombre minimum d'occurences (min_count)\n",
    "\n",
    "- Embedding : \n",
    "    - Word2Vec/FastText/Glove (préentrainés)\n",
    "    - Word2Vec/FastText (customisés)\n",
    "    - Dimension latente de l'embedding\n",
    "- Modèle : \n",
    "    - Couche SimpleRNN ou LSTM\n",
    "    - Dimension de la couche d'entrainement\n",
    "    - Fine-tuning ou non des embeddings ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a1bea",
   "metadata": {},
   "source": [
    "# Experimentations sur les modèles de RNN classiques "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a8bd7",
   "metadata": {},
   "source": [
    "## Préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "729e9ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WordNetLemmatizer', 'PorterStemmer', 'LancasterStemmer', 'SnowballStemmer']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Stemmer_dict = {'WordNetLemmatizer': WordNetLemmatizer().lemmatize, \n",
    "                'PorterStemmer': PorterStemmer().stem, \n",
    "                'LancasterStemmer':LancasterStemmer().stem, \n",
    "                'SnowballStemmer' : SnowballStemmer(\"english\").stem\n",
    "}\n",
    "\n",
    "list(Stemmer_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c21a8",
   "metadata": {},
   "source": [
    "## Core fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbbba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction à optimiser pour optuna\n",
    "\n",
    "def embedding_eva_pre(trial):\n",
    "    # Hyperparamètres\n",
    "    ## Prétraitement\n",
    "    min_count = trial.suggest_int('min_count',1,10)\n",
    "    num_words = trial.suggest_int('num_words',5000,50000)\n",
    "    max_len   = trial.suggest_int('max_len',2,30)\n",
    "    stemmer   = trial.suggest_categorical('stemmer',list(Stemmer_dict.keys()))\n",
    "    ## Embedding\n",
    "    latent_dim = 50\n",
    "    ## Modèle\n",
    "    rnn_size = 64\n",
    "    ## Entrainement\n",
    "    epochs = 50\n",
    "    lr = 1e-3\n",
    "    ## Savepath des poids du modèle\n",
    "    model_savepath = \"./Models/baselineRNN_pre.h5\"\n",
    "\n",
    "\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_params(params={\n",
    "            'num_words':num_words,               \n",
    "            'max_len': max_len,\n",
    "            'min_count': min_count,\n",
    "            'stemmer': stemmer, \n",
    "            'latent_dim': latent_dim, \n",
    "            'rnn_size': rnn_size, \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr \n",
    "        })\n",
    "\n",
    "        # Prétraitement\n",
    "        X_sentence_train, tokenizer, sentences_train = preprocess_data_embedding(X_raw=X_train, \n",
    "                                                        stem_lem_func=Stemmer_dict[stemmer],\n",
    "                                                        tokenizer=None, \n",
    "                                                        stop_words=stopwords.words('english'), \n",
    "                                                        min_count=min_count,\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words, \n",
    "                                                        return_sentences=True) \n",
    "        X_sentence_val = preprocess_data_embedding(X_raw=X_val, \n",
    "                                                        stem_lem_func=PorterStemmer().stem,\n",
    "                                                        tokenizer=tokenizer, \n",
    "                                                        stop_words=stopwords.words('english'), \n",
    "                                                        min_count=1, # mincount = 1 car on est sur le jeu de validation\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words) \n",
    "        # Embedding(custom)\n",
    "        \n",
    "        w2v_model = Word2Vec(\n",
    "            sentences=sentences_train, \n",
    "            vector_size=latent_dim,  # dimension de l’espace latent\n",
    "            window=5,         # taille du contexte\n",
    "            min_count=min_count,      # ignorer les mots trop rares\n",
    "            workers=4,        # parallélisme CPU\n",
    "            sg=0,              # 1 = skip-gram, 0 = CBOW\n",
    "            epochs=50\n",
    "            )\n",
    "\n",
    "\n",
    "        model_vectors = w2v_model.wv\n",
    "        w2v_words = model_vectors.index_to_key\n",
    "\n",
    "        embedding_matrix, vocab_size = build_embedding_matrix(tokenizer=tokenizer,\n",
    "                                          embedding_model=model_vectors, \n",
    "                                          latent_dim=latent_dim\n",
    "                                          )\n",
    "\n",
    "        \n",
    "        # Modèle\n",
    "        model =  build_base_RNN(vocab_size=vocab_size, \n",
    "                        latent_dim=latent_dim,\n",
    "                        input_length=max_len, \n",
    "                        embedding_matrix=embedding_matrix,\n",
    "                        rnn_size = rnn_size)\n",
    "        ## Callbacks\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "        ## Compilation\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "        #Entrainement\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(X_sentence_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_sentence_val,y_val), callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "        # Prédictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(X_sentence_val)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des métriques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        acc = output_dict[\"Accuracy\"]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded01f83",
   "metadata": {},
   "source": [
    "## Définition de l'experiment MLFlow/Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cad89439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization with Optuna...\n",
      "Setting up MLflow experiment...\n"
     ]
    }
   ],
   "source": [
    "# Création de l'étude Optuna et optimisation\n",
    "print(\"Starting hyperparameter optimization with Optuna...\")\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"optuna_word_embedding_experiment_preprocessin\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"optuna_word_embedding_experiment_preprocessin\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Cette experience contient les différents tests pour le modèle RNN simple. \"\n",
    "    \"Ici on évalue simplement l'impact des différents prétraitements sur un modèle avec simpleRNN\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"simple-RNN-preprocessing\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afb2165",
   "metadata": {},
   "source": [
    "## Lancement de l'optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancement de l'optimisation avec Optuna\n",
    "print(\"Starting optimization trials...\")\n",
    "with mlflow.start_run(run_name=\"optuna_word_embedding_experiment_preprocessin\"):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(embedding_eva_pre, n_trials=50)\n",
    "\n",
    "    mlflow.log_params(study.best_params)\n",
    "    mlflow.log_metric(\"best_accuracy\", study.best_value)\n",
    "\n",
    "print(\"Optimization completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e156241",
   "metadata": {},
   "source": [
    "## Extraction meilleur modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea7a2618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run ID: d9f27d7060494334af49d01586fdb855 with metrics:\n",
      "Accuracy: 0.75290625\n",
      "F1_negatif: 0.7471135702178016\n",
      "F1_positif: 0.7584394953105429\n",
      "Precision_negatif: 0.7650487980611776\n",
      "Precision_positif: 0.7418275264447499\n",
      "Recall_negatif: 0.73\n",
      "Recall_positif: 0.7758125\n",
      "ROC_AUC: 0.83355388671875\n",
      "Best run parameters:\n",
      "epochs: 50\n",
      "latent_dim: 50\n",
      "learning_rate: 0.001\n",
      "max_len: 27\n",
      "min_count: 1\n",
      "num_words: 13006\n",
      "rnn_size: 64\n",
      "stemmer: PorterStemmer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'simple_rnn_best_pre'.\n",
      "2025/09/14 16:01:51 WARNING mlflow.tracking._model_registry.fluent: Run with id d9f27d7060494334af49d01586fdb855 has no artifacts at artifact path 'model', registering model based on models:/m-0e60288d816b42cb87f955b8ad2cd669 instead\n",
      "Created version '1' of model 'simple_rnn_best_pre'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting tag epochs = 50 in registered model\n",
      "Setting tag latent_dim = 50 in registered model\n",
      "Setting tag learning_rate = 0.001 in registered model\n",
      "Setting tag max_len = 27 in registered model\n",
      "Setting tag min_count = 1 in registered model\n",
      "Setting tag num_words = 13006 in registered model\n",
      "Setting tag rnn_size = 64 in registered model\n",
      "Setting tag stemmer = PorterStemmer in registered model\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")\n",
    "experiment_id = mlflow.get_experiment_by_name(\"optuna_word_embedding_experiment_preprocessin\").experiment_id\n",
    "runs = client.search_runs(experiment_id)\n",
    "\n",
    "# Métrique pour sélectionner le meilleur modèle\n",
    "metric_to_optimize = \"Accuracy\" # liste des métriques enregistrées dans postprocess_data.py ou sur l'UI MLflow\n",
    "best_run = max(runs, key=lambda run: run.data.metrics.get(metric_to_optimize, float('-inf')))\n",
    "print(f\"Best run ID: {best_run.info.run_id} with metrics:\")\n",
    "for key, value in best_run.data.metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Best run parameters:\")\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Enregistrement du meilleur modèle\n",
    "best_model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "registered_model_name = \"simple_rnn_best_pre\"\n",
    "registered_model = mlflow.register_model(best_model_uri, registered_model_name)\n",
    "# Enregistrement des paramètres sous forme de tags dans le modèle enregistré\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"Setting tag {key} = {value} in registered model\")\n",
    "    client.set_model_version_tag(\n",
    "        name=registered_model_name,\n",
    "        version=str(registered_model.version),\n",
    "        key=str(key),\n",
    "        value=str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a882a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d4aeae5",
   "metadata": {},
   "source": [
    "# Experimentation sur les embeddings (custom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b0f02",
   "metadata": {},
   "source": [
    "## Préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58932e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText, Word2Vec\n",
    "\n",
    "embedding_dict = {'Word2Vec':Word2Vec, \n",
    "                  'FastText':FastText}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca2b238",
   "metadata": {},
   "source": [
    "## Core fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86f78a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction à optimiser pour optuna\n",
    "\n",
    "## Prétraitement\n",
    "min_count = 5\n",
    "num_words = 20000\n",
    "max_len   = 30\n",
    "stemmer   = 'PorterStemmer'\n",
    "# Prétraitement\n",
    "X_sentence_train, tokenizer, sentences_train = preprocess_data_embedding(X_raw=X_train, \n",
    "                                                        stem_lem_func=PorterStemmer().stem,\n",
    "                                                        tokenizer=None, \n",
    "                                                        stop_words=stopwords.words('english'), \n",
    "                                                        min_count=min_count,\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words, \n",
    "                                                        return_sentences=True) \n",
    "X_sentence_val = preprocess_data_embedding(X_raw=X_val, \n",
    "                                                        stem_lem_func=PorterStemmer().stem,\n",
    "                                                        tokenizer=tokenizer, \n",
    "                                                        stop_words=stopwords.words('english'), \n",
    "                                                        min_count=1, # mincount = 1 car on est sur le jeu de validation\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words) \n",
    "\n",
    "def embedding_eval_custom_embed(trial):\n",
    "    # Hyperparamètres\n",
    "\n",
    "    ## Embedding\n",
    "    embedding_model = trial.suggest_categorical('embedding_model',list(embedding_dict.keys()))\n",
    "    latent_dim = trial.suggest_int('latent_dim', 30, 150)\n",
    "    window = trial.suggest_int(\"window\", 2, 10)\n",
    "    sg = trial.suggest_int('sg',0,1)\n",
    "\n",
    "    ## Modèle\n",
    "    rnn_size = 64\n",
    "    ## Entrainement\n",
    "    epochs = 50\n",
    "    lr = 1e-3\n",
    "    ## Savepath des poids du modèle\n",
    "    model_savepath = \"./Models/baselineRNN_pre.h5\"\n",
    "\n",
    "\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_params(params={\n",
    "            'num_words':num_words,               \n",
    "            'max_len': max_len,\n",
    "            'min_count': min_count,\n",
    "            'stemmer': stemmer, \n",
    "            'latent_dim': latent_dim, \n",
    "            'rnn_size': rnn_size, \n",
    "            'embedding_model':embedding_model,\n",
    "            'sg':sg,\n",
    "            'window':window,\n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr \n",
    "        })\n",
    "\n",
    "\n",
    "        # Embedding(custom)\n",
    "        if embedding_model=='Word2Vec':\n",
    "            embedding_model = Word2Vec(\n",
    "                sentences=sentences_train, \n",
    "                vector_size=latent_dim,  # dimension de l’espace latent\n",
    "                window=5,         # taille du contexte\n",
    "                min_count=min_count,      # ignorer les mots trop rares\n",
    "            workers=4,        # parallélisme CPU\n",
    "            sg=sg,              # 1 = skip-gram, 0 = CBOW\n",
    "            epochs=30\n",
    "            )\n",
    "        elif embedding_model=='FastText':\n",
    "            embedding_model = FastText(\n",
    "                sentences=sentences_train, \n",
    "                vector_size=latent_dim, \n",
    "                window=5, \n",
    "                min_count=min_count,\n",
    "                workers=4,\n",
    "                sg=sg,\n",
    "                epochs=30\n",
    "                )\n",
    "\n",
    "\n",
    "        model_vectors = embedding_model.wv\n",
    "        w2v_words = model_vectors.index_to_key\n",
    "\n",
    "        embedding_matrix, vocab_size = build_embedding_matrix(tokenizer=tokenizer,\n",
    "                                          embedding_model=model_vectors, \n",
    "                                          latent_dim=latent_dim\n",
    "                                          )\n",
    "\n",
    "        \n",
    "        # Modèle\n",
    "        model =  build_base_RNN(vocab_size=vocab_size, \n",
    "                        latent_dim=latent_dim,\n",
    "                        input_length=max_len, \n",
    "                        embedding_matrix=embedding_matrix,\n",
    "                        rnn_size = rnn_size)\n",
    "        ## Callbacks\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "        ## Compilation\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "        #Entrainement\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(X_sentence_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_sentence_val,y_val), callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "        # Prédictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(X_sentence_val)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des métriques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        acc = output_dict[\"Accuracy\"]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095303da",
   "metadata": {},
   "source": [
    "## Definition de l'experiment MLFlow/Optuna\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6fdcac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization with Optuna...\n",
      "Setting up MLflow experiment...\n"
     ]
    }
   ],
   "source": [
    "# Création de l'étude Optuna et optimisation\n",
    "print(\"Starting hyperparameter optimization with Optuna...\")\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"optuna_word_embedding_experiment_custom_embedding\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"optuna_word_embedding_experiment_custom_embedding\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Cette experience contient les différents tests pour le modèle RNN simple. \"\n",
    "    \"Ici on évalue l'impact du type d'embedding custom et de la dimension de l'espace latent sur un modèle avec simpleRNN\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"simple-RNN-preprocessing\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efe00de",
   "metadata": {},
   "source": [
    "## Lancement de l'optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91597411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/14 23:15:35 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://b70c20a6-0687-4b41-a51b-c3b2c379e863/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://b70c20a6-0687-4b41-a51b-c3b2c379e863/assets\n",
      "2025/09/14 23:15:42 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpkjp5weh8\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/14 23:15:42 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-14 23:15:42,662] Trial 20 finished with value: 0.75128125 and parameters: {'embedding_model': 'FastText', 'latent_dim': 105, 'window': 4, 'sg': 0}. Best is trial 20 with value: 0.75128125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9539, 107)\n",
      "Words found in pretrained embeddings: 9538/9539 (99.99%)\n",
      "1000/1000 [==============================] - 7s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/14 23:31:07 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://90e3f848-a0c6-4c46-ac94-1d010557c7c6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://90e3f848-a0c6-4c46-ac94-1d010557c7c6/assets\n",
      "2025/09/14 23:31:15 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpy8fltxia\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/14 23:31:15 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-14 23:31:15,189] Trial 21 finished with value: 0.748125 and parameters: {'embedding_model': 'FastText', 'latent_dim': 107, 'window': 4, 'sg': 0}. Best is trial 20 with value: 0.75128125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9539, 128)\n",
      "Words found in pretrained embeddings: 9538/9539 (99.99%)\n",
      "1000/1000 [==============================] - 7s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/14 23:49:30 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f3b62c82-063e-404e-b5e3-5921370a300b/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f3b62c82-063e-404e-b5e3-5921370a300b/assets\n",
      "2025/09/14 23:49:37 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpib947_97\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/14 23:49:37 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-14 23:49:37,758] Trial 22 finished with value: 0.7520625 and parameters: {'embedding_model': 'FastText', 'latent_dim': 128, 'window': 3, 'sg': 0}. Best is trial 22 with value: 0.7520625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9539, 87)\n",
      "Words found in pretrained embeddings: 9538/9539 (99.99%)\n",
      "1000/1000 [==============================] - 7s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 00:09:29 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://5de7e54b-cc75-4177-876e-aeeeb2b61353/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://5de7e54b-cc75-4177-876e-aeeeb2b61353/assets\n",
      "2025/09/15 00:09:36 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpikdd_sqz\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/15 00:09:36 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-15 00:09:37,160] Trial 23 finished with value: 0.75309375 and parameters: {'embedding_model': 'FastText', 'latent_dim': 87, 'window': 5, 'sg': 0}. Best is trial 23 with value: 0.75309375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9539, 82)\n",
      "Words found in pretrained embeddings: 9538/9539 (99.99%)\n",
      "1000/1000 [==============================] - 6s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 00:34:36 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f1fbedb6-6c7c-4daf-a013-69dda45d9a38/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f1fbedb6-6c7c-4daf-a013-69dda45d9a38/assets\n",
      "2025/09/15 00:34:43 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpvlz2sjq6\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/15 00:34:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-15 00:34:43,999] Trial 24 finished with value: 0.74728125 and parameters: {'embedding_model': 'FastText', 'latent_dim': 82, 'window': 5, 'sg': 0}. Best is trial 23 with value: 0.75309375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9539, 68)\n",
      "Words found in pretrained embeddings: 9538/9539 (99.99%)\n",
      "1000/1000 [==============================] - 4s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 00:52:39 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://0e331421-f9d2-4161-b6ad-03bfa41c4815/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://0e331421-f9d2-4161-b6ad-03bfa41c4815/assets\n",
      "2025/09/15 00:52:46 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpm4itqpjy\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/15 00:52:46 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-15 00:52:46,131] Trial 25 finished with value: 0.746 and parameters: {'embedding_model': 'FastText', 'latent_dim': 68, 'window': 6, 'sg': 0}. Best is trial 23 with value: 0.75309375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9539, 104)\n",
      "Words found in pretrained embeddings: 9537/9539 (99.98%)\n",
      "1000/1000 [==============================] - 4s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 00:59:33 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://55c7d27b-03bd-4a85-8351-997dc83de75e/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://55c7d27b-03bd-4a85-8351-997dc83de75e/assets\n",
      "2025/09/15 00:59:40 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmphe28srfq\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/15 00:59:40 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-15 00:59:40,338] Trial 26 finished with value: 0.7395 and parameters: {'embedding_model': 'Word2Vec', 'latent_dim': 104, 'window': 7, 'sg': 0}. Best is trial 23 with value: 0.75309375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9539, 97)\n",
      "Words found in pretrained embeddings: 9538/9539 (99.99%)\n",
      "1000/1000 [==============================] - 7s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 01:14:44 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://883d2e02-1fbc-480a-babd-3e54894ecd9e/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://883d2e02-1fbc-480a-babd-3e54894ecd9e/assets\n",
      "2025/09/15 01:14:51 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpr821o2rh\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/15 01:14:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-15 01:14:51,913] Trial 27 finished with value: 0.750875 and parameters: {'embedding_model': 'FastText', 'latent_dim': 97, 'window': 4, 'sg': 0}. Best is trial 23 with value: 0.75309375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9539, 87)\n",
      "Words found in pretrained embeddings: 9538/9539 (99.99%)\n",
      "1000/1000 [==============================] - 6s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 01:27:35 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://16f2a0f0-884f-4643-b782-ce6d806cb19a/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://16f2a0f0-884f-4643-b782-ce6d806cb19a/assets\n",
      "2025/09/15 01:27:42 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpz010wyok\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/15 01:27:42 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-15 01:27:43,015] Trial 28 finished with value: 0.6356875 and parameters: {'embedding_model': 'FastText', 'latent_dim': 87, 'window': 5, 'sg': 0}. Best is trial 23 with value: 0.75309375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9539, 112)\n",
      "Words found in pretrained embeddings: 9537/9539 (99.98%)\n",
      "1000/1000 [==============================] - 7s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 01:45:46 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://6f8ba309-1b71-443b-8ca1-0ff6e75d953a/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://6f8ba309-1b71-443b-8ca1-0ff6e75d953a/assets\n",
      "2025/09/15 01:45:54 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp902n5hpw\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/15 01:45:54 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-15 01:45:54,860] Trial 29 finished with value: 0.751 and parameters: {'embedding_model': 'Word2Vec', 'latent_dim': 112, 'window': 7, 'sg': 0}. Best is trial 23 with value: 0.75309375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization completed.\n"
     ]
    }
   ],
   "source": [
    "# Lancement de l'optimisation avec Optuna\n",
    "print(\"Starting optimization trials...\")\n",
    "with mlflow.start_run(run_name=\"optuna_word_embedding_experiment_custom_embedding\"):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(embedding_eval_custom_embed, n_trials=30)\n",
    "    mlflow.log_params(study.best_params)\n",
    "    mlflow.log_metric(\"best_accuracy\", study.best_value)\n",
    "\n",
    "print(\"Optimization completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78809adc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffd878eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Successfully registered model 'simple_rnn_best_custom_embed'.\n",
      "2025/09/15 01:52:09 WARNING mlflow.tracking._model_registry.fluent: Run with id 4461c64ebbfd43c884ba93c17bb3838a has no artifacts at artifact path 'model', registering model based on models:/m-93f5ba0e17c84bb6b7c4c24b0acc506d instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run ID: 4461c64ebbfd43c884ba93c17bb3838a with metrics:\n",
      "Accuracy: 0.75309375\n",
      "F1_negatif: 0.7489594255393512\n",
      "F1_positif: 0.7570941064346542\n",
      "Precision_negatif: 0.7617139533380728\n",
      "Precision_positif: 0.7450232952138924\n",
      "Recall_negatif: 0.736625\n",
      "Recall_positif: 0.7695625\n",
      "ROC_AUC: 0.8303465761718751\n",
      "Best run parameters:\n",
      "embedding_model: FastText\n",
      "epochs: 50\n",
      "latent_dim: 87\n",
      "learning_rate: 0.001\n",
      "max_len: 30\n",
      "min_count: 5\n",
      "num_words: 20000\n",
      "rnn_size: 64\n",
      "sg: 0\n",
      "stemmer: PorterStemmer\n",
      "window: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'simple_rnn_best_custom_embed'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting tag embedding_model = FastText in registered model\n",
      "Setting tag epochs = 50 in registered model\n",
      "Setting tag latent_dim = 87 in registered model\n",
      "Setting tag learning_rate = 0.001 in registered model\n",
      "Setting tag max_len = 30 in registered model\n",
      "Setting tag min_count = 5 in registered model\n",
      "Setting tag num_words = 20000 in registered model\n",
      "Setting tag rnn_size = 64 in registered model\n",
      "Setting tag sg = 0 in registered model\n",
      "Setting tag stemmer = PorterStemmer in registered model\n",
      "Setting tag window = 5 in registered model\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")\n",
    "experiment_id = mlflow.get_experiment_by_name(\"optuna_word_embedding_experiment_custom_embedding\").experiment_id\n",
    "runs = client.search_runs(experiment_id)\n",
    "\n",
    "# Métrique pour sélectionner le meilleur modèle\n",
    "metric_to_optimize = \"Accuracy\" # liste des métriques enregistrées dans postprocess_data.py ou sur l'UI MLflow\n",
    "best_run = max(runs, key=lambda run: run.data.metrics.get(metric_to_optimize, float('-inf')))\n",
    "print(f\"Best run ID: {best_run.info.run_id} with metrics:\")\n",
    "for key, value in best_run.data.metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Best run parameters:\")\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Enregistrement du meilleur modèle\n",
    "best_model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "registered_model_name = \"simple_rnn_best_custom_embed\"\n",
    "registered_model = mlflow.register_model(best_model_uri, registered_model_name)\n",
    "# Enregistrement des paramètres sous forme de tags dans le modèle enregistré\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"Setting tag {key} = {value} in registered model\")\n",
    "    client.set_model_version_tag(\n",
    "        name=registered_model_name,\n",
    "        version=str(registered_model.version),\n",
    "        key=str(key),\n",
    "        value=str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b7c999",
   "metadata": {},
   "source": [
    "# Experimentation sur les embeddings (préentrainés)\n",
    "\n",
    "Les embeddings pré-entrainés ont été entrainés sur un très grand nombre de tweets et prennent donc en compte un très grand nombre de situations. On peut donc se passer de la phase de stemming et garder les stopwords. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a8c3c",
   "metadata": {},
   "source": [
    "## Préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f259b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# Word2Vec Google News\n",
    "w2v_google = api.load(\"word2vec-google-news-300\")  # KeyedVectors\n",
    "\n",
    "# FastText wiki-news subwords-300\n",
    "ft_wiki = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "\n",
    "# GloVe Twitter (par exemple 200 dimensions)\n",
    "glove_tw200 = api.load(\"glove-twitter-200\")\n",
    "glove_tw100 = api.load(\"glove-twitter-100\")\n",
    "glove_tw50  = api.load(\"glove-twitter-50\")\n",
    "glove_tw25  = api.load(\"glove-twitter-25\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b959004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {'Word2Vec_google':w2v_google, \n",
    "                  'FastText_wiki':ft_wiki,\n",
    "                  'Glove_twitter_200':glove_tw200, \n",
    "                  'Glove_twitter_100':glove_tw100, \n",
    "                  'Glove_twitter_50':glove_tw50, \n",
    "                  'Glove_twitter_25':glove_tw25\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6540f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prétraitement\n",
    "min_count = 2\n",
    "num_words = 20000\n",
    "max_len   = 30\n",
    "# Prétraitement\n",
    "X_sentence_train, tokenizer, sentences_train = preprocess_data_embedding(X_raw=X_train, \n",
    "                                                        stem_lem_func=None,\n",
    "                                                        tokenizer=None, \n",
    "                                                        stop_words=None, \n",
    "                                                        min_count=min_count,\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words, \n",
    "                                                        return_sentences=True) \n",
    "X_sentence_val = preprocess_data_embedding(X_raw=X_val, \n",
    "                                                        stem_lem_func=None,\n",
    "                                                        tokenizer=tokenizer, \n",
    "                                                        stop_words=None, \n",
    "                                                        min_count=1, # mincount = 1 car on est sur le jeu de validation\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab8767c",
   "metadata": {},
   "source": [
    "## Fonction de base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6212a43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamètres\n",
    "\n",
    "## Modèle\n",
    "rnn_size = 64\n",
    "## Entrainement\n",
    "epochs = 50\n",
    "lr = 1e-3\n",
    "## Savepath des poids du modèle\n",
    "model_savepath = \"./Models/baselineRNN_pretrained_embed.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d87b166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embed_experiment(embedding_name):\n",
    "     with mlflow.start_run():\n",
    "        latent_dim = embedding_dict[embedding_name].vector_size\n",
    "        mlflow.log_params(params={\n",
    "            'num_words':num_words,               \n",
    "            'max_len': max_len,\n",
    "            'min_count': min_count,\n",
    "            'stemmer': 'None', \n",
    "            'latent_dim': latent_dim, \n",
    "            'rnn_size': rnn_size, \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr,\n",
    "            'embedding_name':embedding_name \n",
    "        })\n",
    "\n",
    "\n",
    "        model_vectors = embedding_dict[embedding_name]\n",
    "\n",
    "        embedding_matrix, vocab_size = build_embedding_matrix(tokenizer=tokenizer,\n",
    "                                          embedding_model=model_vectors, \n",
    "                                          latent_dim=latent_dim\n",
    "                                          )\n",
    "\n",
    "        # Modèle\n",
    "        model =  build_base_RNN(vocab_size=vocab_size, \n",
    "                        latent_dim=latent_dim,\n",
    "                        input_length=max_len, \n",
    "                        embedding_matrix=embedding_matrix,\n",
    "                        rnn_size = rnn_size)\n",
    "        ## Callbacks\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "        ## Compilation\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "        #Entrainement\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(X_sentence_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_sentence_val,y_val), callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "                # Prédictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(X_sentence_val)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des métriques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d50b14",
   "metadata": {},
   "source": [
    "## Définition de l'experiment dans MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "079ae708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "2025/09/15 02:48:45 INFO mlflow.tracking.fluent: Experiment with name 'word_embedding_experiment_pretrained_embedding' does not exist. Creating a new experiment.\n",
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization with Optuna...\n",
      "Setting up MLflow experiment...\n"
     ]
    }
   ],
   "source": [
    "# Création de l'étude Optuna et optimisation\n",
    "print(\"Starting hyperparameter optimization with Optuna...\")\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"word_embedding_experiment_pretrained_embedding\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"word_embedding_experiment_pretrained_embedding\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Cette experience contient les différents tests pour le modèle RNN simple. \"\n",
    "    \"Ici on teste plusieurs embeddings préentrainées sur de larges corpora, beaucoup d'attentes par rapport aux embeddings avec Glove entrainés sur des tweets\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"simple-RNN-pretrained-embeddings\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0697ce9",
   "metadata": {},
   "source": [
    "## Lancement de l'experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de3a8135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with Word2Vec_google\n",
      "Embedding matrix shape: (24711, 300)\n",
      "Words found in pretrained embeddings: 20292/24711 (82.12%)\n",
      "1000/1000 [==============================] - 7s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 03:02:53 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 03:02:53 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpft6gpi5_\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpft6gpi5_\\model\\data\\model\\assets\n",
      "2025/09/15 03:03:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with FastText_wiki\n",
      "Embedding matrix shape: (24711, 300)\n",
      "Words found in pretrained embeddings: 21410/24711 (86.64%)\n",
      "1000/1000 [==============================] - 5s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 03:17:56 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 03:17:56 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpg7xm_dcc\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpg7xm_dcc\\model\\data\\model\\assets\n",
      "2025/09/15 03:18:14 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with Glove_twitter_200\n",
      "Embedding matrix shape: (24711, 200)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n",
      "1000/1000 [==============================] - 5s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 03:28:50 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 03:28:50 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp3_kfzaqj\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp3_kfzaqj\\model\\data\\model\\assets\n",
      "2025/09/15 03:28:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with Glove_twitter_100\n",
      "Embedding matrix shape: (24711, 100)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n",
      "1000/1000 [==============================] - 5s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 03:39:24 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 03:39:24 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpbuatvcxy\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpbuatvcxy\\model\\data\\model\\assets\n",
      "2025/09/15 03:39:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with Glove_twitter_50\n",
      "Embedding matrix shape: (24711, 50)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n",
      "1000/1000 [==============================] - 5s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 03:56:37 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 03:56:38 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpkkv9oxpy\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpkkv9oxpy\\model\\data\\model\\assets\n",
      "2025/09/15 03:56:46 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with Glove_twitter_25\n",
      "Embedding matrix shape: (24711, 25)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n",
      "1000/1000 [==============================] - 5s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 04:14:15 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 04:14:15 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpwkeji3ag\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpwkeji3ag\\model\\data\\model\\assets\n",
      "2025/09/15 04:14:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "for embedding_name in list(embedding_dict.keys()):\n",
    "    \n",
    "    print(f\"Running test with {embedding_name}\")\n",
    "    pretrained_embed_experiment(embedding_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa14dca",
   "metadata": {},
   "source": [
    "## Enregistrement du meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "abb9d454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Registered model 'simple_rnn_best_pre' already exists. Creating a new version of this model...\n",
      "2025/09/15 14:33:57 WARNING mlflow.tracking._model_registry.fluent: Run with id ce06b66ec4ff48e6a994d2485c63701f has no artifacts at artifact path 'model', registering model based on models:/m-6b8da83843cc4061a7b1edb36b9eb5f4 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run ID: ce06b66ec4ff48e6a994d2485c63701f with metrics:\n",
      "Accuracy: 0.795875\n",
      "F1_negatif: 0.7963459499906467\n",
      "F1_positif: 0.7954018668170143\n",
      "Precision_negatif: 0.7945128779395296\n",
      "Precision_positif: 0.7972497802335803\n",
      "Recall_negatif: 0.7981875\n",
      "Recall_positif: 0.7935625\n",
      "ROC_AUC: 0.874631751953125\n",
      "Best run parameters:\n",
      "embedding_name: Glove_twitter_200\n",
      "epochs: 50\n",
      "latent_dim: 200\n",
      "learning_rate: 0.001\n",
      "max_len: 30\n",
      "min_count: 2\n",
      "num_words: 20000\n",
      "rnn_size: 64\n",
      "stemmer: None\n",
      "Setting tag embedding_name = Glove_twitter_200 in registered model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'simple_rnn_best_pre'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting tag epochs = 50 in registered model\n",
      "Setting tag latent_dim = 200 in registered model\n",
      "Setting tag learning_rate = 0.001 in registered model\n",
      "Setting tag max_len = 30 in registered model\n",
      "Setting tag min_count = 2 in registered model\n",
      "Setting tag num_words = 20000 in registered model\n",
      "Setting tag rnn_size = 64 in registered model\n",
      "Setting tag stemmer = None in registered model\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")\n",
    "experiment_id = mlflow.get_experiment_by_name(\"word_embedding_experiment_pretrained_embedding\").experiment_id\n",
    "runs = client.search_runs(experiment_id)\n",
    "\n",
    "# Métrique pour sélectionner le meilleur modèle\n",
    "metric_to_optimize = \"Accuracy\" # liste des métriques enregistrées dans postprocess_data.py ou sur l'UI MLflow\n",
    "best_run = max(runs, key=lambda run: run.data.metrics.get(metric_to_optimize, float('-inf')))\n",
    "print(f\"Best run ID: {best_run.info.run_id} with metrics:\")\n",
    "for key, value in best_run.data.metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Best run parameters:\")\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Enregistrement du meilleur modèle\n",
    "best_model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "registered_model_name = \"simple_rnn_best_pre\"\n",
    "registered_model = mlflow.register_model(best_model_uri, registered_model_name)\n",
    "# Enregistrement des paramètres sous forme de tags dans le modèle enregistré\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"Setting tag {key} = {value} in registered model\")\n",
    "    client.set_model_version_tag(\n",
    "        name=registered_model_name,\n",
    "        version=str(registered_model.version),\n",
    "        key=str(key),\n",
    "        value=str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579521f6",
   "metadata": {},
   "source": [
    "# Comparaison des différentes couches de notre réseau de neurones : SimpleRNN vs GRU vs LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5966ad3",
   "metadata": {},
   "source": [
    "## Préparation \n",
    "\n",
    "On reprend les paramètres d'embedding de la meilleure run sur embeddings customs et embeddings préentrainés. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45154a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (24711, 200)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n"
     ]
    }
   ],
   "source": [
    "## Prétraitement\n",
    "min_count = 2\n",
    "num_words = 20000\n",
    "max_len   = 30\n",
    "# Prétraitement\n",
    "X_sentence_train, tokenizer, sentences_train = preprocess_data_embedding(X_raw=X_train, \n",
    "                                                        stem_lem_func=None,\n",
    "                                                        tokenizer=None, \n",
    "                                                        stop_words=None, \n",
    "                                                        min_count=min_count,\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words, \n",
    "                                                        return_sentences=True) \n",
    "X_sentence_val = preprocess_data_embedding(X_raw=X_val, \n",
    "                                                        stem_lem_func=None,\n",
    "                                                        tokenizer=tokenizer, \n",
    "                                                        stop_words=None, \n",
    "                                                        min_count=1, # mincount = 1 car on est sur le jeu de validation\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words) \n",
    "\n",
    "\n",
    "model_vectors = glove_tw200\n",
    "latent_dim = glove_tw200.vector_size\n",
    "\n",
    "embedding_matrix, vocab_size = build_embedding_matrix(tokenizer=tokenizer,\n",
    "                                embedding_model=model_vectors, \n",
    "                                latent_dim=latent_dim\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80933f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Source.preprocess_data import *\n",
    "rnn_layer_name_list = ['SimpleRNN','GRU','LSTM']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a066a0f",
   "metadata": {},
   "source": [
    "## Fonction de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "765bce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modèle\n",
    "rnn_size = 64\n",
    "## Entrainement\n",
    "epochs = 50\n",
    "lr = 1e-3\n",
    "## Savepath des poids du modèle\n",
    "\n",
    "def rnn_layer_experiment(rnn_layer_name):\n",
    "     with mlflow.start_run():\n",
    "        mlflow.log_params(params={\n",
    "            'num_words':num_words,               \n",
    "            'max_len': max_len,\n",
    "            'min_count': min_count,\n",
    "            'stemmer': 'None', \n",
    "            'latent_dim': latent_dim, \n",
    "            'rnn_size': rnn_size, \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr,\n",
    "            'embedding_name':'Glove_twitter_200',\n",
    "            'rnn_layer_name':rnn_layer_name \n",
    "        })\n",
    "        model_savepath = \"./Models/\"+rnn_layer_name+\"_model_exp.h5\"\n",
    "        # Modèle\n",
    "        if rnn_layer_name == 'SimpleRNN':\n",
    "            model =  build_base_RNN(vocab_size=vocab_size, \n",
    "                            latent_dim=latent_dim,\n",
    "                            input_length=max_len, \n",
    "                            embedding_matrix=embedding_matrix,\n",
    "                            rnn_size = rnn_size)\n",
    "        elif rnn_layer_name == 'GRU':\n",
    "            model = build_gru_RNN(vocab_size=vocab_size, \n",
    "                            latent_dim=latent_dim,\n",
    "                            input_length=max_len, \n",
    "                            embedding_matrix=embedding_matrix,\n",
    "                            rnn_size = rnn_size)\n",
    "        elif rnn_layer_name=='LSTM':\n",
    "            model = build_lstm_RNN(vocab_size=vocab_size, \n",
    "                            latent_dim=latent_dim,\n",
    "                            input_length=max_len, \n",
    "                            embedding_matrix=embedding_matrix,\n",
    "                            rnn_size = rnn_size)\n",
    "        ## Callbacks\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "        ## Compilation\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "        #Entrainement\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(X_sentence_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_sentence_val,y_val), callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "                # Prédictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(X_sentence_val)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des métriques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdd9607",
   "metadata": {},
   "source": [
    "## Definition de l'experiment dans MLFLow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a9394a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MLflow experiment...\n"
     ]
    }
   ],
   "source": [
    "# Création de l'étude Optuna et optimisation\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"rnn_layer_experiment_pretrained_embedding\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"rnn_layer_experiment_pretrained_embedding\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Comparaison des impact des types de cellules RNN utilisées : SimpleRNN, GRU et LSTM \"\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"RNN_types-pretrained-embeddings\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362e75c9",
   "metadata": {},
   "source": [
    "## Lancement de l'expériment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af58cb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with SimpleRNN\n",
      "1000/1000 [==============================] - 8s 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 20:51:51 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 20:51:51 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpzg36l18g\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpzg36l18g\\model\\data\\model\\assets\n",
      "2025/09/15 20:52:03 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with GRU\n",
      "1000/1000 [==============================] - 5s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 21:00:15 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 21:00:15 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpmp__baop\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpmp__baop\\model\\data\\model\\assets\n",
      "2025/09/15 21:00:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with LSTM\n",
      "1000/1000 [==============================] - 6s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 21:09:40 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 21:09:40 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmps1u9tisp\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmps1u9tisp\\model\\data\\model\\assets\n",
      "2025/09/15 21:09:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "for rnn_layer_name in rnn_layer_name_list:\n",
    "    \n",
    "    print(f\"Running test with {rnn_layer_name}\")\n",
    "    rnn_layer_experiment(rnn_layer_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f7e94",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb8369b0",
   "metadata": {},
   "source": [
    "# Retour sur la longueur des séquences     \n",
    "\n",
    "Ici on va revenir sur la longueur des séquences utilisées car LSTM permet de garder des séquences plus longues sans pour autant avoir d'évanescence de gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb59f6",
   "metadata": {},
   "source": [
    "## Fonction de base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c298ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Modèle\n",
    "rnn_size = 64\n",
    "## Entrainement\n",
    "epochs = 50\n",
    "lr = 1e-3\n",
    "## Savepath des poids du modèle\n",
    "\n",
    "def lstm_maxlen_experiment(max_len):\n",
    "     with mlflow.start_run():\n",
    "        latent_dim = glove_tw200.vector_size\n",
    "\n",
    "        mlflow.log_params(params={\n",
    "            'num_words':num_words,               \n",
    "            'max_len': max_len,\n",
    "            'min_count': min_count,\n",
    "            'stemmer': 'None', \n",
    "            'latent_dim': latent_dim, \n",
    "            'rnn_size': rnn_size, \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr,\n",
    "            'embedding_name':'Glove_twitter_200',\n",
    "            'rnn_layer_name':'LSTM' \n",
    "        })\n",
    "\n",
    "        ## Prétraitement\n",
    "\n",
    "        # Prétraitement\n",
    "        X_sentence_train, tokenizer, sentences_train = preprocess_data_embedding(X_raw=X_train, \n",
    "                                                        stem_lem_func=None,\n",
    "                                                        tokenizer=None, \n",
    "                                                        stop_words=None, \n",
    "                                                        min_count=min_count,\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words, \n",
    "                                                        return_sentences=True) \n",
    "        X_sentence_val = preprocess_data_embedding(X_raw=X_val, \n",
    "                                                        stem_lem_func=None,\n",
    "                                                        tokenizer=tokenizer, \n",
    "                                                        stop_words=None, \n",
    "                                                        min_count=1, # mincount = 1 car on est sur le jeu de validation\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words) \n",
    "\n",
    "\n",
    "        model_vectors = glove_tw200\n",
    "\n",
    "        embedding_matrix, vocab_size = build_embedding_matrix(tokenizer=tokenizer,\n",
    "                                embedding_model=model_vectors, \n",
    "                                latent_dim=latent_dim\n",
    "                              )\n",
    "        model_savepath = f\"./Models/{rnn_layer_name}_model_exp_len{max_len}.h5\"\n",
    "        # Modèle\n",
    "\n",
    "        model = build_lstm_RNN(vocab_size=vocab_size, \n",
    "                            latent_dim=latent_dim,\n",
    "                            input_length=max_len, \n",
    "                            embedding_matrix=embedding_matrix,\n",
    "                            rnn_size = rnn_size)\n",
    "        ## Callbacks\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "        ## Compilation\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "        #Entrainement\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(X_sentence_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_sentence_val,y_val), callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "                # Prédictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(X_sentence_val)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des métriques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5404a5",
   "metadata": {},
   "source": [
    "## Experiment MLFLow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdc6830b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MLflow experiment...\n"
     ]
    }
   ],
   "source": [
    "# Création de l'étude Optuna et optimisation\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"lstm_maxlen_experiment\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"lstm_maxlen_experiment\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Comparaison des impact des types de cellules RNN utilisées : SimpleRNN, GRU et LSTM \"\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"LSTM_pretrained_embedding\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f405b051",
   "metadata": {},
   "source": [
    "## Lancement de l'experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c0c6c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 16s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 23:06:53 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 23:06:53 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpeobtdrra\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpeobtdrra\\model\\data\\model\\assets\n",
      "2025/09/15 23:07:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with sequence length of 75 tokens\n",
      "Embedding matrix shape: (24711, 200)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n",
      "1000/1000 [==============================] - 26s 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 00:06:53 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/16 00:06:53 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_10_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpbaxxk0ux\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpbaxxk0ux\\model\\data\\model\\assets\n",
      "2025/09/16 00:07:18 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with sequence length of 80 tokens\n",
      "Embedding matrix shape: (24711, 200)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n",
      "1000/1000 [==============================] - 19s 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 01:12:46 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/16 01:12:46 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_11_layer_call_fn, lstm_cell_11_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpvujtzpq5\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpvujtzpq5\\model\\data\\model\\assets\n",
      "2025/09/16 01:13:05 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with sequence length of 85 tokens\n",
      "Embedding matrix shape: (24711, 200)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n",
      "1000/1000 [==============================] - 19s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 02:03:47 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/16 02:03:47 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_12_layer_call_fn, lstm_cell_12_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp4kp924jp\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp4kp924jp\\model\\data\\model\\assets\n",
      "2025/09/16 02:04:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with sequence length of 90 tokens\n",
      "Embedding matrix shape: (24711, 200)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n",
      "1000/1000 [==============================] - 25s 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 03:09:26 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/16 03:09:26 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp0qlemuot\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp0qlemuot\\model\\data\\model\\assets\n",
      "2025/09/16 03:09:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with sequence length of 95 tokens\n",
      "Embedding matrix shape: (24711, 200)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n",
      "1000/1000 [==============================] - 22s 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 04:12:22 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/16 04:12:22 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_14_layer_call_fn, lstm_cell_14_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpybz59rw1\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpybz59rw1\\model\\data\\model\\assets\n",
      "2025/09/16 04:12:54 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "for max_len in list(range(30,100,5)):\n",
    "    \n",
    "    print(f\"Running test with sequence length of {max_len} tokens\")\n",
    "    lstm_maxlen_experiment(max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecba592",
   "metadata": {},
   "source": [
    "## Enregistrement du mailleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "406df028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Successfully registered model 'lstm_maxlen_best'.\n",
      "2025/09/15 20:13:01 WARNING mlflow.tracking._model_registry.fluent: Run with id b63acc4f64914a5fb77ab6b26a974e13 has no artifacts at artifact path 'model', registering model based on models:/m-612a40cdbb1d4774b96429b02b0740f8 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run ID: b63acc4f64914a5fb77ab6b26a974e13 with metrics:\n",
      "Accuracy: 0.81365625\n",
      "F1_negatif: 0.811518159117489\n",
      "F1_positif: 0.8157463770355035\n",
      "Precision_negatif: 0.8209375199846518\n",
      "Precision_positif: 0.8066980382570433\n",
      "Recall_negatif: 0.8023125\n",
      "Recall_positif: 0.825\n",
      "ROC_AUC: 0.89485159765625\n",
      "Best run parameters:\n",
      "embedding_name: Glove_twitter_200\n",
      "epochs: 50\n",
      "latent_dim: 200\n",
      "learning_rate: 0.001\n",
      "max_len: 50\n",
      "min_count: 2\n",
      "num_words: 20000\n",
      "rnn_layer_name: LSTM\n",
      "rnn_size: 64\n",
      "stemmer: None\n",
      "Setting tag embedding_name = Glove_twitter_200 in registered model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'lstm_maxlen_best'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting tag epochs = 50 in registered model\n",
      "Setting tag latent_dim = 200 in registered model\n",
      "Setting tag learning_rate = 0.001 in registered model\n",
      "Setting tag max_len = 50 in registered model\n",
      "Setting tag min_count = 2 in registered model\n",
      "Setting tag num_words = 20000 in registered model\n",
      "Setting tag rnn_layer_name = LSTM in registered model\n",
      "Setting tag rnn_size = 64 in registered model\n",
      "Setting tag stemmer = None in registered model\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")\n",
    "experiment_id = mlflow.get_experiment_by_name(\"lstm_maxlen_experiment\").experiment_id\n",
    "runs = client.search_runs(experiment_id)\n",
    "\n",
    "# Métrique pour sélectionner le meilleur modèle\n",
    "metric_to_optimize = \"Accuracy\" # liste des métriques enregistrées dans postprocess_data.py ou sur l'UI MLflow\n",
    "best_run = max(runs, key=lambda run: run.data.metrics.get(metric_to_optimize, float('-inf')))\n",
    "print(f\"Best run ID: {best_run.info.run_id} with metrics:\")\n",
    "for key, value in best_run.data.metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Best run parameters:\")\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Enregistrement du meilleur modèle\n",
    "best_model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "registered_model_name = \"lstm_maxlen_best\"\n",
    "registered_model = mlflow.register_model(best_model_uri, registered_model_name)\n",
    "# Enregistrement des paramètres sous forme de tags dans le modèle enregistré\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"Setting tag {key} = {value} in registered model\")\n",
    "    client.set_model_version_tag(\n",
    "        name=registered_model_name,\n",
    "        version=str(registered_model.version),\n",
    "        key=str(key),\n",
    "        value=str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3d788b",
   "metadata": {},
   "source": [
    "Quand on regarde les améliorations obtenues en augmentant la longueur des séquences, il n'est pas réellement pertinent d'augmenter la longueur des séquences au delà de 50 tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c8dba",
   "metadata": {},
   "source": [
    "# Essai avec architecture Bidirectionnal-LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901638b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# GloVe Twitter (par exemple 200 dimensions)\n",
    "glove_tw200 = api.load(\"glove-twitter-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab7afc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (24711, 200)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n"
     ]
    }
   ],
   "source": [
    "## Prétraitement\n",
    "min_count = 2\n",
    "num_words = 30000\n",
    "max_len   = 50\n",
    "# Prétraitement\n",
    "X_sentence_train, tokenizer, sentences_train = preprocess_data_embedding(X_raw=X_train, \n",
    "                                                        stem_lem_func=None,\n",
    "                                                        tokenizer=None, \n",
    "                                                        stop_words=None, \n",
    "                                                        min_count=min_count,\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words, \n",
    "                                                        return_sentences=True) \n",
    "X_sentence_val = preprocess_data_embedding(X_raw=X_val, \n",
    "                                                        stem_lem_func=None,\n",
    "                                                        tokenizer=tokenizer, \n",
    "                                                        stop_words=None, \n",
    "                                                        min_count=1, # mincount = 1 car on est sur le jeu de validation\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words) \n",
    "\n",
    "\n",
    "model_vectors = glove_tw200\n",
    "latent_dim = glove_tw200.vector_size\n",
    "\n",
    "embedding_matrix, vocab_size = build_embedding_matrix(tokenizer=tokenizer,\n",
    "                                embedding_model=model_vectors, \n",
    "                                latent_dim=latent_dim\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d63fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modèle\n",
    "rnn_size = 128\n",
    "## Entrainement\n",
    "epochs = 50\n",
    "lr = 1e-3\n",
    "max_len = 50\n",
    "## Savepath des poids du modèle\n",
    "\n",
    "def rnn_layer_experiment_bi(rnn_layer_name):\n",
    "     with mlflow.start_run():\n",
    "        mlflow.log_params(params={\n",
    "            'num_words':num_words,               \n",
    "            'max_len': max_len,\n",
    "            'min_count': min_count,\n",
    "            'stemmer': 'None', \n",
    "            'latent_dim': latent_dim, \n",
    "            'rnn_size': rnn_size, \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr,\n",
    "            'embedding_name':'Glove_twitter_200',\n",
    "            'rnn_layer_name':rnn_layer_name \n",
    "        })\n",
    "        model_savepath = \"./Models/\"+rnn_layer_name+\"_model_exp.h5\"\n",
    "        # Modèle\n",
    "\n",
    "        if rnn_layer_name=='LSTM':\n",
    "            model = build_lstm_RNN(vocab_size=vocab_size, \n",
    "                            latent_dim=latent_dim,\n",
    "                            input_length=max_len, \n",
    "                            embedding_matrix=embedding_matrix,\n",
    "                            rnn_size = rnn_size)\n",
    "        elif rnn_layer_name=='Bi-LSTM':\n",
    "            model = build_bilstm_RNN(vocab_size=vocab_size, \n",
    "                            latent_dim=latent_dim,\n",
    "                            input_length=max_len, \n",
    "                            embedding_matrix=embedding_matrix,\n",
    "                            rnn_size = rnn_size)\n",
    "\n",
    "        ## Callbacks\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "        ## Compilation\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "        #Entrainement\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(X_sentence_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_sentence_val,y_val), callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "                # Prédictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(X_sentence_val)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des métriques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21bb2138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "2025/09/23 01:22:11 INFO mlflow.tracking.fluent: Experiment with name 'bilstm_experiment' does not exist. Creating a new experiment.\n",
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MLflow experiment...\n"
     ]
    }
   ],
   "source": [
    "# Création de l'étude Optuna et optimisation\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"bilstm_experiment\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"bilstm_experiment\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Essai Bidirectionnal LSTM \"\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"RNN_types-pretrained-embeddings\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a349bfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with Bi-LSTM\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 50, 200)           4942200   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 50, 256)          336896    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,295,609\n",
      "Trainable params: 353,409\n",
      "Non-trainable params: 4,942,200\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 519s 258ms/step - loss: 0.4830 - accuracy: 0.7658 - val_loss: 0.4472 - val_accuracy: 0.7937 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 432s 216ms/step - loss: 0.4432 - accuracy: 0.7923 - val_loss: 0.4246 - val_accuracy: 0.8048 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 472s 236ms/step - loss: 0.4287 - accuracy: 0.8002 - val_loss: 0.4147 - val_accuracy: 0.8096 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 2477s 1s/step - loss: 0.4171 - accuracy: 0.8071 - val_loss: 0.4109 - val_accuracy: 0.8118 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 604s 302ms/step - loss: 0.4079 - accuracy: 0.8130 - val_loss: 0.4063 - val_accuracy: 0.8143 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 604s 302ms/step - loss: 0.3994 - accuracy: 0.8174 - val_loss: 0.4016 - val_accuracy: 0.8167 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 479s 240ms/step - loss: 0.3923 - accuracy: 0.8217 - val_loss: 0.4014 - val_accuracy: 0.8162 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 487s 244ms/step - loss: 0.3865 - accuracy: 0.8241 - val_loss: 0.4019 - val_accuracy: 0.8197 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 486s 243ms/step - loss: 0.3785 - accuracy: 0.8274 - val_loss: 0.3968 - val_accuracy: 0.8192 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 504s 252ms/step - loss: 0.3738 - accuracy: 0.8314 - val_loss: 0.4004 - val_accuracy: 0.8169 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 512s 256ms/step - loss: 0.3690 - accuracy: 0.8322 - val_loss: 0.3996 - val_accuracy: 0.8195 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 527s 264ms/step - loss: 0.3563 - accuracy: 0.8395 - val_loss: 0.4013 - val_accuracy: 0.8210 - lr: 5.0000e-04\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 482s 241ms/step - loss: 0.3511 - accuracy: 0.8418 - val_loss: 0.4045 - val_accuracy: 0.8189 - lr: 5.0000e-04\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 561s 281ms/step - loss: 0.3431 - accuracy: 0.8463 - val_loss: 0.4061 - val_accuracy: 0.8190 - lr: 2.5000e-04\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 489s 244ms/step - loss: 0.3405 - accuracy: 0.8471 - val_loss: 0.4030 - val_accuracy: 0.8195 - lr: 2.5000e-04\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 531s 265ms/step - loss: 0.3358 - accuracy: 0.8503 - val_loss: 0.4097 - val_accuracy: 0.8187 - lr: 1.2500e-04\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 533s 267ms/step - loss: 0.3340 - accuracy: 0.8514 - val_loss: 0.4078 - val_accuracy: 0.8199 - lr: 1.2500e-04\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 498s 249ms/step - loss: 0.3333 - accuracy: 0.8507 - val_loss: 0.4078 - val_accuracy: 0.8194 - lr: 6.2500e-05\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 533s 267ms/step - loss: 0.3307 - accuracy: 0.8526 - val_loss: 0.4104 - val_accuracy: 0.8201 - lr: 6.2500e-05\n",
      "1000/1000 [==============================] - 72s 72ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/23 04:41:45 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/23 04:41:45 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpi_stprq1\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpi_stprq1\\model\\data\\model\\assets\n",
      "2025/09/23 04:42:07 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Running test with Bi-LSTM\")\n",
    "rnn_layer_experiment_bi(\"Bi-LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d7ea798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Successfully registered model 'bilstm_best'.\n",
      "2025/09/23 04:42:52 WARNING mlflow.tracking._model_registry.fluent: Run with id 54b89645ebab457780a374fdf83b5ef5 has no artifacts at artifact path 'model', registering model based on models:/m-300d21ff50d94271a49fbeae0dd719c4 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run ID: 54b89645ebab457780a374fdf83b5ef5 with metrics:\n",
      "Accuracy: 0.8191875\n",
      "F1_negatif: 0.822613281010485\n",
      "F1_positif: 0.8156267924287808\n",
      "Precision_negatif: 0.8073173667107956\n",
      "Precision_positif: 0.8320114419451307\n",
      "Recall_negatif: 0.8385\n",
      "Recall_positif: 0.799875\n",
      "ROC_AUC: 0.9025231015624999\n",
      "Best run parameters:\n",
      "embedding_name: Glove_twitter_200\n",
      "epochs: 50\n",
      "latent_dim: 200\n",
      "learning_rate: 0.001\n",
      "max_len: 50\n",
      "min_count: 2\n",
      "num_words: 30000\n",
      "rnn_layer_name: Bi-LSTM\n",
      "rnn_size: 128\n",
      "stemmer: None\n",
      "Setting tag embedding_name = Glove_twitter_200 in registered model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'bilstm_best'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting tag epochs = 50 in registered model\n",
      "Setting tag latent_dim = 200 in registered model\n",
      "Setting tag learning_rate = 0.001 in registered model\n",
      "Setting tag max_len = 50 in registered model\n",
      "Setting tag min_count = 2 in registered model\n",
      "Setting tag num_words = 30000 in registered model\n",
      "Setting tag rnn_layer_name = Bi-LSTM in registered model\n",
      "Setting tag rnn_size = 128 in registered model\n",
      "Setting tag stemmer = None in registered model\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")\n",
    "experiment_id = mlflow.get_experiment_by_name(\"bilstm_experiment\").experiment_id\n",
    "runs = client.search_runs(experiment_id)\n",
    "\n",
    "# Métrique pour sélectionner le meilleur modèle\n",
    "metric_to_optimize = \"Accuracy\" # liste des métriques enregistrées dans postprocess_data.py ou sur l'UI MLflow\n",
    "best_run = max(runs, key=lambda run: run.data.metrics.get(metric_to_optimize, float('-inf')))\n",
    "print(f\"Best run ID: {best_run.info.run_id} with metrics:\")\n",
    "for key, value in best_run.data.metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Best run parameters:\")\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Enregistrement du meilleur modèle\n",
    "best_model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "registered_model_name = \"bilstm_best\"\n",
    "registered_model = mlflow.register_model(best_model_uri, registered_model_name)\n",
    "# Enregistrement des paramètres sous forme de tags dans le modèle enregistré\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"Setting tag {key} = {value} in registered model\")\n",
    "    client.set_model_version_tag(\n",
    "        name=registered_model_name,\n",
    "        version=str(registered_model.version),\n",
    "        key=str(key),\n",
    "        value=str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31e9bd5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exp_models/bilstm_tokenizer.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# sauvegarde\n",
    "joblib.dump(tokenizer, \"exp_models/bilstm_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a173d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "6000d608-179f-4307-9446-917deff355dc",
       "rows": [
        [
         "5523",
         "Bit disappointed to find out at  BSL last night that there is no sign for 'otter' "
        ],
        [
         "146829",
         "The count down begins...10 days left in Europe! "
        ],
        [
         "46086",
         "Aw no izzy from greys is dieing. She's one of my fav characters  hopefully they find some miracle cure before it's too late!"
        ],
        [
         "42276",
         "@PaulPunktastic urgh im working again today till 4  sucks cause it means im too tired to try and do anything else  what bout you?"
        ],
        [
         "85720",
         "Photo: fatalattraction: Â Lol yess, the good girl  http://tumblr.com/xeo1wxu92"
        ],
        [
         "105337",
         "Taking my daughter to the movies. Eating ice cream while we wait. Very tasty too "
        ],
        [
         "150016",
         "@pedrobarra hmmm, you might be right... but I want reaaal uplifting TRANCE "
        ],
        [
         "53019",
         "Sicky baby needs to feel better and not cry "
        ],
        [
         "119736",
         "@frphoto pictures of your bliss?  In JPEG format?  Sorry, it's message board terminology and u don't play w/us on the boards "
        ],
        [
         "81086",
         "@latortuga Come to Seattle so we can 'rawk out' more. Also, I accidentally replied to myself "
        ],
        [
         "46908",
         "Off to north bay to look at apartments .. 10 hours in the car today "
        ],
        [
         "2075",
         "@iamjersey kids in glass houses! good choice "
        ],
        [
         "32562",
         "@zombietwitch I like your life. i used to have that life too "
        ],
        [
         "121469",
         "@IAMJREAL I wanna see you monday  is that possible?? haha"
        ],
        [
         "3114",
         "@LittleMissNat think it's the weather, I've had an off day too! Did you get your nail varnish? "
        ],
        [
         "138789",
         "Getting the suncream out - the sprogs are doing Race for Life today, and me + OH + Charlie are going out to support "
        ],
        [
         "68790",
         "Have you hugged someone today? Giving YOU a BIG HUG! "
        ],
        [
         "46609",
         "Know she's busy a lot but would be so happy if 1 day Miley responded to my tweets. Especially about the music I sent to her. Until then "
        ],
        [
         "98488",
         "i forgot my phone AND my purse today  so no txts or food for me "
        ],
        [
         "14515",
         "god shaun's still at the top of the trending topics list. how come bradie disappeared off it so fast?  and only like 2 more tweets til 300"
        ],
        [
         "85021",
         "just realized i have no money at all  but it has never stopped me before ... so why should it now. although i had a credit card back then!"
        ],
        [
         "25357",
         "@MrsOvenMitt Haha thank you  "
        ],
        [
         "143303",
         "Just spent from 4 til 1130 working on a school project on campus  now has a 10 pg paper and final monday "
        ],
        [
         "151340",
         "heading to nashville with @andrewbelle and his girl! excited to meet my potential new home "
        ],
        [
         "58564",
         "@hyperbomb ... your brother sounds like me. "
        ],
        [
         "31829",
         "have a good day everyonr "
        ],
        [
         "155522",
         "just got home from jades and now im writing her a BFF love letter  &lt;3"
        ],
        [
         "159974",
         "Still sick this sucks  "
        ],
        [
         "65220",
         "@ilanalynn why hello  there "
        ],
        [
         "8991",
         "omg its storming. thundering very loudly. "
        ],
        [
         "1910",
         "@michelkesterson heh, probably not. But I can work remotely "
        ],
        [
         "113232",
         "@bigname happy birthday to you ! "
        ],
        [
         "154856",
         "@boskabout Kapotlachen!  #microsoftshouldchange #l #attitude"
        ],
        [
         "72204",
         "@ciqua Well, it's great we can support each other.  "
        ],
        [
         "64726",
         "@thebeerrun i think i left my camera in the cab "
        ],
        [
         "18418",
         "sitting here drinking a screwdriver and watching season 1 of gossip girl  i love summer"
        ],
        [
         "125468",
         "open office being silly today. keeps crashing me out #wastedtime "
        ],
        [
         "115252",
         "@kieronjames oh yes sir. bit of a gathering kicking off here tonight "
        ],
        [
         "37641",
         "Picked up Luna from the vet. She looks like she's been crying  I'm not kidding. Her lashes are wet and she has tear trails. My poor bebe!"
        ],
        [
         "96569",
         "wooo chix rice! "
        ],
        [
         "110466",
         "No BIG Jon moments. But he waa in GREAT spirits. Did the butt dance w/Joe. "
        ],
        [
         "130053",
         "Syncrisis is playing at ANTISOCIAL at the social tonight.  Can't wait!  it's free come out!  follow my night @antisocialparty if u want "
        ],
        [
         "64399",
         "i just LOVE editing.when i have nothing else to do, it saves me from boredom. lol! "
        ],
        [
         "36689",
         "Gone Phishin'.... "
        ],
        [
         "75678",
         "@live_yush Haha, funny ! You are really going dumb aren't u. I will still get them when i switch on "
        ],
        [
         "15131",
         "Ticketmaster have emailed me the times of the acts for Saturday  can't wait. Free Peace, Twisted Wheel, The Enemy, Kasabian and Oasis "
        ],
        [
         "15224",
         "@saraparker me? cute? pleaseeeee... just telling the truth! Ps. i lost 5kg in almost 4 weeks!But i hope 8 more... "
        ],
        [
         "69254",
         "does anyone else miss chatting in chat rooms? I do  but can't find  one i feel comfortable in suggestions please?"
        ],
        [
         "37229",
         "Yummy...dairy queen "
        ],
        [
         "102183",
         "@nellgwynne No, I have not. And my CPU here at the office does not have speakers   I'll listen when I get home."
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 32000
       }
      },
      "text/plain": [
       "5523      Bit disappointed to find out at  BSL last nigh...\n",
       "146829     The count down begins...10 days left in Europe! \n",
       "46086     Aw no izzy from greys is dieing. She's one of ...\n",
       "42276     @PaulPunktastic urgh im working again today ti...\n",
       "85720     Photo: fatalattraction: Â Lol yess, the good g...\n",
       "                                ...                        \n",
       "69398     So... I'm going to spin class tonight. I'm gua...\n",
       "60318                           Visiting popstaronline.com \n",
       "29232                         PICTURE STILL NOT WORKING!!! \n",
       "100704    waiting for Chelsea to get here. Her flight wa...\n",
       "56427     @BroadwayBlue ikr!!!! i miss our ducks so much.  \n",
       "Name: text, Length: 32000, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b533c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env_P7_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
