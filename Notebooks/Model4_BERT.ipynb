{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c92577",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8616ccf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\tensorflow_hub\\__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n",
      "c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "Num GPUs Available:  1\n",
      "GPUs disponibles : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Version TF : 2.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, SimpleRNN, Dense, LSTM, Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing\n",
    "from mlflow import MlflowClient\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "cwd = Path.cwd()\n",
    "parent = cwd.parent\n",
    "sys.path.append(str(parent))\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\",\"Source\")))\n",
    "\n",
    "mlruns_path = Path(\"../mlruns\").resolve() \n",
    "mlflow_uri = mlruns_path.as_uri()\n",
    "mlflow.set_tracking_uri(mlflow_uri)\n",
    "\n",
    "from preprocess_data import *  ## import all functions from preprocess_data.py\n",
    "from postprocess_data import * ## import all functions from postprocess_data.py\n",
    "from utils import *  ## import all functions from utils.py\n",
    "\n",
    "# import nltk\n",
    "# import optuna\n",
    "# ü§ó\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "# from nltk.corpus import stopwords  \n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "nw = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "\n",
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")\n",
    "os.environ[\"TF_KERAS\"]='1'\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPUs disponibles :\", tf.config.list_physical_devices(\"GPU\"))\n",
    "print(\"Version TF :\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f45b2",
   "metadata": {},
   "source": [
    "# Pr√©paration data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4e0f0a",
   "metadata": {},
   "source": [
    "## Importation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "196d7d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 32000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\data\\dataset_source_registry.py:148: UserWarning: Failed to determine whether UCVolumeDatasetSource can resolve source information for '../Data/raw_data.csv'. Exception: \n",
      "  return _dataset_source_registry.resolve(\n",
      "c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\data\\dataset_source_registry.py:148: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n",
      "  return _dataset_source_registry.resolve(\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/AI+Engineer/Project+7%C2%A0-+D%C3%A9tectez+les+Bad+Buzz+gr%C3%A2ce+au+Deep+Learning/sentiment140.zip',\n",
    "                header=None,\n",
    "                compression='zip',\n",
    "                encoding='cp1252')\n",
    "\n",
    "df.columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "data_size = 0.02\n",
    "sample_df, _ = train_test_split(df, test_size=1-data_size, random_state=42, stratify=df['target'])\n",
    "sample_df = sample_df.reset_index(drop=True)\n",
    "print(f\"Sample size: {sample_df.shape[0]} rows\")\n",
    "data_numrows = sample_df.shape[0]\n",
    "# On ne garde que les colonnes 'target' et 'text'\n",
    "sample_df = sample_df[['target', 'text']]\n",
    "sample_df[\"target\"] = sample_df[\"target\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "sample_df.to_csv('../Data/raw_data.csv', index=False)\n",
    "dataset = mlflow.data.from_pandas(\n",
    "    sample_df,\n",
    "    source=\"../Data/raw_data.csv\",\n",
    "    name=\"dataset_v1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca7f92",
   "metadata": {},
   "source": [
    "## Train/Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c0ba358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X_raw = sample_df['text']\n",
    "y = sample_df['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_raw, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d88e673",
   "metadata": {},
   "source": [
    "# Mod√®les BERT classiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b8e27f",
   "metadata": {},
   "source": [
    "## Fonction centrale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ad5576",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_size = 128\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "def test_bert_model(bert_model_name):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_input(dataset)\n",
    "        mlflow.log_params(params={\n",
    "            'rnn_size': rnn_size, \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr,\n",
    "            'bert_model_name':bert_model_name\n",
    "        })\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "        encodings_train = tokenizer(X_train.to_list(), \n",
    "                                    truncation=True, \n",
    "                                    padding=True, \n",
    "                                    max_length=64,\n",
    "                                    return_tensors=\"tf\")\n",
    "        encodings_val = tokenizer(X_val.to_list(), \n",
    "                                  truncation=True, \n",
    "                                  padding=True, \n",
    "                                  max_length = 64,\n",
    "                                  return_tensors=\"tf\")\n",
    "\n",
    "        dataset_train = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                {\"input_ids\": encodings_train[\"input_ids\"], \n",
    "                 \"attention_mask\": encodings_train[\"attention_mask\"]\n",
    "                 },y_train\n",
    "                )\n",
    "                ).batch(32)\n",
    "        \n",
    "        dataset_val = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                {\"input_ids\": encodings_val[\"input_ids\"], \n",
    "                 \"attention_mask\": encodings_val[\"attention_mask\"]\n",
    "                 },y_val\n",
    "                )\n",
    "                ).batch(32)\n",
    "        # On charge le mod√®le pr√©-entrainn√©\n",
    "        base_model = TFAutoModel.from_pretrained(bert_model_name, from_pt=True)\n",
    "        base_model.trainable = False # Pas de fine-tuning ou d'entrainement car impossible √† faire avec les ressources disponibles\n",
    "\n",
    "        # Construction du mod√®le keras \n",
    "        ## Une input layer pour les input ids\n",
    "        input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "        ## Une input layer pour le masque d'attention\n",
    "        attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "        ## On r√©cup√®re \n",
    "\n",
    "        outputs = base_model(input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n",
    "\n",
    "        # On prend le token [CLS] comme vecteur de phrase\n",
    "        cls_token = token_embeddings[:, 0, :]\n",
    "        max_tokens = tf.reduce_max(token_embeddings, axis=1)  #\n",
    "        mean_tokens = tf.reduce_mean(token_embeddings, axis=1)  #\n",
    "\n",
    "        all_tokens = tf.concat([cls_token, max_tokens, mean_tokens], axis=-1)  # [batch, hidden_size*2]\n",
    "    \n",
    "        x = tf.keras.layers.Dense(rnn_size, activation=\"relu\", \n",
    "                                  kernel_regularizer=regularizers.L2(1e-4),\n",
    "                                  bias_regularizer=regularizers.L2(1e-4), \n",
    "                                #   activity_regularizer=regularizers.L2(1e-4),\n",
    "                                  )(all_tokens)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        logits = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "        model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=logits)\n",
    "\n",
    "        ## Callbacks\n",
    "        model_savepath = f\"./Models/MY_{'_'.join(bert_model_name.split('/'))}_dense{rnn_size}.h5\"\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=True, mode='max')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=20)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "\n",
    "\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "        # Summary\n",
    "        model.summary()\n",
    "        # History\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(dataset_train, epochs=epochs, batch_size=64, validation_data=dataset_val, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "                # Pr√©dictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(dataset_val)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des m√©triques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du mod√®le dans MLflow\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b67ee9",
   "metadata": {},
   "source": [
    "## Experiment MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b09a8d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MLflow experiment...\n"
     ]
    }
   ],
   "source": [
    "# Cr√©ation de l'√©tude Optuna et optimisation\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"BERT_models_experiment\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"BERT_models_experiment\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Comparaison de plusieurs mod√®les BERT\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"BERT_pretrained\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4ce314",
   "metadata": {},
   "source": [
    "## Liste des candidats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68246c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_list = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"bert-base-cased\",\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"distilbert-base-cased\",\n",
    "    \"bert-base-multilingual-uncased\",\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    \"distilroberta-base\",\n",
    "    \"xlm-roberta-base\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c722ca",
   "metadata": {},
   "source": [
    "## Lancement experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbb5c1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 68s 85ms/step - loss: 0.6120 - accuracy: 0.6503 - val_loss: 0.5641 - val_accuracy: 0.7522 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 65s 82ms/step - loss: 0.6136 - accuracy: 0.6568 - val_loss: 0.5765 - val_accuracy: 0.7514 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 66s 83ms/step - loss: 0.6035 - accuracy: 0.6583 - val_loss: 0.5728 - val_accuracy: 0.7402 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 67s 84ms/step - loss: 0.6046 - accuracy: 0.6568 - val_loss: 0.5642 - val_accuracy: 0.7381 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 78s 98ms/step - loss: 0.6038 - accuracy: 0.6547 - val_loss: 0.5571 - val_accuracy: 0.7423 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 78s 97ms/step - loss: 0.6044 - accuracy: 0.6566 - val_loss: 0.5556 - val_accuracy: 0.7544 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 77s 96ms/step - loss: 0.6128 - accuracy: 0.6532 - val_loss: 0.5532 - val_accuracy: 0.7494 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 79s 98ms/step - loss: 0.6022 - accuracy: 0.6604 - val_loss: 0.5582 - val_accuracy: 0.7558 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 79s 99ms/step - loss: 0.5999 - accuracy: 0.6636 - val_loss: 0.5524 - val_accuracy: 0.7513 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 89s 111ms/step - loss: 0.6022 - accuracy: 0.6596 - val_loss: 0.5779 - val_accuracy: 0.7642 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 79s 99ms/step - loss: 0.6022 - accuracy: 0.6586 - val_loss: 0.5652 - val_accuracy: 0.7458 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 78s 97ms/step - loss: 0.6029 - accuracy: 0.6591 - val_loss: 0.5545 - val_accuracy: 0.7550 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 78s 97ms/step - loss: 0.6078 - accuracy: 0.6604 - val_loss: 0.5524 - val_accuracy: 0.7605 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 70s 88ms/step - loss: 0.5993 - accuracy: 0.6660 - val_loss: 0.5565 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 65s 82ms/step - loss: 0.6042 - accuracy: 0.6604 - val_loss: 0.5601 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 66s 83ms/step - loss: 0.6030 - accuracy: 0.6594 - val_loss: 0.5621 - val_accuracy: 0.7622 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.6045 - accuracy: 0.6570 - val_loss: 0.5719 - val_accuracy: 0.7220 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.6048 - accuracy: 0.6634 - val_loss: 0.5524 - val_accuracy: 0.7616 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 67s 84ms/step - loss: 0.6024 - accuracy: 0.6582 - val_loss: 0.5529 - val_accuracy: 0.7641 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.5927 - accuracy: 0.6727 - val_loss: 0.5658 - val_accuracy: 0.7550 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.5948 - accuracy: 0.6661 - val_loss: 0.5536 - val_accuracy: 0.7620 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 66s 83ms/step - loss: 0.5931 - accuracy: 0.6693 - val_loss: 0.5494 - val_accuracy: 0.7614 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.5933 - accuracy: 0.6662 - val_loss: 0.5494 - val_accuracy: 0.7627 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 76s 95ms/step - loss: 0.5905 - accuracy: 0.6651 - val_loss: 0.5544 - val_accuracy: 0.7652 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 67s 84ms/step - loss: 0.5909 - accuracy: 0.6681 - val_loss: 0.5552 - val_accuracy: 0.7627 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 66s 83ms/step - loss: 0.5940 - accuracy: 0.6638 - val_loss: 0.5584 - val_accuracy: 0.7627 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.5919 - accuracy: 0.6679 - val_loss: 0.5608 - val_accuracy: 0.7608 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 65s 82ms/step - loss: 0.5897 - accuracy: 0.6680 - val_loss: 0.5545 - val_accuracy: 0.7580 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 65s 81ms/step - loss: 0.5888 - accuracy: 0.6676 - val_loss: 0.5587 - val_accuracy: 0.7638 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 76s 96ms/step - loss: 0.5959 - accuracy: 0.6664 - val_loss: 0.5552 - val_accuracy: 0.7656 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 67s 84ms/step - loss: 0.5932 - accuracy: 0.6659 - val_loss: 0.5718 - val_accuracy: 0.7542 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 67s 83ms/step - loss: 0.5928 - accuracy: 0.6679 - val_loss: 0.5623 - val_accuracy: 0.7653 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 68s 85ms/step - loss: 0.5882 - accuracy: 0.6728 - val_loss: 0.5658 - val_accuracy: 0.7627 - lr: 2.5000e-04\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.5880 - accuracy: 0.6776 - val_loss: 0.5480 - val_accuracy: 0.7639 - lr: 2.5000e-04\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 65s 81ms/step - loss: 0.5867 - accuracy: 0.6705 - val_loss: 0.5467 - val_accuracy: 0.7652 - lr: 2.5000e-04\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 65s 81ms/step - loss: 0.5870 - accuracy: 0.6752 - val_loss: 0.5444 - val_accuracy: 0.7645 - lr: 2.5000e-04\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 65s 81ms/step - loss: 0.5872 - accuracy: 0.6767 - val_loss: 0.5614 - val_accuracy: 0.7592 - lr: 2.5000e-04\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 65s 81ms/step - loss: 0.5879 - accuracy: 0.6697 - val_loss: 0.5561 - val_accuracy: 0.7583 - lr: 2.5000e-04\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 65s 81ms/step - loss: 0.5880 - accuracy: 0.6730 - val_loss: 0.5489 - val_accuracy: 0.7598 - lr: 2.5000e-04\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.5877 - accuracy: 0.6728 - val_loss: 0.5441 - val_accuracy: 0.7644 - lr: 2.5000e-04\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 67s 84ms/step - loss: 0.5848 - accuracy: 0.6744 - val_loss: 0.5426 - val_accuracy: 0.7609 - lr: 2.5000e-04\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 77s 96ms/step - loss: 0.5850 - accuracy: 0.6767 - val_loss: 0.5518 - val_accuracy: 0.7664 - lr: 2.5000e-04\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 67s 84ms/step - loss: 0.5846 - accuracy: 0.6752 - val_loss: 0.5472 - val_accuracy: 0.7594 - lr: 2.5000e-04\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.5851 - accuracy: 0.6730 - val_loss: 0.5489 - val_accuracy: 0.7631 - lr: 2.5000e-04\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 65s 81ms/step - loss: 0.5862 - accuracy: 0.6716 - val_loss: 0.5476 - val_accuracy: 0.7648 - lr: 2.5000e-04\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 65s 82ms/step - loss: 0.5827 - accuracy: 0.6775 - val_loss: 0.5476 - val_accuracy: 0.7638 - lr: 2.5000e-04\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 65s 81ms/step - loss: 0.5851 - accuracy: 0.6761 - val_loss: 0.5499 - val_accuracy: 0.7642 - lr: 2.5000e-04\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 65s 81ms/step - loss: 0.5866 - accuracy: 0.6721 - val_loss: 0.5479 - val_accuracy: 0.7633 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 68s 85ms/step - loss: 0.5855 - accuracy: 0.6746 - val_loss: 0.5457 - val_accuracy: 0.7616 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 67s 84ms/step - loss: 0.5825 - accuracy: 0.6745 - val_loss: 0.5430 - val_accuracy: 0.7652 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 76s 94ms/step - loss: 0.5850 - accuracy: 0.6732 - val_loss: 0.5357 - val_accuracy: 0.7680 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 68s 85ms/step - loss: 0.5834 - accuracy: 0.6746 - val_loss: 0.5375 - val_accuracy: 0.7648 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 69s 86ms/step - loss: 0.5816 - accuracy: 0.6772 - val_loss: 0.5362 - val_accuracy: 0.7617 - lr: 2.5000e-04\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 68s 85ms/step - loss: 0.5851 - accuracy: 0.6737 - val_loss: 0.5508 - val_accuracy: 0.7603 - lr: 2.5000e-04\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.5845 - accuracy: 0.6784 - val_loss: 0.5391 - val_accuracy: 0.7670 - lr: 2.5000e-04\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 69s 86ms/step - loss: 0.5838 - accuracy: 0.6739 - val_loss: 0.5414 - val_accuracy: 0.7652 - lr: 2.5000e-04\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 68s 85ms/step - loss: 0.5896 - accuracy: 0.6713 - val_loss: 0.5418 - val_accuracy: 0.7570 - lr: 2.5000e-04\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 69s 86ms/step - loss: 0.5824 - accuracy: 0.6728 - val_loss: 0.5360 - val_accuracy: 0.7659 - lr: 2.5000e-04\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 69s 87ms/step - loss: 0.5850 - accuracy: 0.6707 - val_loss: 0.5459 - val_accuracy: 0.7623 - lr: 2.5000e-04\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 70s 87ms/step - loss: 0.5831 - accuracy: 0.6795 - val_loss: 0.5390 - val_accuracy: 0.7630 - lr: 2.5000e-04\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.5849 - accuracy: 0.6767 - val_loss: 0.5416 - val_accuracy: 0.7644 - lr: 2.5000e-04\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.5816 - accuracy: 0.6757 - val_loss: 0.5412 - val_accuracy: 0.7689 - lr: 1.2500e-04\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.5832 - accuracy: 0.6762 - val_loss: 0.5318 - val_accuracy: 0.7692 - lr: 1.2500e-04\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 65s 81ms/step - loss: 0.5794 - accuracy: 0.6796 - val_loss: 0.5401 - val_accuracy: 0.7669 - lr: 1.2500e-04\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 65s 82ms/step - loss: 0.5831 - accuracy: 0.6738 - val_loss: 0.5329 - val_accuracy: 0.7634 - lr: 1.2500e-04\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 65s 81ms/step - loss: 0.5824 - accuracy: 0.6742 - val_loss: 0.5404 - val_accuracy: 0.7636 - lr: 1.2500e-04\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.5819 - accuracy: 0.6781 - val_loss: 0.5312 - val_accuracy: 0.7675 - lr: 1.2500e-04\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.5816 - accuracy: 0.6762 - val_loss: 0.5368 - val_accuracy: 0.7669 - lr: 1.2500e-04\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.5803 - accuracy: 0.6777 - val_loss: 0.5345 - val_accuracy: 0.7659 - lr: 1.2500e-04\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.5786 - accuracy: 0.6784 - val_loss: 0.5340 - val_accuracy: 0.7663 - lr: 1.2500e-04\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.5803 - accuracy: 0.6777 - val_loss: 0.5362 - val_accuracy: 0.7669 - lr: 1.2500e-04\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 65s 82ms/step - loss: 0.5791 - accuracy: 0.6801 - val_loss: 0.5367 - val_accuracy: 0.7680 - lr: 1.2500e-04\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 67s 84ms/step - loss: 0.5820 - accuracy: 0.6737 - val_loss: 0.5406 - val_accuracy: 0.7664 - lr: 1.2500e-04\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 68s 85ms/step - loss: 0.5804 - accuracy: 0.6794 - val_loss: 0.5328 - val_accuracy: 0.7686 - lr: 1.2500e-04\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 68s 85ms/step - loss: 0.5825 - accuracy: 0.6782 - val_loss: 0.5311 - val_accuracy: 0.7680 - lr: 1.2500e-04\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 65s 81ms/step - loss: 0.5805 - accuracy: 0.6754 - val_loss: 0.5411 - val_accuracy: 0.7677 - lr: 1.2500e-04\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.5793 - accuracy: 0.6760 - val_loss: 0.5407 - val_accuracy: 0.7614 - lr: 1.2500e-04\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 80s 99ms/step - loss: 0.5801 - accuracy: 0.6758 - val_loss: 0.5384 - val_accuracy: 0.7709 - lr: 1.2500e-04\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 69s 86ms/step - loss: 0.5807 - accuracy: 0.6770 - val_loss: 0.5363 - val_accuracy: 0.7697 - lr: 1.2500e-04\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 66s 83ms/step - loss: 0.5840 - accuracy: 0.6762 - val_loss: 0.5374 - val_accuracy: 0.7689 - lr: 1.2500e-04\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 76s 95ms/step - loss: 0.5808 - accuracy: 0.6786 - val_loss: 0.5377 - val_accuracy: 0.7711 - lr: 1.2500e-04\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 66s 83ms/step - loss: 0.5795 - accuracy: 0.6751 - val_loss: 0.5373 - val_accuracy: 0.7672 - lr: 1.2500e-04\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 118s 147ms/step - loss: 0.5791 - accuracy: 0.6793 - val_loss: 0.5397 - val_accuracy: 0.7709 - lr: 1.2500e-04\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 130s 163ms/step - loss: 0.5812 - accuracy: 0.6749 - val_loss: 0.5356 - val_accuracy: 0.7705 - lr: 1.2500e-04\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 141s 176ms/step - loss: 0.5802 - accuracy: 0.6808 - val_loss: 0.5355 - val_accuracy: 0.7669 - lr: 1.2500e-04\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 146s 183ms/step - loss: 0.5788 - accuracy: 0.6815 - val_loss: 0.5345 - val_accuracy: 0.7673 - lr: 6.2500e-05\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 143s 179ms/step - loss: 0.5761 - accuracy: 0.6794 - val_loss: 0.5366 - val_accuracy: 0.7692 - lr: 6.2500e-05\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 131s 164ms/step - loss: 0.5804 - accuracy: 0.6741 - val_loss: 0.5329 - val_accuracy: 0.7627 - lr: 6.2500e-05\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 131s 163ms/step - loss: 0.5814 - accuracy: 0.6755 - val_loss: 0.5443 - val_accuracy: 0.7664 - lr: 6.2500e-05\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 136s 171ms/step - loss: 0.5766 - accuracy: 0.6761 - val_loss: 0.5371 - val_accuracy: 0.7661 - lr: 6.2500e-05\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 132s 165ms/step - loss: 0.5783 - accuracy: 0.6809 - val_loss: 0.5350 - val_accuracy: 0.7681 - lr: 6.2500e-05\n",
      "200/200 [==============================] - 29s 121ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/07 13:09:48 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/10/07 13:09:48 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as serving, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmplo4ggctn\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmplo4ggctn\\model\\data\\model\\assets\n",
      "2025/10/07 13:11:11 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmplo4ggctn\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/10/07 13:11:11 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "for model_name in bert_model_list:\n",
    "    print(f\"Running test with BERT model : {model_name}\")\n",
    "    test_bert_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bcedc2",
   "metadata": {},
   "source": [
    "# Mod√®les BERT sp√©cifiques (HuggingFace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69613b3",
   "metadata": {},
   "source": [
    "## Fonction centrale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c57003d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_size = 128\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "def test_bert_model_other(bert_model_name):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_input(dataset)\n",
    "        mlflow.log_params(params={\n",
    "            'rnn_size': rnn_size, \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr,\n",
    "            'bert_model_name':bert_model_name\n",
    "        })\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "        encodings_train = tokenizer(X_train.to_list(), \n",
    "                                    truncation=True, \n",
    "                                    padding=True, \n",
    "                                    max_length=64,\n",
    "                                    return_tensors=\"tf\")\n",
    "        encodings_val = tokenizer(X_val.to_list(), \n",
    "                                  truncation=True, \n",
    "                                  padding=True, \n",
    "                                  max_length = 64,\n",
    "                                  return_tensors=\"tf\")\n",
    "\n",
    "        dataset_train = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                {\"input_ids\": encodings_train[\"input_ids\"], \n",
    "                 \"attention_mask\": encodings_train[\"attention_mask\"]\n",
    "                 },y_train\n",
    "                )\n",
    "                ).batch(32)\n",
    "        \n",
    "        dataset_val = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                {\"input_ids\": encodings_val[\"input_ids\"], \n",
    "                 \"attention_mask\": encodings_val[\"attention_mask\"]\n",
    "                 },y_val\n",
    "                )\n",
    "                ).batch(32)\n",
    "        # On charge le mod√®le pr√©-entrainn√©\n",
    "        try:\n",
    "            base_model = TFAutoModel.from_pretrained(bert_model_name, from_pt=False)# Forcer la version tensorflow\n",
    "            mlflow.log_param(\"from_pt\", False)\n",
    "        except:\n",
    "            print(bert_model_name, \" not available in TF\")\n",
    "            base_model = TFAutoModel.from_pretrained(bert_model_name,from_pt=True)\n",
    "            mlflow.log_param(\"from_pt\", True)\n",
    "\n",
    "            \n",
    "    \n",
    "        base_model.trainable = False # Pas de fine-tuning ou d'entrainement car impossible √† faire avec les ressources disponibles\n",
    "\n",
    "        # Construction du mod√®le keras \n",
    "        ## Une input layer pour les input ids\n",
    "        input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "        ## Une input layer pour le masque d'attention\n",
    "        attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "        ## On r√©cup√®re \n",
    "\n",
    "        outputs = base_model(input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n",
    "\n",
    "        # On prend le token [CLS] comme vecteur de phrase\n",
    "        cls_token = token_embeddings[:, 0, :]\n",
    "        max_tokens = tf.reduce_max(token_embeddings, axis=1)  #\n",
    "        mean_tokens = tf.reduce_mean(token_embeddings, axis=1)  #\n",
    "\n",
    "        all_tokens = tf.concat([cls_token, max_tokens, mean_tokens], axis=-1)  # [batch, hidden_size*2]\n",
    "    \n",
    "        x = tf.keras.layers.Dense(rnn_size, activation=\"relu\", \n",
    "                                  kernel_regularizer=regularizers.L2(1e-4),\n",
    "                                  bias_regularizer=regularizers.L2(1e-4), \n",
    "                                #   activity_regularizer=regularizers.L2(1e-4),\n",
    "                                  )(all_tokens)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        logits = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "        model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=logits)\n",
    "\n",
    "        ## Callbacks\n",
    "        model_savepath = f\"./Models/MY_{'_'.join(bert_model_name.split('/'))}_dense{rnn_size}.h5\"\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=True, mode='max')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=20)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "\n",
    "\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "        # Summary\n",
    "        model.summary()\n",
    "        # History\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(dataset_train, epochs=epochs, batch_size=64, validation_data=dataset_val, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "                # Pr√©dictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(dataset_val)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des m√©triques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du mod√®le dans MLflow\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c6e0d",
   "metadata": {},
   "source": [
    "## Experiment MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54605b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MLflow experiment...\n"
     ]
    }
   ],
   "source": [
    "# Cr√©ation de l'√©tude Optuna et optimisation\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"BERT_models_experiment\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"BERT_models_experiment\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Comparaison de plusieurs mod√®les BERT\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"BERT_pretrained_huggingface\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700c709",
   "metadata": {},
   "source": [
    "## Liste des candidats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93bc1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D'autrs mod√®les BERT √† tester (sp√©cialis√©s en analyse de sentiments)\n",
    "other_bert_models = [\n",
    "    \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"bhadresh-savani/roberta-base-emotion\",\n",
    "    \"gilf/english-yelp-sentiment\",\n",
    "    \"joeddav/distilbert-base-uncased-go-emotions-student\",\n",
    "    \"lordtt13/emo-mobilebert\",\n",
    "    \"Kapiche/twitter-roberta-base-sentiment\",\n",
    "    \"HARSHU550/Sentiments\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2e1024",
   "metadata": {},
   "source": [
    "## Lancement experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd97fffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 142s 178ms/step - loss: 0.5377 - accuracy: 0.7421 - val_loss: 0.5030 - val_accuracy: 0.7638 - lr: 5.0000e-04\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 132s 166ms/step - loss: 0.5315 - accuracy: 0.7456 - val_loss: 0.5140 - val_accuracy: 0.7550 - lr: 5.0000e-04\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 136s 170ms/step - loss: 0.5349 - accuracy: 0.7427 - val_loss: 0.5085 - val_accuracy: 0.7597 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 136s 170ms/step - loss: 0.5332 - accuracy: 0.7484 - val_loss: 0.5059 - val_accuracy: 0.7613 - lr: 5.0000e-04\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 135s 168ms/step - loss: 0.5308 - accuracy: 0.7479 - val_loss: 0.5066 - val_accuracy: 0.7625 - lr: 5.0000e-04\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 144s 180ms/step - loss: 0.5321 - accuracy: 0.7460 - val_loss: 0.5034 - val_accuracy: 0.7645 - lr: 5.0000e-04\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 133s 166ms/step - loss: 0.5332 - accuracy: 0.7442 - val_loss: 0.5023 - val_accuracy: 0.7623 - lr: 5.0000e-04\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 134s 168ms/step - loss: 0.5331 - accuracy: 0.7441 - val_loss: 0.5085 - val_accuracy: 0.7573 - lr: 5.0000e-04\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 137s 171ms/step - loss: 0.5332 - accuracy: 0.7456 - val_loss: 0.5032 - val_accuracy: 0.7616 - lr: 5.0000e-04\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 134s 167ms/step - loss: 0.5232 - accuracy: 0.7528 - val_loss: 0.5048 - val_accuracy: 0.7591 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 139s 174ms/step - loss: 0.5238 - accuracy: 0.7527 - val_loss: 0.5029 - val_accuracy: 0.7622 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 146s 183ms/step - loss: 0.5227 - accuracy: 0.7528 - val_loss: 0.4996 - val_accuracy: 0.7647 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 133s 166ms/step - loss: 0.5218 - accuracy: 0.7532 - val_loss: 0.5046 - val_accuracy: 0.7597 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 139s 173ms/step - loss: 0.5203 - accuracy: 0.7538 - val_loss: 0.5025 - val_accuracy: 0.7606 - lr: 2.5000e-04\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 139s 173ms/step - loss: 0.5217 - accuracy: 0.7532 - val_loss: 0.4989 - val_accuracy: 0.7655 - lr: 2.5000e-04\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 132s 165ms/step - loss: 0.5221 - accuracy: 0.7549 - val_loss: 0.4986 - val_accuracy: 0.7639 - lr: 2.5000e-04\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 138s 172ms/step - loss: 0.5212 - accuracy: 0.7525 - val_loss: 0.5011 - val_accuracy: 0.7650 - lr: 2.5000e-04\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 137s 172ms/step - loss: 0.5220 - accuracy: 0.7519 - val_loss: 0.4995 - val_accuracy: 0.7630 - lr: 2.5000e-04\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 143s 179ms/step - loss: 0.5233 - accuracy: 0.7509 - val_loss: 0.4996 - val_accuracy: 0.7663 - lr: 2.5000e-04\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 135s 169ms/step - loss: 0.5196 - accuracy: 0.7553 - val_loss: 0.4995 - val_accuracy: 0.7627 - lr: 2.5000e-04\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 135s 168ms/step - loss: 0.5211 - accuracy: 0.7534 - val_loss: 0.4989 - val_accuracy: 0.7627 - lr: 2.5000e-04\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 146s 183ms/step - loss: 0.5199 - accuracy: 0.7542 - val_loss: 0.4954 - val_accuracy: 0.7700 - lr: 2.5000e-04\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 139s 174ms/step - loss: 0.5200 - accuracy: 0.7544 - val_loss: 0.4954 - val_accuracy: 0.7675 - lr: 2.5000e-04\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 147s 183ms/step - loss: 0.5180 - accuracy: 0.7541 - val_loss: 0.4959 - val_accuracy: 0.7653 - lr: 2.5000e-04\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 145s 181ms/step - loss: 0.5191 - accuracy: 0.7517 - val_loss: 0.4972 - val_accuracy: 0.7631 - lr: 2.5000e-04\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 143s 179ms/step - loss: 0.5243 - accuracy: 0.7530 - val_loss: 0.4960 - val_accuracy: 0.7669 - lr: 2.5000e-04\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 145s 181ms/step - loss: 0.5167 - accuracy: 0.7552 - val_loss: 0.4952 - val_accuracy: 0.7667 - lr: 2.5000e-04\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 135s 169ms/step - loss: 0.5208 - accuracy: 0.7546 - val_loss: 0.4943 - val_accuracy: 0.7677 - lr: 2.5000e-04\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 142s 178ms/step - loss: 0.5198 - accuracy: 0.7555 - val_loss: 0.5020 - val_accuracy: 0.7628 - lr: 2.5000e-04\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 139s 173ms/step - loss: 0.5181 - accuracy: 0.7546 - val_loss: 0.4989 - val_accuracy: 0.7630 - lr: 2.5000e-04\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 139s 174ms/step - loss: 0.5191 - accuracy: 0.7524 - val_loss: 0.4952 - val_accuracy: 0.7655 - lr: 2.5000e-04\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 138s 173ms/step - loss: 0.5186 - accuracy: 0.7529 - val_loss: 0.4992 - val_accuracy: 0.7641 - lr: 2.5000e-04\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 135s 169ms/step - loss: 0.5156 - accuracy: 0.7567 - val_loss: 0.5035 - val_accuracy: 0.7614 - lr: 2.5000e-04\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 136s 170ms/step - loss: 0.5213 - accuracy: 0.7532 - val_loss: 0.4954 - val_accuracy: 0.7689 - lr: 2.5000e-04\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 153s 192ms/step - loss: 0.5175 - accuracy: 0.7534 - val_loss: 0.4931 - val_accuracy: 0.7683 - lr: 2.5000e-04\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 163s 204ms/step - loss: 0.5160 - accuracy: 0.7558 - val_loss: 0.4964 - val_accuracy: 0.7631 - lr: 2.5000e-04\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 172s 215ms/step - loss: 0.5160 - accuracy: 0.7537 - val_loss: 0.4960 - val_accuracy: 0.7650 - lr: 2.5000e-04\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 165s 207ms/step - loss: 0.5153 - accuracy: 0.7568 - val_loss: 0.4968 - val_accuracy: 0.7652 - lr: 2.5000e-04\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 167s 208ms/step - loss: 0.5206 - accuracy: 0.7526 - val_loss: 0.4961 - val_accuracy: 0.7627 - lr: 2.5000e-04\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 153s 191ms/step - loss: 0.5182 - accuracy: 0.7546 - val_loss: 0.4964 - val_accuracy: 0.7670 - lr: 2.5000e-04\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 151s 189ms/step - loss: 0.5157 - accuracy: 0.7519 - val_loss: 0.5021 - val_accuracy: 0.7608 - lr: 2.5000e-04\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 155s 194ms/step - loss: 0.5200 - accuracy: 0.7552 - val_loss: 0.4974 - val_accuracy: 0.7648 - lr: 2.5000e-04\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 146s 183ms/step - loss: 0.5168 - accuracy: 0.7582 - val_loss: 0.4939 - val_accuracy: 0.7655 - lr: 2.5000e-04\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 143s 179ms/step - loss: 0.5161 - accuracy: 0.7539 - val_loss: 0.4969 - val_accuracy: 0.7622 - lr: 2.5000e-04\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 150s 188ms/step - loss: 0.5170 - accuracy: 0.7558 - val_loss: 0.4953 - val_accuracy: 0.7647 - lr: 2.5000e-04\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 156s 195ms/step - loss: 0.5112 - accuracy: 0.7592 - val_loss: 0.4938 - val_accuracy: 0.7648 - lr: 1.2500e-04\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 146s 183ms/step - loss: 0.5105 - accuracy: 0.7602 - val_loss: 0.4940 - val_accuracy: 0.7656 - lr: 1.2500e-04\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 140s 174ms/step - loss: 0.5119 - accuracy: 0.7586 - val_loss: 0.4923 - val_accuracy: 0.7678 - lr: 1.2500e-04\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 134s 167ms/step - loss: 0.5132 - accuracy: 0.7565 - val_loss: 0.4930 - val_accuracy: 0.7678 - lr: 1.2500e-04\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 163s 205ms/step - loss: 0.5088 - accuracy: 0.7598 - val_loss: 0.4953 - val_accuracy: 0.7656 - lr: 1.2500e-04\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 153s 192ms/step - loss: 0.5087 - accuracy: 0.7611 - val_loss: 0.4937 - val_accuracy: 0.7677 - lr: 1.2500e-04\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 137s 171ms/step - loss: 0.5102 - accuracy: 0.7596 - val_loss: 0.4947 - val_accuracy: 0.7670 - lr: 1.2500e-04\n",
      "200/200 [==============================] - 27s 118ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/08 01:19:50 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/10/08 01:19:50 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpzfjfwu0i\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpzfjfwu0i\\model\\data\\model\\assets\n",
      "2025/10/08 01:21:05 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpzfjfwu0i\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/10/08 01:21:05 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with BERT model : joeddav/distilbert-base-uncased-go-emotions-student\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bassm\\.cache\\huggingface\\hub\\models--joeddav--distilbert-base-uncased-go-emotions-student. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joeddav/distilbert-base-uncased-go-emotions-student  not available in TF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_model_5 (TFDist  TFBaseModelOutput(l  66362880   ['input_ids[0][0]',              \n",
      " ilBertModel)                   ast_hidden_state=(N               'attention_mask[0][0]']         \n",
      "                                one, None, 768),                                                  \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_11 (S  (None, 768)         0           ['tf_distil_bert_model_5[0][0]'] \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_11 (TFOpLam  (None, 768)         0           ['tf_distil_bert_model_5[0][0]'] \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_11 (TFOpLa  (None, 768)         0           ['tf_distil_bert_model_5[0][0]'] \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.concat_11 (TFOpLambda)      (None, 2304)         0           ['tf.__operators__.getitem_11[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.math.reduce_max_11[0][0]',  \n",
      "                                                                  'tf.math.reduce_mean_11[0][0]'] \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 128)          295040      ['tf.concat_11[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_485 (Dropout)          (None, 128)          0           ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 1)            129         ['dropout_485[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,658,049\n",
      "Trainable params: 295,169\n",
      "Non-trainable params: 66,362,880\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 97s 114ms/step - loss: 0.5005 - accuracy: 0.7763 - val_loss: 0.4463 - val_accuracy: 0.7977 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 78s 97ms/step - loss: 0.4761 - accuracy: 0.7890 - val_loss: 0.4428 - val_accuracy: 0.8023 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 74s 93ms/step - loss: 0.4700 - accuracy: 0.7911 - val_loss: 0.4376 - val_accuracy: 0.8025 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.4660 - accuracy: 0.7957 - val_loss: 0.4345 - val_accuracy: 0.8042 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 90s 113ms/step - loss: 0.4661 - accuracy: 0.7957 - val_loss: 0.4388 - val_accuracy: 0.8019 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 75s 93ms/step - loss: 0.4609 - accuracy: 0.7973 - val_loss: 0.4409 - val_accuracy: 0.8008 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 57s 72ms/step - loss: 0.4608 - accuracy: 0.7969 - val_loss: 0.4370 - val_accuracy: 0.8044 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 71s 89ms/step - loss: 0.4586 - accuracy: 0.7970 - val_loss: 0.4367 - val_accuracy: 0.8069 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.4627 - accuracy: 0.7973 - val_loss: 0.4396 - val_accuracy: 0.8020 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.4593 - accuracy: 0.7982 - val_loss: 0.4343 - val_accuracy: 0.8083 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.4610 - accuracy: 0.7996 - val_loss: 0.4389 - val_accuracy: 0.8061 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 75s 93ms/step - loss: 0.4618 - accuracy: 0.7954 - val_loss: 0.4371 - val_accuracy: 0.8070 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 47s 59ms/step - loss: 0.4621 - accuracy: 0.7963 - val_loss: 0.4381 - val_accuracy: 0.8061 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4570 - accuracy: 0.8004 - val_loss: 0.4412 - val_accuracy: 0.8078 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 31s 38ms/step - loss: 0.4610 - accuracy: 0.7979 - val_loss: 0.4402 - val_accuracy: 0.8034 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 31s 38ms/step - loss: 0.4609 - accuracy: 0.7957 - val_loss: 0.4382 - val_accuracy: 0.8080 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 30s 38ms/step - loss: 0.4587 - accuracy: 0.7964 - val_loss: 0.4406 - val_accuracy: 0.8073 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 30s 38ms/step - loss: 0.4615 - accuracy: 0.7974 - val_loss: 0.4374 - val_accuracy: 0.8056 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 30s 38ms/step - loss: 0.4604 - accuracy: 0.7993 - val_loss: 0.4391 - val_accuracy: 0.8058 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 31s 38ms/step - loss: 0.4592 - accuracy: 0.8005 - val_loss: 0.4360 - val_accuracy: 0.8055 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 31s 38ms/step - loss: 0.4485 - accuracy: 0.8026 - val_loss: 0.4388 - val_accuracy: 0.8044 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 37s 46ms/step - loss: 0.4447 - accuracy: 0.8048 - val_loss: 0.4288 - val_accuracy: 0.8103 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 31s 38ms/step - loss: 0.4413 - accuracy: 0.8070 - val_loss: 0.4355 - val_accuracy: 0.8050 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 30s 38ms/step - loss: 0.4433 - accuracy: 0.8034 - val_loss: 0.4334 - val_accuracy: 0.8086 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 30s 38ms/step - loss: 0.4391 - accuracy: 0.8066 - val_loss: 0.4340 - val_accuracy: 0.8081 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 30s 38ms/step - loss: 0.4408 - accuracy: 0.8057 - val_loss: 0.4348 - val_accuracy: 0.8087 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 34s 43ms/step - loss: 0.4377 - accuracy: 0.8064 - val_loss: 0.4375 - val_accuracy: 0.8098 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 34s 43ms/step - loss: 0.4397 - accuracy: 0.8066 - val_loss: 0.4339 - val_accuracy: 0.8098 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 35s 44ms/step - loss: 0.4360 - accuracy: 0.8090 - val_loss: 0.4321 - val_accuracy: 0.8073 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 35s 44ms/step - loss: 0.4365 - accuracy: 0.8088 - val_loss: 0.4309 - val_accuracy: 0.8083 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 41s 51ms/step - loss: 0.4362 - accuracy: 0.8055 - val_loss: 0.4334 - val_accuracy: 0.8098 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 41s 52ms/step - loss: 0.4360 - accuracy: 0.8094 - val_loss: 0.4340 - val_accuracy: 0.8098 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 40s 50ms/step - loss: 0.4280 - accuracy: 0.8112 - val_loss: 0.4303 - val_accuracy: 0.8114 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 35s 44ms/step - loss: 0.4242 - accuracy: 0.8148 - val_loss: 0.4314 - val_accuracy: 0.8097 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 36s 45ms/step - loss: 0.4224 - accuracy: 0.8149 - val_loss: 0.4320 - val_accuracy: 0.8108 - lr: 2.5000e-04\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 34s 42ms/step - loss: 0.4231 - accuracy: 0.8136 - val_loss: 0.4326 - val_accuracy: 0.8089 - lr: 2.5000e-04\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 34s 43ms/step - loss: 0.4165 - accuracy: 0.8167 - val_loss: 0.4323 - val_accuracy: 0.8077 - lr: 2.5000e-04\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 34s 43ms/step - loss: 0.4169 - accuracy: 0.8168 - val_loss: 0.4334 - val_accuracy: 0.8106 - lr: 2.5000e-04\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 35s 43ms/step - loss: 0.4169 - accuracy: 0.8156 - val_loss: 0.4302 - val_accuracy: 0.8092 - lr: 2.5000e-04\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 35s 43ms/step - loss: 0.4176 - accuracy: 0.8154 - val_loss: 0.4313 - val_accuracy: 0.8102 - lr: 2.5000e-04\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 34s 43ms/step - loss: 0.4164 - accuracy: 0.8179 - val_loss: 0.4332 - val_accuracy: 0.8097 - lr: 2.5000e-04\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 35s 43ms/step - loss: 0.4163 - accuracy: 0.8170 - val_loss: 0.4394 - val_accuracy: 0.8072 - lr: 2.5000e-04\n",
      "200/200 [==============================] - 8s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/08 01:54:52 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/10/08 01:54:52 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000019153475870>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000019153475870>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000019153477C10>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000019153477C10>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001912C76E5F0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001912C76E5F0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001912C76F340>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001912C76F340>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000018F172B0400>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000018F172B0400>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001910428A7A0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001910428A7A0>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, transformer_layer_call_fn, transformer_layer_call_and_return_conditional_losses while saving (showing 5 of 165). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpn0ul_w41\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpn0ul_w41\\model\\data\\model\\assets\n",
      "2025/10/08 01:55:30 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpn0ul_w41\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/10/08 01:55:30 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with BERT model : lordtt13/emo-mobilebert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bassm\\.cache\\huggingface\\hub\\models--lordtt13--emo-mobilebert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lordtt13/emo-mobilebert  not available in TF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMobileBertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing TFMobileBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMobileBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFMobileBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMobileBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_mobile_bert_model_1 (TFMobi  TFBaseModelOutputWi  21195776   ['input_ids[0][0]',              \n",
      " leBertModel)                   thPooling(last_hidd               'attention_mask[0][0]']         \n",
      "                                en_state=(None, Non                                               \n",
      "                                e, 512),                                                          \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 512),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_12 (S  (None, 512)         0           ['tf_mobile_bert_model_1[0][0]'] \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_12 (TFOpLam  (None, 512)         0           ['tf_mobile_bert_model_1[0][0]'] \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_12 (TFOpLa  (None, 512)         0           ['tf_mobile_bert_model_1[0][0]'] \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.concat_12 (TFOpLambda)      (None, 1536)         0           ['tf.__operators__.getitem_12[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.math.reduce_max_12[0][0]',  \n",
      "                                                                  'tf.math.reduce_mean_12[0][0]'] \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 128)          196736      ['tf.concat_12[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_584 (Dropout)          (None, 128)          0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 1)            129         ['dropout_584[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,392,641\n",
      "Trainable params: 196,865\n",
      "Non-trainable params: 21,195,776\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 209s 221ms/step - loss: 0.7916 - accuracy: 0.5722 - val_loss: 0.6841 - val_accuracy: 0.5922 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 158s 197ms/step - loss: 0.6849 - accuracy: 0.5849 - val_loss: 0.6777 - val_accuracy: 0.5936 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 161s 202ms/step - loss: 0.6798 - accuracy: 0.5873 - val_loss: 0.6767 - val_accuracy: 0.5942 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 169s 211ms/step - loss: 0.6800 - accuracy: 0.5838 - val_loss: 0.6720 - val_accuracy: 0.5980 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 142s 178ms/step - loss: 0.6772 - accuracy: 0.5860 - val_loss: 0.6734 - val_accuracy: 0.5948 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 129s 162ms/step - loss: 0.6754 - accuracy: 0.5852 - val_loss: 0.6730 - val_accuracy: 0.5925 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 131s 164ms/step - loss: 0.6744 - accuracy: 0.5884 - val_loss: 0.6693 - val_accuracy: 0.5994 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 132s 165ms/step - loss: 0.6774 - accuracy: 0.5824 - val_loss: 0.6696 - val_accuracy: 0.6028 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 126s 157ms/step - loss: 0.6757 - accuracy: 0.5876 - val_loss: 0.6717 - val_accuracy: 0.5973 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 125s 157ms/step - loss: 0.6751 - accuracy: 0.5864 - val_loss: 0.6724 - val_accuracy: 0.5975 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 126s 158ms/step - loss: 0.6762 - accuracy: 0.5857 - val_loss: 0.6703 - val_accuracy: 0.6023 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 128s 160ms/step - loss: 0.6751 - accuracy: 0.5834 - val_loss: 0.6693 - val_accuracy: 0.5975 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 128s 160ms/step - loss: 0.6741 - accuracy: 0.5896 - val_loss: 0.6687 - val_accuracy: 0.5964 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 132s 165ms/step - loss: 0.6719 - accuracy: 0.5867 - val_loss: 0.6669 - val_accuracy: 0.6030 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 127s 158ms/step - loss: 0.6716 - accuracy: 0.5895 - val_loss: 0.6680 - val_accuracy: 0.5994 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 131s 164ms/step - loss: 0.6713 - accuracy: 0.5886 - val_loss: 0.6645 - val_accuracy: 0.6059 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 127s 159ms/step - loss: 0.6723 - accuracy: 0.5855 - val_loss: 0.6674 - val_accuracy: 0.6031 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 126s 157ms/step - loss: 0.6716 - accuracy: 0.5859 - val_loss: 0.6663 - val_accuracy: 0.6041 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 128s 159ms/step - loss: 0.6716 - accuracy: 0.5904 - val_loss: 0.6665 - val_accuracy: 0.6048 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 125s 156ms/step - loss: 0.6712 - accuracy: 0.5841 - val_loss: 0.6665 - val_accuracy: 0.5962 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 128s 160ms/step - loss: 0.6726 - accuracy: 0.5878 - val_loss: 0.6656 - val_accuracy: 0.6030 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6709 - accuracy: 0.5886 - val_loss: 0.6656 - val_accuracy: 0.6062 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 126s 158ms/step - loss: 0.6713 - accuracy: 0.5854 - val_loss: 0.6657 - val_accuracy: 0.6027 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 128s 160ms/step - loss: 0.6712 - accuracy: 0.5871 - val_loss: 0.6662 - val_accuracy: 0.6028 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 125s 157ms/step - loss: 0.6716 - accuracy: 0.5845 - val_loss: 0.6656 - val_accuracy: 0.6031 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 125s 157ms/step - loss: 0.6726 - accuracy: 0.5819 - val_loss: 0.6680 - val_accuracy: 0.6045 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 126s 157ms/step - loss: 0.6682 - accuracy: 0.5906 - val_loss: 0.6663 - val_accuracy: 0.6050 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 126s 158ms/step - loss: 0.6692 - accuracy: 0.5876 - val_loss: 0.6652 - val_accuracy: 0.6031 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6686 - accuracy: 0.5932 - val_loss: 0.6645 - val_accuracy: 0.6091 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 127s 158ms/step - loss: 0.6684 - accuracy: 0.5884 - val_loss: 0.6656 - val_accuracy: 0.6052 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6681 - accuracy: 0.5926 - val_loss: 0.6657 - val_accuracy: 0.6087 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 126s 158ms/step - loss: 0.6681 - accuracy: 0.5912 - val_loss: 0.6636 - val_accuracy: 0.6062 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 128s 160ms/step - loss: 0.6670 - accuracy: 0.5924 - val_loss: 0.6630 - val_accuracy: 0.6070 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 126s 158ms/step - loss: 0.6679 - accuracy: 0.5920 - val_loss: 0.6660 - val_accuracy: 0.6055 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 132s 165ms/step - loss: 0.6671 - accuracy: 0.5965 - val_loss: 0.6628 - val_accuracy: 0.6094 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6657 - accuracy: 0.5939 - val_loss: 0.6632 - val_accuracy: 0.6087 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6659 - accuracy: 0.5927 - val_loss: 0.6617 - val_accuracy: 0.6080 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 132s 165ms/step - loss: 0.6664 - accuracy: 0.5929 - val_loss: 0.6639 - val_accuracy: 0.6098 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6659 - accuracy: 0.5935 - val_loss: 0.6635 - val_accuracy: 0.6087 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 128s 160ms/step - loss: 0.6673 - accuracy: 0.5908 - val_loss: 0.6628 - val_accuracy: 0.6081 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 126s 157ms/step - loss: 0.6660 - accuracy: 0.5925 - val_loss: 0.6632 - val_accuracy: 0.6077 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 125s 157ms/step - loss: 0.6668 - accuracy: 0.5909 - val_loss: 0.6650 - val_accuracy: 0.6053 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 125s 157ms/step - loss: 0.6657 - accuracy: 0.5917 - val_loss: 0.6640 - val_accuracy: 0.6023 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6672 - accuracy: 0.5902 - val_loss: 0.6618 - val_accuracy: 0.6055 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6654 - accuracy: 0.5931 - val_loss: 0.6619 - val_accuracy: 0.6014 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6645 - accuracy: 0.5959 - val_loss: 0.6650 - val_accuracy: 0.6073 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 126s 158ms/step - loss: 0.6673 - accuracy: 0.5920 - val_loss: 0.6638 - val_accuracy: 0.6089 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 128s 160ms/step - loss: 0.6635 - accuracy: 0.5982 - val_loss: 0.6616 - val_accuracy: 0.6128 - lr: 2.5000e-04\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6637 - accuracy: 0.5948 - val_loss: 0.6601 - val_accuracy: 0.6112 - lr: 2.5000e-04\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6642 - accuracy: 0.5957 - val_loss: 0.6615 - val_accuracy: 0.6067 - lr: 2.5000e-04\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6636 - accuracy: 0.5982 - val_loss: 0.6617 - val_accuracy: 0.6095 - lr: 2.5000e-04\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 128s 160ms/step - loss: 0.6621 - accuracy: 0.6014 - val_loss: 0.6620 - val_accuracy: 0.6120 - lr: 2.5000e-04\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6636 - accuracy: 0.5991 - val_loss: 0.6596 - val_accuracy: 0.6131 - lr: 2.5000e-04\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 132s 165ms/step - loss: 0.6622 - accuracy: 0.5999 - val_loss: 0.6624 - val_accuracy: 0.6134 - lr: 2.5000e-04\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6637 - accuracy: 0.5939 - val_loss: 0.6608 - val_accuracy: 0.6094 - lr: 2.5000e-04\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 128s 161ms/step - loss: 0.6627 - accuracy: 0.5955 - val_loss: 0.6624 - val_accuracy: 0.6100 - lr: 2.5000e-04\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 131s 164ms/step - loss: 0.6626 - accuracy: 0.5966 - val_loss: 0.6596 - val_accuracy: 0.6137 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 128s 160ms/step - loss: 0.6614 - accuracy: 0.5979 - val_loss: 0.6605 - val_accuracy: 0.6125 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 132s 165ms/step - loss: 0.6621 - accuracy: 0.6025 - val_loss: 0.6615 - val_accuracy: 0.6139 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 128s 160ms/step - loss: 0.6625 - accuracy: 0.5967 - val_loss: 0.6617 - val_accuracy: 0.6083 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 127s 159ms/step - loss: 0.6610 - accuracy: 0.5993 - val_loss: 0.6624 - val_accuracy: 0.6117 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 126s 157ms/step - loss: 0.6618 - accuracy: 0.5980 - val_loss: 0.6589 - val_accuracy: 0.6134 - lr: 2.5000e-04\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6632 - accuracy: 0.5998 - val_loss: 0.6619 - val_accuracy: 0.6130 - lr: 2.5000e-04\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6616 - accuracy: 0.6014 - val_loss: 0.6599 - val_accuracy: 0.6114 - lr: 2.5000e-04\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6620 - accuracy: 0.6004 - val_loss: 0.6605 - val_accuracy: 0.6145 - lr: 2.5000e-04\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6623 - accuracy: 0.5962 - val_loss: 0.6613 - val_accuracy: 0.6127 - lr: 2.5000e-04\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 127s 159ms/step - loss: 0.6617 - accuracy: 0.6017 - val_loss: 0.6601 - val_accuracy: 0.6116 - lr: 2.5000e-04\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6617 - accuracy: 0.5982 - val_loss: 0.6599 - val_accuracy: 0.6108 - lr: 2.5000e-04\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 125s 156ms/step - loss: 0.6625 - accuracy: 0.6003 - val_loss: 0.6593 - val_accuracy: 0.6139 - lr: 2.5000e-04\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 126s 157ms/step - loss: 0.6617 - accuracy: 0.6012 - val_loss: 0.6615 - val_accuracy: 0.6142 - lr: 2.5000e-04\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 128s 160ms/step - loss: 0.6630 - accuracy: 0.5986 - val_loss: 0.6601 - val_accuracy: 0.6114 - lr: 2.5000e-04\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 125s 156ms/step - loss: 0.6636 - accuracy: 0.5954 - val_loss: 0.6599 - val_accuracy: 0.6087 - lr: 2.5000e-04\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 126s 157ms/step - loss: 0.6603 - accuracy: 0.6009 - val_loss: 0.6577 - val_accuracy: 0.6137 - lr: 1.2500e-04\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 126s 157ms/step - loss: 0.6611 - accuracy: 0.6027 - val_loss: 0.6582 - val_accuracy: 0.6123 - lr: 1.2500e-04\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 126s 158ms/step - loss: 0.6605 - accuracy: 0.5992 - val_loss: 0.6586 - val_accuracy: 0.6125 - lr: 1.2500e-04\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6591 - accuracy: 0.6028 - val_loss: 0.6578 - val_accuracy: 0.6123 - lr: 1.2500e-04\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6607 - accuracy: 0.5995 - val_loss: 0.6589 - val_accuracy: 0.6127 - lr: 1.2500e-04\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 131s 163ms/step - loss: 0.6600 - accuracy: 0.6042 - val_loss: 0.6584 - val_accuracy: 0.6153 - lr: 1.2500e-04\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 132s 164ms/step - loss: 0.6600 - accuracy: 0.6008 - val_loss: 0.6583 - val_accuracy: 0.6169 - lr: 1.2500e-04\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 128s 159ms/step - loss: 0.6583 - accuracy: 0.6055 - val_loss: 0.6582 - val_accuracy: 0.6159 - lr: 1.2500e-04\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 129s 162ms/step - loss: 0.6603 - accuracy: 0.6052 - val_loss: 0.6577 - val_accuracy: 0.6150 - lr: 1.2500e-04\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 127s 159ms/step - loss: 0.6591 - accuracy: 0.6025 - val_loss: 0.6573 - val_accuracy: 0.6148 - lr: 1.2500e-04\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6589 - accuracy: 0.6059 - val_loss: 0.6560 - val_accuracy: 0.6167 - lr: 1.2500e-04\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 132s 165ms/step - loss: 0.6586 - accuracy: 0.6068 - val_loss: 0.6560 - val_accuracy: 0.6170 - lr: 1.2500e-04\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6592 - accuracy: 0.6054 - val_loss: 0.6598 - val_accuracy: 0.6136 - lr: 1.2500e-04\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6585 - accuracy: 0.6048 - val_loss: 0.6572 - val_accuracy: 0.6139 - lr: 1.2500e-04\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 132s 165ms/step - loss: 0.6588 - accuracy: 0.6048 - val_loss: 0.6579 - val_accuracy: 0.6172 - lr: 1.2500e-04\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6587 - accuracy: 0.6052 - val_loss: 0.6578 - val_accuracy: 0.6153 - lr: 1.2500e-04\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6585 - accuracy: 0.6050 - val_loss: 0.6572 - val_accuracy: 0.6145 - lr: 1.2500e-04\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 126s 158ms/step - loss: 0.6587 - accuracy: 0.6037 - val_loss: 0.6577 - val_accuracy: 0.6161 - lr: 1.2500e-04\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6578 - accuracy: 0.6064 - val_loss: 0.6573 - val_accuracy: 0.6167 - lr: 1.2500e-04\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6597 - accuracy: 0.6014 - val_loss: 0.6586 - val_accuracy: 0.6128 - lr: 1.2500e-04\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 125s 156ms/step - loss: 0.6597 - accuracy: 0.6032 - val_loss: 0.6569 - val_accuracy: 0.6152 - lr: 1.2500e-04\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 131s 164ms/step - loss: 0.6577 - accuracy: 0.6055 - val_loss: 0.6557 - val_accuracy: 0.6187 - lr: 6.2500e-05\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6574 - accuracy: 0.6060 - val_loss: 0.6560 - val_accuracy: 0.6164 - lr: 6.2500e-05\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 128s 160ms/step - loss: 0.6571 - accuracy: 0.6087 - val_loss: 0.6556 - val_accuracy: 0.6142 - lr: 6.2500e-05\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 127s 158ms/step - loss: 0.6575 - accuracy: 0.6071 - val_loss: 0.6556 - val_accuracy: 0.6169 - lr: 6.2500e-05\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6576 - accuracy: 0.6054 - val_loss: 0.6561 - val_accuracy: 0.6162 - lr: 6.2500e-05\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.6572 - accuracy: 0.6097 - val_loss: 0.6557 - val_accuracy: 0.6150 - lr: 6.2500e-05\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 126s 158ms/step - loss: 0.6568 - accuracy: 0.6073 - val_loss: 0.6551 - val_accuracy: 0.6159 - lr: 6.2500e-05\n",
      "200/200 [==============================] - 36s 129ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/08 05:33:45 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/10/08 05:33:45 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 2127). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpnyp0mcba\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpnyp0mcba\\model\\data\\model\\assets\n",
      "2025/10/08 05:36:57 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpnyp0mcba\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/10/08 05:36:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with BERT model : Kapiche/twitter-roberta-base-sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bassm\\.cache\\huggingface\\hub\\models--Kapiche--twitter-roberta-base-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kapiche/twitter-roberta-base-sentiment  not available in TF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['roberta.embeddings.position_ids', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model_4 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, None                                               \n",
      "                                , 768),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_13 (S  (None, 768)         0           ['tf_roberta_model_4[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_13 (TFOpLam  (None, 768)         0           ['tf_roberta_model_4[0][0]']     \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_13 (TFOpLa  (None, 768)         0           ['tf_roberta_model_4[0][0]']     \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.concat_13 (TFOpLambda)      (None, 2304)         0           ['tf.__operators__.getitem_13[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.math.reduce_max_13[0][0]',  \n",
      "                                                                  'tf.math.reduce_mean_13[0][0]'] \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 128)          295040      ['tf.concat_13[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_659 (Dropout)          (None, 128)          0           ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 1)            129         ['dropout_659[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124,940,801\n",
      "Trainable params: 295,169\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 70s 78ms/step - loss: 0.5041 - accuracy: 0.7814 - val_loss: 0.4393 - val_accuracy: 0.8073 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 67s 83ms/step - loss: 0.4704 - accuracy: 0.7948 - val_loss: 0.4278 - val_accuracy: 0.8133 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 67s 83ms/step - loss: 0.4624 - accuracy: 0.7977 - val_loss: 0.4259 - val_accuracy: 0.8144 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 59s 73ms/step - loss: 0.4563 - accuracy: 0.8018 - val_loss: 0.4231 - val_accuracy: 0.8122 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 66s 83ms/step - loss: 0.4562 - accuracy: 0.8011 - val_loss: 0.4184 - val_accuracy: 0.8153 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 67s 84ms/step - loss: 0.4540 - accuracy: 0.8004 - val_loss: 0.4164 - val_accuracy: 0.8156 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 67s 84ms/step - loss: 0.4532 - accuracy: 0.8024 - val_loss: 0.4205 - val_accuracy: 0.8161 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 69s 85ms/step - loss: 0.4525 - accuracy: 0.8021 - val_loss: 0.4193 - val_accuracy: 0.8163 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 58s 72ms/step - loss: 0.4514 - accuracy: 0.8041 - val_loss: 0.4288 - val_accuracy: 0.8087 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 58s 72ms/step - loss: 0.4515 - accuracy: 0.8027 - val_loss: 0.4261 - val_accuracy: 0.8117 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 58s 72ms/step - loss: 0.4483 - accuracy: 0.8031 - val_loss: 0.4177 - val_accuracy: 0.8147 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 59s 73ms/step - loss: 0.4482 - accuracy: 0.8029 - val_loss: 0.4208 - val_accuracy: 0.8147 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.4512 - accuracy: 0.8036 - val_loss: 0.4225 - val_accuracy: 0.8164 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.4482 - accuracy: 0.8046 - val_loss: 0.4221 - val_accuracy: 0.8123 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 59s 73ms/step - loss: 0.4488 - accuracy: 0.8044 - val_loss: 0.4178 - val_accuracy: 0.8159 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 67s 83ms/step - loss: 0.4471 - accuracy: 0.8057 - val_loss: 0.4178 - val_accuracy: 0.8173 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 60s 74ms/step - loss: 0.4370 - accuracy: 0.8105 - val_loss: 0.4153 - val_accuracy: 0.8159 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 66s 83ms/step - loss: 0.4396 - accuracy: 0.8070 - val_loss: 0.4130 - val_accuracy: 0.8184 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4364 - accuracy: 0.8093 - val_loss: 0.4149 - val_accuracy: 0.8145 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 59s 73ms/step - loss: 0.4350 - accuracy: 0.8112 - val_loss: 0.4129 - val_accuracy: 0.8134 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 59s 73ms/step - loss: 0.4353 - accuracy: 0.8100 - val_loss: 0.4171 - val_accuracy: 0.8125 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.4347 - accuracy: 0.8108 - val_loss: 0.4148 - val_accuracy: 0.8134 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 66s 83ms/step - loss: 0.4345 - accuracy: 0.8096 - val_loss: 0.4118 - val_accuracy: 0.8230 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 60s 74ms/step - loss: 0.4325 - accuracy: 0.8099 - val_loss: 0.4134 - val_accuracy: 0.8177 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4358 - accuracy: 0.8104 - val_loss: 0.4091 - val_accuracy: 0.8184 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 58s 72ms/step - loss: 0.4322 - accuracy: 0.8105 - val_loss: 0.4103 - val_accuracy: 0.8180 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 59s 73ms/step - loss: 0.4298 - accuracy: 0.8110 - val_loss: 0.4112 - val_accuracy: 0.8191 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 58s 73ms/step - loss: 0.4325 - accuracy: 0.8112 - val_loss: 0.4109 - val_accuracy: 0.8164 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 58s 72ms/step - loss: 0.4307 - accuracy: 0.8104 - val_loss: 0.4111 - val_accuracy: 0.8188 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 58s 73ms/step - loss: 0.4309 - accuracy: 0.8103 - val_loss: 0.4139 - val_accuracy: 0.8147 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 58s 72ms/step - loss: 0.4326 - accuracy: 0.8108 - val_loss: 0.4156 - val_accuracy: 0.8172 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 58s 73ms/step - loss: 0.4304 - accuracy: 0.8126 - val_loss: 0.4093 - val_accuracy: 0.8194 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 58s 73ms/step - loss: 0.4313 - accuracy: 0.8089 - val_loss: 0.4096 - val_accuracy: 0.8213 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 58s 73ms/step - loss: 0.4322 - accuracy: 0.8113 - val_loss: 0.4087 - val_accuracy: 0.8197 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 58s 73ms/step - loss: 0.4319 - accuracy: 0.8112 - val_loss: 0.4130 - val_accuracy: 0.8144 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 58s 72ms/step - loss: 0.4330 - accuracy: 0.8105 - val_loss: 0.4117 - val_accuracy: 0.8181 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 58s 72ms/step - loss: 0.4311 - accuracy: 0.8105 - val_loss: 0.4137 - val_accuracy: 0.8209 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4303 - accuracy: 0.8130 - val_loss: 0.4136 - val_accuracy: 0.8189 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 60s 74ms/step - loss: 0.4286 - accuracy: 0.8134 - val_loss: 0.4129 - val_accuracy: 0.8148 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4293 - accuracy: 0.8123 - val_loss: 0.4113 - val_accuracy: 0.8225 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 59s 73ms/step - loss: 0.4281 - accuracy: 0.8115 - val_loss: 0.4143 - val_accuracy: 0.8188 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 59s 73ms/step - loss: 0.4320 - accuracy: 0.8112 - val_loss: 0.4127 - val_accuracy: 0.8173 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 58s 73ms/step - loss: 0.4301 - accuracy: 0.8131 - val_loss: 0.4116 - val_accuracy: 0.8197 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 58s 73ms/step - loss: 0.4287 - accuracy: 0.8107 - val_loss: 0.4083 - val_accuracy: 0.8217 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 58s 73ms/step - loss: 0.4299 - accuracy: 0.8118 - val_loss: 0.4136 - val_accuracy: 0.8152 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 59s 73ms/step - loss: 0.4292 - accuracy: 0.8118 - val_loss: 0.4174 - val_accuracy: 0.8081 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4312 - accuracy: 0.8116 - val_loss: 0.4168 - val_accuracy: 0.8172 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.4274 - accuracy: 0.8122 - val_loss: 0.4113 - val_accuracy: 0.8191 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4311 - accuracy: 0.8116 - val_loss: 0.4095 - val_accuracy: 0.8152 - lr: 5.0000e-04\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 59s 73ms/step - loss: 0.4276 - accuracy: 0.8112 - val_loss: 0.4135 - val_accuracy: 0.8178 - lr: 5.0000e-04\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 58s 73ms/step - loss: 0.4305 - accuracy: 0.8127 - val_loss: 0.4158 - val_accuracy: 0.8194 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4309 - accuracy: 0.8123 - val_loss: 0.4105 - val_accuracy: 0.8211 - lr: 5.0000e-04\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4307 - accuracy: 0.8131 - val_loss: 0.4131 - val_accuracy: 0.8134 - lr: 5.0000e-04\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4299 - accuracy: 0.8112 - val_loss: 0.4089 - val_accuracy: 0.8194 - lr: 5.0000e-04\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4211 - accuracy: 0.8145 - val_loss: 0.4135 - val_accuracy: 0.8156 - lr: 2.5000e-04\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.4207 - accuracy: 0.8164 - val_loss: 0.4152 - val_accuracy: 0.8184 - lr: 2.5000e-04\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.4217 - accuracy: 0.8168 - val_loss: 0.4118 - val_accuracy: 0.8191 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.4174 - accuracy: 0.8169 - val_loss: 0.4100 - val_accuracy: 0.8186 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 58s 73ms/step - loss: 0.4189 - accuracy: 0.8184 - val_loss: 0.4101 - val_accuracy: 0.8198 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 59s 73ms/step - loss: 0.4195 - accuracy: 0.8166 - val_loss: 0.4135 - val_accuracy: 0.8127 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4180 - accuracy: 0.8186 - val_loss: 0.4089 - val_accuracy: 0.8213 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 60s 74ms/step - loss: 0.4194 - accuracy: 0.8167 - val_loss: 0.4074 - val_accuracy: 0.8211 - lr: 2.5000e-04\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.4181 - accuracy: 0.8170 - val_loss: 0.4110 - val_accuracy: 0.8186 - lr: 2.5000e-04\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4177 - accuracy: 0.8177 - val_loss: 0.4111 - val_accuracy: 0.8191 - lr: 2.5000e-04\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4168 - accuracy: 0.8153 - val_loss: 0.4121 - val_accuracy: 0.8191 - lr: 2.5000e-04\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 59s 73ms/step - loss: 0.4171 - accuracy: 0.8174 - val_loss: 0.4082 - val_accuracy: 0.8198 - lr: 2.5000e-04\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4137 - accuracy: 0.8173 - val_loss: 0.4098 - val_accuracy: 0.8202 - lr: 2.5000e-04\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4132 - accuracy: 0.8213 - val_loss: 0.4068 - val_accuracy: 0.8200 - lr: 2.5000e-04\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 59s 73ms/step - loss: 0.4171 - accuracy: 0.8170 - val_loss: 0.4073 - val_accuracy: 0.8217 - lr: 2.5000e-04\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4171 - accuracy: 0.8198 - val_loss: 0.4092 - val_accuracy: 0.8220 - lr: 2.5000e-04\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4198 - accuracy: 0.8168 - val_loss: 0.4134 - val_accuracy: 0.8163 - lr: 2.5000e-04\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 58s 72ms/step - loss: 0.4168 - accuracy: 0.8182 - val_loss: 0.4110 - val_accuracy: 0.8203 - lr: 2.5000e-04\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4164 - accuracy: 0.8163 - val_loss: 0.4136 - val_accuracy: 0.8123 - lr: 2.5000e-04\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4149 - accuracy: 0.8193 - val_loss: 0.4074 - val_accuracy: 0.8209 - lr: 2.5000e-04\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 58s 73ms/step - loss: 0.4161 - accuracy: 0.8161 - val_loss: 0.4086 - val_accuracy: 0.8203 - lr: 2.5000e-04\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.4180 - accuracy: 0.8148 - val_loss: 0.4081 - val_accuracy: 0.8194 - lr: 2.5000e-04\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.4157 - accuracy: 0.8202 - val_loss: 0.4135 - val_accuracy: 0.8164 - lr: 2.5000e-04\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.4154 - accuracy: 0.8217 - val_loss: 0.4058 - val_accuracy: 0.8203 - lr: 2.5000e-04\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 58s 73ms/step - loss: 0.4154 - accuracy: 0.8190 - val_loss: 0.4116 - val_accuracy: 0.8197 - lr: 2.5000e-04\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 58s 73ms/step - loss: 0.4166 - accuracy: 0.8202 - val_loss: 0.4110 - val_accuracy: 0.8205 - lr: 2.5000e-04\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.4160 - accuracy: 0.8176 - val_loss: 0.4061 - val_accuracy: 0.8228 - lr: 2.5000e-04\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.4119 - accuracy: 0.8169 - val_loss: 0.4086 - val_accuracy: 0.8184 - lr: 2.5000e-04\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.4149 - accuracy: 0.8191 - val_loss: 0.4060 - val_accuracy: 0.8205 - lr: 2.5000e-04\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4134 - accuracy: 0.8191 - val_loss: 0.4061 - val_accuracy: 0.8194 - lr: 2.5000e-04\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 58s 72ms/step - loss: 0.4160 - accuracy: 0.8181 - val_loss: 0.4123 - val_accuracy: 0.8205 - lr: 2.5000e-04\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 59s 73ms/step - loss: 0.4144 - accuracy: 0.8212 - val_loss: 0.4103 - val_accuracy: 0.8194 - lr: 2.5000e-04\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4139 - accuracy: 0.8178 - val_loss: 0.4140 - val_accuracy: 0.8164 - lr: 2.5000e-04\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.4125 - accuracy: 0.8198 - val_loss: 0.4081 - val_accuracy: 0.8188 - lr: 2.5000e-04\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4052 - accuracy: 0.8227 - val_loss: 0.4064 - val_accuracy: 0.8227 - lr: 1.2500e-04\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 59s 73ms/step - loss: 0.4108 - accuracy: 0.8214 - val_loss: 0.4076 - val_accuracy: 0.8220 - lr: 1.2500e-04\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4077 - accuracy: 0.8229 - val_loss: 0.4059 - val_accuracy: 0.8223 - lr: 1.2500e-04\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 60s 74ms/step - loss: 0.4074 - accuracy: 0.8220 - val_loss: 0.4096 - val_accuracy: 0.8211 - lr: 1.2500e-04\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 67s 84ms/step - loss: 0.4050 - accuracy: 0.8261 - val_loss: 0.4039 - val_accuracy: 0.8234 - lr: 1.2500e-04\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.4060 - accuracy: 0.8232 - val_loss: 0.4072 - val_accuracy: 0.8209 - lr: 1.2500e-04\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.4090 - accuracy: 0.8199 - val_loss: 0.4073 - val_accuracy: 0.8214 - lr: 1.2500e-04\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 58s 73ms/step - loss: 0.4068 - accuracy: 0.8238 - val_loss: 0.4078 - val_accuracy: 0.8200 - lr: 1.2500e-04\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 58s 73ms/step - loss: 0.4053 - accuracy: 0.8221 - val_loss: 0.4109 - val_accuracy: 0.8180 - lr: 1.2500e-04\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 59s 73ms/step - loss: 0.4077 - accuracy: 0.8230 - val_loss: 0.4075 - val_accuracy: 0.8200 - lr: 1.2500e-04\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4081 - accuracy: 0.8243 - val_loss: 0.4047 - val_accuracy: 0.8211 - lr: 1.2500e-04\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 58s 72ms/step - loss: 0.4067 - accuracy: 0.8234 - val_loss: 0.4069 - val_accuracy: 0.8202 - lr: 1.2500e-04\n",
      "200/200 [==============================] - 14s 55ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/08 07:19:42 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/10/08 07:19:43 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as serving, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpch6agocs\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpch6agocs\\model\\data\\model\\assets\n",
      "2025/10/08 07:20:37 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpch6agocs\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/10/08 07:20:37 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with BERT model : HARSHU550/Sentiments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bassm\\.cache\\huggingface\\hub\\models--HARSHU550--Sentiments. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARSHU550/Sentiments  not available in TF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_model_7 (TFDist  TFBaseModelOutput(l  66362880   ['input_ids[0][0]',              \n",
      " ilBertModel)                   ast_hidden_state=(N               'attention_mask[0][0]']         \n",
      "                                one, None, 768),                                                  \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_14 (S  (None, 768)         0           ['tf_distil_bert_model_7[0][0]'] \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_14 (TFOpLam  (None, 768)         0           ['tf_distil_bert_model_7[0][0]'] \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_14 (TFOpLa  (None, 768)         0           ['tf_distil_bert_model_7[0][0]'] \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.concat_14 (TFOpLambda)      (None, 2304)         0           ['tf.__operators__.getitem_14[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.math.reduce_max_14[0][0]',  \n",
      "                                                                  'tf.math.reduce_mean_14[0][0]'] \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 128)          295040      ['tf.concat_14[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_698 (Dropout)          (None, 128)          0           ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 1)            129         ['dropout_698[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,658,049\n",
      "Trainable params: 295,169\n",
      "Non-trainable params: 66,362,880\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 41s 42ms/step - loss: 0.5580 - accuracy: 0.7394 - val_loss: 0.4916 - val_accuracy: 0.7706 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 38s 47ms/step - loss: 0.5254 - accuracy: 0.7562 - val_loss: 0.4789 - val_accuracy: 0.7794 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 37s 46ms/step - loss: 0.5161 - accuracy: 0.7601 - val_loss: 0.4738 - val_accuracy: 0.7816 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 32s 39ms/step - loss: 0.5141 - accuracy: 0.7604 - val_loss: 0.4739 - val_accuracy: 0.7766 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.5122 - accuracy: 0.7613 - val_loss: 0.4763 - val_accuracy: 0.7763 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.5125 - accuracy: 0.7650 - val_loss: 0.4713 - val_accuracy: 0.7795 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 37s 46ms/step - loss: 0.5084 - accuracy: 0.7638 - val_loss: 0.4716 - val_accuracy: 0.7841 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 32s 39ms/step - loss: 0.5063 - accuracy: 0.7657 - val_loss: 0.4789 - val_accuracy: 0.7714 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.5090 - accuracy: 0.7679 - val_loss: 0.4716 - val_accuracy: 0.7837 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 39s 49ms/step - loss: 0.5072 - accuracy: 0.7656 - val_loss: 0.4682 - val_accuracy: 0.7873 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.5058 - accuracy: 0.7684 - val_loss: 0.4700 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 37s 47ms/step - loss: 0.5037 - accuracy: 0.7643 - val_loss: 0.4714 - val_accuracy: 0.7877 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.5050 - accuracy: 0.7673 - val_loss: 0.4721 - val_accuracy: 0.7809 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.5037 - accuracy: 0.7687 - val_loss: 0.4718 - val_accuracy: 0.7867 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.5045 - accuracy: 0.7693 - val_loss: 0.4704 - val_accuracy: 0.7823 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.5054 - accuracy: 0.7697 - val_loss: 0.4712 - val_accuracy: 0.7823 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 37s 46ms/step - loss: 0.5043 - accuracy: 0.7676 - val_loss: 0.4698 - val_accuracy: 0.7884 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.5079 - accuracy: 0.7691 - val_loss: 0.4730 - val_accuracy: 0.7855 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.5009 - accuracy: 0.7734 - val_loss: 0.4681 - val_accuracy: 0.7855 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 32s 39ms/step - loss: 0.5048 - accuracy: 0.7696 - val_loss: 0.4686 - val_accuracy: 0.7875 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 32s 39ms/step - loss: 0.4913 - accuracy: 0.7759 - val_loss: 0.4673 - val_accuracy: 0.7877 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 37s 46ms/step - loss: 0.4913 - accuracy: 0.7773 - val_loss: 0.4659 - val_accuracy: 0.7914 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4900 - accuracy: 0.7771 - val_loss: 0.4651 - val_accuracy: 0.7875 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4896 - accuracy: 0.7785 - val_loss: 0.4679 - val_accuracy: 0.7830 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4891 - accuracy: 0.7747 - val_loss: 0.4623 - val_accuracy: 0.7883 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4881 - accuracy: 0.7768 - val_loss: 0.4612 - val_accuracy: 0.7909 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4864 - accuracy: 0.7769 - val_loss: 0.4629 - val_accuracy: 0.7891 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4825 - accuracy: 0.7808 - val_loss: 0.4637 - val_accuracy: 0.7875 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4866 - accuracy: 0.7795 - val_loss: 0.4624 - val_accuracy: 0.7892 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4815 - accuracy: 0.7792 - val_loss: 0.4629 - val_accuracy: 0.7855 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 37s 46ms/step - loss: 0.4844 - accuracy: 0.7809 - val_loss: 0.4589 - val_accuracy: 0.7934 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4858 - accuracy: 0.7802 - val_loss: 0.4602 - val_accuracy: 0.7881 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4830 - accuracy: 0.7784 - val_loss: 0.4628 - val_accuracy: 0.7894 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4818 - accuracy: 0.7784 - val_loss: 0.4589 - val_accuracy: 0.7916 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4831 - accuracy: 0.7804 - val_loss: 0.4715 - val_accuracy: 0.7842 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4824 - accuracy: 0.7809 - val_loss: 0.4604 - val_accuracy: 0.7906 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4836 - accuracy: 0.7779 - val_loss: 0.4684 - val_accuracy: 0.7767 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4815 - accuracy: 0.7804 - val_loss: 0.4601 - val_accuracy: 0.7908 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 32s 39ms/step - loss: 0.4812 - accuracy: 0.7793 - val_loss: 0.4614 - val_accuracy: 0.7884 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4795 - accuracy: 0.7788 - val_loss: 0.4614 - val_accuracy: 0.7847 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4813 - accuracy: 0.7791 - val_loss: 0.4645 - val_accuracy: 0.7822 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4739 - accuracy: 0.7813 - val_loss: 0.4611 - val_accuracy: 0.7837 - lr: 2.5000e-04\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4715 - accuracy: 0.7841 - val_loss: 0.4599 - val_accuracy: 0.7895 - lr: 2.5000e-04\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 32s 39ms/step - loss: 0.4716 - accuracy: 0.7847 - val_loss: 0.4620 - val_accuracy: 0.7883 - lr: 2.5000e-04\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4701 - accuracy: 0.7857 - val_loss: 0.4605 - val_accuracy: 0.7916 - lr: 2.5000e-04\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4709 - accuracy: 0.7865 - val_loss: 0.4613 - val_accuracy: 0.7891 - lr: 2.5000e-04\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4680 - accuracy: 0.7867 - val_loss: 0.4608 - val_accuracy: 0.7886 - lr: 2.5000e-04\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4697 - accuracy: 0.7861 - val_loss: 0.4602 - val_accuracy: 0.7889 - lr: 2.5000e-04\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4678 - accuracy: 0.7862 - val_loss: 0.4602 - val_accuracy: 0.7877 - lr: 2.5000e-04\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4659 - accuracy: 0.7887 - val_loss: 0.4598 - val_accuracy: 0.7878 - lr: 2.5000e-04\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4656 - accuracy: 0.7856 - val_loss: 0.4625 - val_accuracy: 0.7848 - lr: 2.5000e-04\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4642 - accuracy: 0.7879 - val_loss: 0.4599 - val_accuracy: 0.7927 - lr: 1.2500e-04\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4617 - accuracy: 0.7903 - val_loss: 0.4588 - val_accuracy: 0.7930 - lr: 1.2500e-04\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4588 - accuracy: 0.7896 - val_loss: 0.4588 - val_accuracy: 0.7898 - lr: 1.2500e-04\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4613 - accuracy: 0.7877 - val_loss: 0.4576 - val_accuracy: 0.7875 - lr: 1.2500e-04\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4575 - accuracy: 0.7909 - val_loss: 0.4577 - val_accuracy: 0.7903 - lr: 1.2500e-04\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4577 - accuracy: 0.7911 - val_loss: 0.4591 - val_accuracy: 0.7922 - lr: 1.2500e-04\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4554 - accuracy: 0.7923 - val_loss: 0.4575 - val_accuracy: 0.7911 - lr: 1.2500e-04\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4566 - accuracy: 0.7910 - val_loss: 0.4574 - val_accuracy: 0.7905 - lr: 1.2500e-04\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4553 - accuracy: 0.7925 - val_loss: 0.4579 - val_accuracy: 0.7902 - lr: 1.2500e-04\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4570 - accuracy: 0.7910 - val_loss: 0.4587 - val_accuracy: 0.7911 - lr: 1.2500e-04\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4540 - accuracy: 0.7944 - val_loss: 0.4621 - val_accuracy: 0.7892 - lr: 1.2500e-04\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4550 - accuracy: 0.7920 - val_loss: 0.4584 - val_accuracy: 0.7895 - lr: 1.2500e-04\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4545 - accuracy: 0.7924 - val_loss: 0.4595 - val_accuracy: 0.7884 - lr: 1.2500e-04\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 36s 45ms/step - loss: 0.4554 - accuracy: 0.7903 - val_loss: 0.4565 - val_accuracy: 0.7948 - lr: 1.2500e-04\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4545 - accuracy: 0.7928 - val_loss: 0.4579 - val_accuracy: 0.7898 - lr: 1.2500e-04\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4546 - accuracy: 0.7925 - val_loss: 0.4558 - val_accuracy: 0.7906 - lr: 1.2500e-04\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4533 - accuracy: 0.7937 - val_loss: 0.4579 - val_accuracy: 0.7891 - lr: 1.2500e-04\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4536 - accuracy: 0.7918 - val_loss: 0.4571 - val_accuracy: 0.7900 - lr: 1.2500e-04\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4546 - accuracy: 0.7932 - val_loss: 0.4568 - val_accuracy: 0.7900 - lr: 1.2500e-04\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4553 - accuracy: 0.7929 - val_loss: 0.4564 - val_accuracy: 0.7916 - lr: 1.2500e-04\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4538 - accuracy: 0.7914 - val_loss: 0.4571 - val_accuracy: 0.7934 - lr: 1.2500e-04\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4511 - accuracy: 0.7925 - val_loss: 0.4572 - val_accuracy: 0.7900 - lr: 1.2500e-04\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4503 - accuracy: 0.7931 - val_loss: 0.4578 - val_accuracy: 0.7895 - lr: 1.2500e-04\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4500 - accuracy: 0.7924 - val_loss: 0.4565 - val_accuracy: 0.7912 - lr: 1.2500e-04\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4488 - accuracy: 0.7966 - val_loss: 0.4588 - val_accuracy: 0.7911 - lr: 1.2500e-04\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 32s 39ms/step - loss: 0.4507 - accuracy: 0.7955 - val_loss: 0.4580 - val_accuracy: 0.7900 - lr: 1.2500e-04\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4473 - accuracy: 0.7962 - val_loss: 0.4567 - val_accuracy: 0.7909 - lr: 6.2500e-05\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4488 - accuracy: 0.7968 - val_loss: 0.4564 - val_accuracy: 0.7947 - lr: 6.2500e-05\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4447 - accuracy: 0.7988 - val_loss: 0.4584 - val_accuracy: 0.7925 - lr: 6.2500e-05\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4451 - accuracy: 0.7959 - val_loss: 0.4576 - val_accuracy: 0.7908 - lr: 6.2500e-05\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4445 - accuracy: 0.7991 - val_loss: 0.4567 - val_accuracy: 0.7925 - lr: 6.2500e-05\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4458 - accuracy: 0.7971 - val_loss: 0.4560 - val_accuracy: 0.7933 - lr: 6.2500e-05\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4429 - accuracy: 0.7965 - val_loss: 0.4575 - val_accuracy: 0.7925 - lr: 6.2500e-05\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4457 - accuracy: 0.7973 - val_loss: 0.4561 - val_accuracy: 0.7895 - lr: 6.2500e-05\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4432 - accuracy: 0.7995 - val_loss: 0.4557 - val_accuracy: 0.7911 - lr: 6.2500e-05\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4437 - accuracy: 0.7975 - val_loss: 0.4579 - val_accuracy: 0.7942 - lr: 6.2500e-05\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 31s 38ms/step - loss: 0.4419 - accuracy: 0.7996 - val_loss: 0.4578 - val_accuracy: 0.7923 - lr: 6.2500e-05\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 32s 39ms/step - loss: 0.4457 - accuracy: 0.7988 - val_loss: 0.4592 - val_accuracy: 0.7902 - lr: 6.2500e-05\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4414 - accuracy: 0.7977 - val_loss: 0.4578 - val_accuracy: 0.7923 - lr: 6.2500e-05\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4433 - accuracy: 0.8009 - val_loss: 0.4564 - val_accuracy: 0.7919 - lr: 6.2500e-05\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4434 - accuracy: 0.7961 - val_loss: 0.4556 - val_accuracy: 0.7936 - lr: 6.2500e-05\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4408 - accuracy: 0.8011 - val_loss: 0.4550 - val_accuracy: 0.7920 - lr: 6.2500e-05\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4436 - accuracy: 0.7995 - val_loss: 0.4556 - val_accuracy: 0.7920 - lr: 6.2500e-05\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4427 - accuracy: 0.7991 - val_loss: 0.4573 - val_accuracy: 0.7945 - lr: 6.2500e-05\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4418 - accuracy: 0.7996 - val_loss: 0.4572 - val_accuracy: 0.7933 - lr: 6.2500e-05\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 31s 38ms/step - loss: 0.4413 - accuracy: 0.7992 - val_loss: 0.4576 - val_accuracy: 0.7922 - lr: 6.2500e-05\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4401 - accuracy: 0.7996 - val_loss: 0.4572 - val_accuracy: 0.7928 - lr: 6.2500e-05\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4394 - accuracy: 0.8008 - val_loss: 0.4580 - val_accuracy: 0.7922 - lr: 6.2500e-05\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4417 - accuracy: 0.7986 - val_loss: 0.4562 - val_accuracy: 0.7931 - lr: 6.2500e-05\n",
      "200/200 [==============================] - 7s 30ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/08 08:15:38 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/10/08 08:15:38 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000018F16DA1900>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000018F16DA1900>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x00000191192CB9A0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x00000191192CB9A0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x00000191192C8AC0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x00000191192C8AC0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001912929D1E0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001912929D1E0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x00000191292A6B60>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x00000191292A6B60>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x00000191292A4310>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x00000191292A4310>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, transformer_layer_call_fn, transformer_layer_call_and_return_conditional_losses while saving (showing 5 of 165). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpl6zaojzq\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpl6zaojzq\\model\\data\\model\\assets\n",
      "2025/10/08 08:16:08 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpl6zaojzq\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/10/08 08:16:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "for model_name in other_bert_models:\n",
    "    print(f\"Running test with BERT model : {model_name}\")\n",
    "    test_bert_model_other(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a48d89",
   "metadata": {},
   "source": [
    "# Optimisation de la t√™te du mod√®le "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a946834",
   "metadata": {},
   "source": [
    "## Fonction de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcc87c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_size = 128\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "def test_bert_model_other_v2(bert_model_name, rnn_size=rnn_size):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_input(dataset)\n",
    "        mlflow.log_params(params={\n",
    "            'rnn_size': rnn_size, \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr,\n",
    "            'bert_model_name':bert_model_name\n",
    "        })\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "        encodings_train = tokenizer(X_train.to_list(), \n",
    "                                    truncation=True, \n",
    "                                    padding=True, \n",
    "                                    max_length=64,\n",
    "                                    return_tensors=\"tf\")\n",
    "        encodings_val = tokenizer(X_val.to_list(), \n",
    "                                  truncation=True, \n",
    "                                  padding=True, \n",
    "                                  max_length = 64,\n",
    "                                  return_tensors=\"tf\")\n",
    "\n",
    "        dataset_train = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                {\"input_ids\": encodings_train[\"input_ids\"], \n",
    "                 \"attention_mask\": encodings_train[\"attention_mask\"]\n",
    "                 },y_train\n",
    "                )\n",
    "                ).batch(32)\n",
    "        \n",
    "        dataset_val = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                {\"input_ids\": encodings_val[\"input_ids\"], \n",
    "                 \"attention_mask\": encodings_val[\"attention_mask\"]\n",
    "                 },y_val\n",
    "                )\n",
    "                ).batch(32)\n",
    "        # On charge le mod√®le pr√©-entrainn√©\n",
    "        try:\n",
    "            base_model = TFAutoModel.from_pretrained(bert_model_name, from_pt=False)# Forcer la version tensorflow\n",
    "            mlflow.log_param(\"from_pt\", False)\n",
    "        except:\n",
    "            print(bert_model_name, \" not available in TF\")\n",
    "            base_model = TFAutoModel.from_pretrained(bert_model_name,from_pt=True)\n",
    "            mlflow.log_param(\"from_pt\", True)\n",
    "\n",
    "            \n",
    "    \n",
    "        base_model.trainable = False # Pas de fine-tuning ou d'entrainement car impossible √† faire avec les ressources disponibles\n",
    "\n",
    "        # Construction du mod√®le keras \n",
    "        ## Une input layer pour les input ids\n",
    "        input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "        ## Une input layer pour le masque d'attention\n",
    "        attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "        ## On r√©cup√®re \n",
    "\n",
    "        outputs = base_model(input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n",
    "\n",
    "        # On prend le token [CLS] comme vecteur de phrase\n",
    "        cls_token = token_embeddings[:, 0, :]\n",
    "        max_tokens = tf.reduce_max(token_embeddings, axis=1)  #\n",
    "        mean_tokens = tf.reduce_mean(token_embeddings, axis=1)  #\n",
    "\n",
    "        all_tokens = tf.concat([cls_token, max_tokens, mean_tokens], axis=-1)  # [batch, hidden_size*2]\n",
    "    \n",
    "        x = tf.keras.layers.Dense(rnn_size, activation=\"relu\", \n",
    "                                  kernel_regularizer=regularizers.L2(1e-4),\n",
    "                                  bias_regularizer=regularizers.L2(1e-4), \n",
    "                                #   activity_regularizer=regularizers.L2(1e-4),\n",
    "                                  )(all_tokens)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        logits = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "        model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=logits)\n",
    "\n",
    "        ## Callbacks\n",
    "        model_savepath = f\"./Models/MY_{'_'.join(bert_model_name.split('/'))}_dense{rnn_size}.h5\"\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=True, mode='max')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=20)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "\n",
    "\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "        # Summary\n",
    "        model.summary()\n",
    "        # History\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(dataset_train, epochs=epochs, batch_size=64, validation_data=dataset_val, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "                # Pr√©dictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(dataset_val)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des m√©triques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du mod√®le dans MLflow\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71b98277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MLflow experiment...\n"
     ]
    }
   ],
   "source": [
    "# Cr√©ation de l'√©tude Optuna et optimisation\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"BERT_models_experiment\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"BERT_models_experiment\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Optimisation de la t√™te du mod√®le bas√© sur BERT.\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"BERT_pretrained\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25d4578",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7381cc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f39cadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with BERT model : Kapiche/twitter-roberta-base-sentiment\n",
      "Kapiche/twitter-roberta-base-sentiment  not available in TF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['roberta.embeddings.position_ids', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model_1 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, None                                               \n",
      "                                , 768),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model_1[0][0]']     \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.math.reduce_max (TFOpLambda  (None, 768)         0           ['tf_roberta_model_1[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpLambd  (None, 768)         0           ['tf_roberta_model_1[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 2304)         0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'tf.math.reduce_max[0][0]',    \n",
      "                                                                  'tf.math.reduce_mean[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           147520      ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_74 (Dropout)           (None, 64)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            65          ['dropout_74[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124,793,217\n",
      "Trainable params: 147,585\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 79s 89ms/step - loss: 0.4976 - accuracy: 0.7782 - val_loss: 0.4302 - val_accuracy: 0.8072 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 77s 97ms/step - loss: 0.4752 - accuracy: 0.7902 - val_loss: 0.4231 - val_accuracy: 0.8106 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 83s 104ms/step - loss: 0.4629 - accuracy: 0.7966 - val_loss: 0.4192 - val_accuracy: 0.8119 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 84s 105ms/step - loss: 0.4577 - accuracy: 0.7968 - val_loss: 0.4190 - val_accuracy: 0.8138 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 76s 95ms/step - loss: 0.4563 - accuracy: 0.7990 - val_loss: 0.4209 - val_accuracy: 0.8133 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.4549 - accuracy: 0.7995 - val_loss: 0.4241 - val_accuracy: 0.8095 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 83s 104ms/step - loss: 0.4528 - accuracy: 0.8004 - val_loss: 0.4166 - val_accuracy: 0.8172 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 75s 93ms/step - loss: 0.4532 - accuracy: 0.7983 - val_loss: 0.4165 - val_accuracy: 0.8158 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 73s 91ms/step - loss: 0.4544 - accuracy: 0.8018 - val_loss: 0.4194 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 77s 96ms/step - loss: 0.4526 - accuracy: 0.8008 - val_loss: 0.4168 - val_accuracy: 0.8161 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 76s 95ms/step - loss: 0.4518 - accuracy: 0.8034 - val_loss: 0.4170 - val_accuracy: 0.8158 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 73s 91ms/step - loss: 0.4523 - accuracy: 0.8023 - val_loss: 0.4150 - val_accuracy: 0.8139 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.4510 - accuracy: 0.8020 - val_loss: 0.4178 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.4521 - accuracy: 0.8030 - val_loss: 0.4317 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.4502 - accuracy: 0.8012 - val_loss: 0.4180 - val_accuracy: 0.8142 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 75s 93ms/step - loss: 0.4506 - accuracy: 0.8039 - val_loss: 0.4198 - val_accuracy: 0.8144 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 76s 95ms/step - loss: 0.4511 - accuracy: 0.8034 - val_loss: 0.4187 - val_accuracy: 0.8167 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.4489 - accuracy: 0.8031 - val_loss: 0.4180 - val_accuracy: 0.8138 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 75s 93ms/step - loss: 0.4483 - accuracy: 0.8040 - val_loss: 0.4224 - val_accuracy: 0.8105 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.4474 - accuracy: 0.8039 - val_loss: 0.4197 - val_accuracy: 0.8152 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.4445 - accuracy: 0.8050 - val_loss: 0.4200 - val_accuracy: 0.8167 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 74s 93ms/step - loss: 0.4494 - accuracy: 0.8021 - val_loss: 0.4196 - val_accuracy: 0.8155 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.4428 - accuracy: 0.8092 - val_loss: 0.4211 - val_accuracy: 0.8117 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.4395 - accuracy: 0.8086 - val_loss: 0.4137 - val_accuracy: 0.8163 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.4394 - accuracy: 0.8068 - val_loss: 0.4186 - val_accuracy: 0.8128 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 74s 93ms/step - loss: 0.4384 - accuracy: 0.8096 - val_loss: 0.4142 - val_accuracy: 0.8145 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 76s 95ms/step - loss: 0.4371 - accuracy: 0.8114 - val_loss: 0.4146 - val_accuracy: 0.8159 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 79s 98ms/step - loss: 0.4368 - accuracy: 0.8081 - val_loss: 0.4155 - val_accuracy: 0.8188 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.4352 - accuracy: 0.8085 - val_loss: 0.4179 - val_accuracy: 0.8156 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.4339 - accuracy: 0.8084 - val_loss: 0.4130 - val_accuracy: 0.8170 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 82s 102ms/step - loss: 0.4368 - accuracy: 0.8080 - val_loss: 0.4109 - val_accuracy: 0.8197 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 74s 93ms/step - loss: 0.4371 - accuracy: 0.8077 - val_loss: 0.4117 - val_accuracy: 0.8173 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 74s 92ms/step - loss: 0.4362 - accuracy: 0.8099 - val_loss: 0.4120 - val_accuracy: 0.8142 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 74s 93ms/step - loss: 0.4354 - accuracy: 0.8085 - val_loss: 0.4133 - val_accuracy: 0.8155 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 74s 93ms/step - loss: 0.4370 - accuracy: 0.8098 - val_loss: 0.4114 - val_accuracy: 0.8195 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 74s 92ms/step - loss: 0.4348 - accuracy: 0.8100 - val_loss: 0.4157 - val_accuracy: 0.8156 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 75s 93ms/step - loss: 0.4341 - accuracy: 0.8105 - val_loss: 0.4116 - val_accuracy: 0.8180 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.4324 - accuracy: 0.8136 - val_loss: 0.4158 - val_accuracy: 0.8131 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 70s 88ms/step - loss: 0.4332 - accuracy: 0.8133 - val_loss: 0.4184 - val_accuracy: 0.8105 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 68s 86ms/step - loss: 0.4348 - accuracy: 0.8092 - val_loss: 0.4136 - val_accuracy: 0.8175 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 67s 84ms/step - loss: 0.4323 - accuracy: 0.8095 - val_loss: 0.4112 - val_accuracy: 0.8178 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 66s 83ms/step - loss: 0.4293 - accuracy: 0.8131 - val_loss: 0.4116 - val_accuracy: 0.8141 - lr: 2.5000e-04\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 67s 84ms/step - loss: 0.4298 - accuracy: 0.8136 - val_loss: 0.4120 - val_accuracy: 0.8189 - lr: 2.5000e-04\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 70s 88ms/step - loss: 0.4269 - accuracy: 0.8154 - val_loss: 0.4131 - val_accuracy: 0.8145 - lr: 2.5000e-04\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 72s 90ms/step - loss: 0.4266 - accuracy: 0.8157 - val_loss: 0.4130 - val_accuracy: 0.8166 - lr: 2.5000e-04\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 71s 89ms/step - loss: 0.4260 - accuracy: 0.8162 - val_loss: 0.4112 - val_accuracy: 0.8145 - lr: 2.5000e-04\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 69s 86ms/step - loss: 0.4239 - accuracy: 0.8140 - val_loss: 0.4120 - val_accuracy: 0.8159 - lr: 2.5000e-04\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 71s 89ms/step - loss: 0.4247 - accuracy: 0.8166 - val_loss: 0.4084 - val_accuracy: 0.8152 - lr: 2.5000e-04\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 70s 88ms/step - loss: 0.4255 - accuracy: 0.8146 - val_loss: 0.4155 - val_accuracy: 0.8167 - lr: 2.5000e-04\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 71s 89ms/step - loss: 0.4216 - accuracy: 0.8154 - val_loss: 0.4089 - val_accuracy: 0.8189 - lr: 2.5000e-04\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 72s 90ms/step - loss: 0.4263 - accuracy: 0.8143 - val_loss: 0.4125 - val_accuracy: 0.8130 - lr: 2.5000e-04\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 70s 87ms/step - loss: 0.4231 - accuracy: 0.8169 - val_loss: 0.4085 - val_accuracy: 0.8161 - lr: 2.5000e-04\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 68s 85ms/step - loss: 0.4243 - accuracy: 0.8145 - val_loss: 0.4121 - val_accuracy: 0.8178 - lr: 2.5000e-04\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 65s 81ms/step - loss: 0.4229 - accuracy: 0.8192 - val_loss: 0.4117 - val_accuracy: 0.8189 - lr: 2.5000e-04\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.4218 - accuracy: 0.8153 - val_loss: 0.4110 - val_accuracy: 0.8184 - lr: 2.5000e-04\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 71s 88ms/step - loss: 0.4240 - accuracy: 0.8158 - val_loss: 0.4080 - val_accuracy: 0.8156 - lr: 2.5000e-04\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 77s 97ms/step - loss: 0.4189 - accuracy: 0.8194 - val_loss: 0.4066 - val_accuracy: 0.8198 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 70s 88ms/step - loss: 0.4217 - accuracy: 0.8160 - val_loss: 0.4077 - val_accuracy: 0.8181 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 70s 88ms/step - loss: 0.4236 - accuracy: 0.8135 - val_loss: 0.4152 - val_accuracy: 0.8177 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 70s 88ms/step - loss: 0.4205 - accuracy: 0.8148 - val_loss: 0.4066 - val_accuracy: 0.8169 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 70s 87ms/step - loss: 0.4212 - accuracy: 0.8148 - val_loss: 0.4068 - val_accuracy: 0.8167 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 69s 86ms/step - loss: 0.4219 - accuracy: 0.8131 - val_loss: 0.4097 - val_accuracy: 0.8145 - lr: 2.5000e-04\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 69s 86ms/step - loss: 0.4221 - accuracy: 0.8129 - val_loss: 0.4105 - val_accuracy: 0.8194 - lr: 2.5000e-04\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 69s 87ms/step - loss: 0.4208 - accuracy: 0.8153 - val_loss: 0.4123 - val_accuracy: 0.8144 - lr: 2.5000e-04\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 67s 84ms/step - loss: 0.4225 - accuracy: 0.8166 - val_loss: 0.4076 - val_accuracy: 0.8197 - lr: 2.5000e-04\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 69s 86ms/step - loss: 0.4199 - accuracy: 0.8155 - val_loss: 0.4116 - val_accuracy: 0.8170 - lr: 2.5000e-04\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 70s 87ms/step - loss: 0.4203 - accuracy: 0.8153 - val_loss: 0.4081 - val_accuracy: 0.8195 - lr: 2.5000e-04\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 70s 87ms/step - loss: 0.4162 - accuracy: 0.8197 - val_loss: 0.4064 - val_accuracy: 0.8153 - lr: 1.2500e-04\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 70s 87ms/step - loss: 0.4133 - accuracy: 0.8163 - val_loss: 0.4081 - val_accuracy: 0.8145 - lr: 1.2500e-04\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 69s 86ms/step - loss: 0.4155 - accuracy: 0.8194 - val_loss: 0.4090 - val_accuracy: 0.8153 - lr: 1.2500e-04\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 69s 86ms/step - loss: 0.4148 - accuracy: 0.8189 - val_loss: 0.4076 - val_accuracy: 0.8183 - lr: 1.2500e-04\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 70s 87ms/step - loss: 0.4166 - accuracy: 0.8172 - val_loss: 0.4107 - val_accuracy: 0.8139 - lr: 1.2500e-04\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 70s 87ms/step - loss: 0.4122 - accuracy: 0.8182 - val_loss: 0.4085 - val_accuracy: 0.8177 - lr: 1.2500e-04\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 70s 87ms/step - loss: 0.4126 - accuracy: 0.8188 - val_loss: 0.4114 - val_accuracy: 0.8112 - lr: 1.2500e-04\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 69s 87ms/step - loss: 0.4173 - accuracy: 0.8209 - val_loss: 0.4082 - val_accuracy: 0.8153 - lr: 1.2500e-04\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 77s 97ms/step - loss: 0.4156 - accuracy: 0.8193 - val_loss: 0.4063 - val_accuracy: 0.8202 - lr: 1.2500e-04\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 69s 86ms/step - loss: 0.4152 - accuracy: 0.8177 - val_loss: 0.4082 - val_accuracy: 0.8180 - lr: 1.2500e-04\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 68s 85ms/step - loss: 0.4132 - accuracy: 0.8191 - val_loss: 0.4097 - val_accuracy: 0.8155 - lr: 1.2500e-04\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 69s 86ms/step - loss: 0.4131 - accuracy: 0.8217 - val_loss: 0.4064 - val_accuracy: 0.8200 - lr: 6.2500e-05\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 69s 87ms/step - loss: 0.4095 - accuracy: 0.8231 - val_loss: 0.4095 - val_accuracy: 0.8170 - lr: 6.2500e-05\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 70s 87ms/step - loss: 0.4088 - accuracy: 0.8224 - val_loss: 0.4061 - val_accuracy: 0.8189 - lr: 6.2500e-05\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 69s 87ms/step - loss: 0.4107 - accuracy: 0.8207 - val_loss: 0.4089 - val_accuracy: 0.8177 - lr: 6.2500e-05\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 68s 85ms/step - loss: 0.4108 - accuracy: 0.8212 - val_loss: 0.4064 - val_accuracy: 0.8189 - lr: 6.2500e-05\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 70s 88ms/step - loss: 0.4094 - accuracy: 0.8230 - val_loss: 0.4080 - val_accuracy: 0.8177 - lr: 6.2500e-05\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 68s 84ms/step - loss: 0.4096 - accuracy: 0.8221 - val_loss: 0.4075 - val_accuracy: 0.8180 - lr: 6.2500e-05\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 69s 87ms/step - loss: 0.4116 - accuracy: 0.8208 - val_loss: 0.4060 - val_accuracy: 0.8158 - lr: 6.2500e-05\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 70s 88ms/step - loss: 0.4073 - accuracy: 0.8231 - val_loss: 0.4074 - val_accuracy: 0.8189 - lr: 6.2500e-05\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 69s 86ms/step - loss: 0.4083 - accuracy: 0.8227 - val_loss: 0.4119 - val_accuracy: 0.8136 - lr: 6.2500e-05\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 70s 88ms/step - loss: 0.4103 - accuracy: 0.8216 - val_loss: 0.4088 - val_accuracy: 0.8147 - lr: 6.2500e-05\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 70s 87ms/step - loss: 0.4092 - accuracy: 0.8224 - val_loss: 0.4078 - val_accuracy: 0.8173 - lr: 6.2500e-05\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 69s 86ms/step - loss: 0.4124 - accuracy: 0.8223 - val_loss: 0.4101 - val_accuracy: 0.8166 - lr: 6.2500e-05\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 70s 88ms/step - loss: 0.4098 - accuracy: 0.8217 - val_loss: 0.4076 - val_accuracy: 0.8180 - lr: 3.1250e-05\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 70s 88ms/step - loss: 0.4099 - accuracy: 0.8226 - val_loss: 0.4078 - val_accuracy: 0.8192 - lr: 3.1250e-05\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 69s 86ms/step - loss: 0.4096 - accuracy: 0.8219 - val_loss: 0.4081 - val_accuracy: 0.8188 - lr: 3.1250e-05\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 70s 88ms/step - loss: 0.4073 - accuracy: 0.8231 - val_loss: 0.4090 - val_accuracy: 0.8166 - lr: 3.1250e-05\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 68s 85ms/step - loss: 0.4084 - accuracy: 0.8220 - val_loss: 0.4071 - val_accuracy: 0.8186 - lr: 3.1250e-05\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 70s 88ms/step - loss: 0.4065 - accuracy: 0.8254 - val_loss: 0.4065 - val_accuracy: 0.8195 - lr: 3.1250e-05\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 69s 86ms/step - loss: 0.4074 - accuracy: 0.8255 - val_loss: 0.4076 - val_accuracy: 0.8191 - lr: 3.1250e-05\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 69s 87ms/step - loss: 0.4074 - accuracy: 0.8211 - val_loss: 0.4064 - val_accuracy: 0.8202 - lr: 3.1250e-05\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 70s 88ms/step - loss: 0.4076 - accuracy: 0.8243 - val_loss: 0.4075 - val_accuracy: 0.8189 - lr: 3.1250e-05\n",
      "200/200 [==============================] - 16s 65ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/09 00:18:12 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/10/09 00:18:13 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as serving, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp1ab9pma9\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp1ab9pma9\\model\\data\\model\\assets\n",
      "2025/10/09 00:19:30 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp1ab9pma9\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/10/09 00:19:30 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with BERT model : Kapiche/twitter-roberta-base-sentiment\n",
      "Kapiche/twitter-roberta-base-sentiment  not available in TF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['roberta.embeddings.position_ids', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model_1 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, None                                               \n",
      "                                , 768),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model_1[0][0]']     \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.math.reduce_max (TFOpLambda  (None, 768)         0           ['tf_roberta_model_1[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpLambd  (None, 768)         0           ['tf_roberta_model_1[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 2304)         0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'tf.math.reduce_max[0][0]',    \n",
      "                                                                  'tf.math.reduce_mean[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 192)          442560      ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_74 (Dropout)           (None, 192)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            193         ['dropout_74[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 125,088,385\n",
      "Trainable params: 442,753\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 87s 94ms/step - loss: 0.5023 - accuracy: 0.7826 - val_loss: 0.4381 - val_accuracy: 0.8056 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 79s 99ms/step - loss: 0.4710 - accuracy: 0.7944 - val_loss: 0.4348 - val_accuracy: 0.8116 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 72s 91ms/step - loss: 0.4596 - accuracy: 0.7978 - val_loss: 0.4237 - val_accuracy: 0.8106 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 80s 99ms/step - loss: 0.4560 - accuracy: 0.7996 - val_loss: 0.4312 - val_accuracy: 0.8123 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 71s 89ms/step - loss: 0.4557 - accuracy: 0.7994 - val_loss: 0.4210 - val_accuracy: 0.8122 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 80s 100ms/step - loss: 0.4534 - accuracy: 0.8016 - val_loss: 0.4229 - val_accuracy: 0.8127 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 79s 99ms/step - loss: 0.4530 - accuracy: 0.8040 - val_loss: 0.4244 - val_accuracy: 0.8131 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 79s 99ms/step - loss: 0.4507 - accuracy: 0.8025 - val_loss: 0.4247 - val_accuracy: 0.8148 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 71s 89ms/step - loss: 0.4508 - accuracy: 0.8001 - val_loss: 0.4234 - val_accuracy: 0.8112 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 72s 90ms/step - loss: 0.4508 - accuracy: 0.8040 - val_loss: 0.4216 - val_accuracy: 0.8138 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 80s 100ms/step - loss: 0.4507 - accuracy: 0.8027 - val_loss: 0.4198 - val_accuracy: 0.8150 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 72s 90ms/step - loss: 0.4480 - accuracy: 0.8037 - val_loss: 0.4249 - val_accuracy: 0.8092 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 72s 90ms/step - loss: 0.4526 - accuracy: 0.8034 - val_loss: 0.4255 - val_accuracy: 0.8078 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 72s 90ms/step - loss: 0.4504 - accuracy: 0.8043 - val_loss: 0.4222 - val_accuracy: 0.8125 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 72s 90ms/step - loss: 0.4519 - accuracy: 0.8002 - val_loss: 0.4244 - val_accuracy: 0.8105 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 79s 99ms/step - loss: 0.4482 - accuracy: 0.8047 - val_loss: 0.4186 - val_accuracy: 0.8158 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 79s 99ms/step - loss: 0.4501 - accuracy: 0.8057 - val_loss: 0.4185 - val_accuracy: 0.8161 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 72s 90ms/step - loss: 0.4479 - accuracy: 0.8057 - val_loss: 0.4168 - val_accuracy: 0.8150 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 70s 88ms/step - loss: 0.4476 - accuracy: 0.8053 - val_loss: 0.4187 - val_accuracy: 0.8153 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 78s 98ms/step - loss: 0.4468 - accuracy: 0.8054 - val_loss: 0.4208 - val_accuracy: 0.8166 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 74s 92ms/step - loss: 0.4468 - accuracy: 0.8040 - val_loss: 0.4223 - val_accuracy: 0.8156 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 79s 99ms/step - loss: 0.4507 - accuracy: 0.8020 - val_loss: 0.4189 - val_accuracy: 0.8184 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.4459 - accuracy: 0.8066 - val_loss: 0.4208 - val_accuracy: 0.8164 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 71s 88ms/step - loss: 0.4471 - accuracy: 0.8057 - val_loss: 0.4181 - val_accuracy: 0.8156 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.4512 - accuracy: 0.8008 - val_loss: 0.4180 - val_accuracy: 0.8188 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 73s 91ms/step - loss: 0.4508 - accuracy: 0.8028 - val_loss: 0.4245 - val_accuracy: 0.8139 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 71s 89ms/step - loss: 0.4475 - accuracy: 0.8023 - val_loss: 0.4245 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 72s 90ms/step - loss: 0.4493 - accuracy: 0.8044 - val_loss: 0.4203 - val_accuracy: 0.8147 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 71s 89ms/step - loss: 0.4374 - accuracy: 0.8100 - val_loss: 0.4158 - val_accuracy: 0.8148 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.4366 - accuracy: 0.8097 - val_loss: 0.4182 - val_accuracy: 0.8159 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.4347 - accuracy: 0.8093 - val_loss: 0.4163 - val_accuracy: 0.8144 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 74s 92ms/step - loss: 0.4308 - accuracy: 0.8106 - val_loss: 0.4134 - val_accuracy: 0.8195 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 67s 84ms/step - loss: 0.4311 - accuracy: 0.8101 - val_loss: 0.4193 - val_accuracy: 0.8166 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.4317 - accuracy: 0.8095 - val_loss: 0.4214 - val_accuracy: 0.8133 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 65s 81ms/step - loss: 0.4300 - accuracy: 0.8117 - val_loss: 0.4123 - val_accuracy: 0.8183 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 61s 76ms/step - loss: 0.4307 - accuracy: 0.8104 - val_loss: 0.4141 - val_accuracy: 0.8166 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 64s 81ms/step - loss: 0.4298 - accuracy: 0.8121 - val_loss: 0.4137 - val_accuracy: 0.8194 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 66s 82ms/step - loss: 0.4325 - accuracy: 0.8101 - val_loss: 0.4143 - val_accuracy: 0.8163 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 61s 77ms/step - loss: 0.4334 - accuracy: 0.8105 - val_loss: 0.4129 - val_accuracy: 0.8161 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 74s 93ms/step - loss: 0.4302 - accuracy: 0.8112 - val_loss: 0.4155 - val_accuracy: 0.8208 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.4308 - accuracy: 0.8120 - val_loss: 0.4167 - val_accuracy: 0.8138 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 67s 83ms/step - loss: 0.4345 - accuracy: 0.8094 - val_loss: 0.4163 - val_accuracy: 0.8142 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 71s 88ms/step - loss: 0.4321 - accuracy: 0.8131 - val_loss: 0.4147 - val_accuracy: 0.8194 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 71s 89ms/step - loss: 0.4280 - accuracy: 0.8103 - val_loss: 0.4150 - val_accuracy: 0.8125 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 65s 81ms/step - loss: 0.4261 - accuracy: 0.8147 - val_loss: 0.4134 - val_accuracy: 0.8194 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 78s 98ms/step - loss: 0.4211 - accuracy: 0.8121 - val_loss: 0.4076 - val_accuracy: 0.8216 - lr: 2.5000e-04\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 77s 96ms/step - loss: 0.4216 - accuracy: 0.8161 - val_loss: 0.4133 - val_accuracy: 0.8156 - lr: 2.5000e-04\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 66s 83ms/step - loss: 0.4208 - accuracy: 0.8172 - val_loss: 0.4112 - val_accuracy: 0.8175 - lr: 2.5000e-04\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 67s 84ms/step - loss: 0.4200 - accuracy: 0.8152 - val_loss: 0.4134 - val_accuracy: 0.8156 - lr: 2.5000e-04\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.4215 - accuracy: 0.8190 - val_loss: 0.4113 - val_accuracy: 0.8158 - lr: 2.5000e-04\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 77s 96ms/step - loss: 0.4196 - accuracy: 0.8168 - val_loss: 0.4153 - val_accuracy: 0.8119 - lr: 2.5000e-04\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 67s 84ms/step - loss: 0.4191 - accuracy: 0.8162 - val_loss: 0.4150 - val_accuracy: 0.8172 - lr: 2.5000e-04\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 66s 83ms/step - loss: 0.4212 - accuracy: 0.8162 - val_loss: 0.4111 - val_accuracy: 0.8178 - lr: 2.5000e-04\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 143s 178ms/step - loss: 0.4186 - accuracy: 0.8162 - val_loss: 0.4126 - val_accuracy: 0.8159 - lr: 2.5000e-04\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 139s 174ms/step - loss: 0.4196 - accuracy: 0.8161 - val_loss: 0.4074 - val_accuracy: 0.8178 - lr: 2.5000e-04\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 134s 168ms/step - loss: 0.4190 - accuracy: 0.8193 - val_loss: 0.4103 - val_accuracy: 0.8175 - lr: 2.5000e-04\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 134s 167ms/step - loss: 0.4194 - accuracy: 0.8164 - val_loss: 0.4080 - val_accuracy: 0.8175 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 163s 204ms/step - loss: 0.4201 - accuracy: 0.8176 - val_loss: 0.4112 - val_accuracy: 0.8161 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 140s 175ms/step - loss: 0.4196 - accuracy: 0.8155 - val_loss: 0.4101 - val_accuracy: 0.8186 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 148s 185ms/step - loss: 0.4169 - accuracy: 0.8164 - val_loss: 0.4126 - val_accuracy: 0.8202 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 136s 170ms/step - loss: 0.4150 - accuracy: 0.8171 - val_loss: 0.4087 - val_accuracy: 0.8200 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 135s 169ms/step - loss: 0.4155 - accuracy: 0.8169 - val_loss: 0.4142 - val_accuracy: 0.8095 - lr: 2.5000e-04\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 140s 176ms/step - loss: 0.4170 - accuracy: 0.8173 - val_loss: 0.4081 - val_accuracy: 0.8191 - lr: 2.5000e-04\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 140s 175ms/step - loss: 0.4151 - accuracy: 0.8175 - val_loss: 0.4141 - val_accuracy: 0.8177 - lr: 2.5000e-04\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 135s 169ms/step - loss: 0.4127 - accuracy: 0.8179 - val_loss: 0.4119 - val_accuracy: 0.8177 - lr: 2.5000e-04\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 145s 181ms/step - loss: 0.4113 - accuracy: 0.8193 - val_loss: 0.4075 - val_accuracy: 0.8180 - lr: 1.2500e-04\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 84s 104ms/step - loss: 0.4125 - accuracy: 0.8184 - val_loss: 0.4057 - val_accuracy: 0.8186 - lr: 1.2500e-04\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.4070 - accuracy: 0.8212 - val_loss: 0.4082 - val_accuracy: 0.8184 - lr: 1.2500e-04\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.4095 - accuracy: 0.8215 - val_loss: 0.4090 - val_accuracy: 0.8178 - lr: 1.2500e-04\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.4063 - accuracy: 0.8214 - val_loss: 0.4066 - val_accuracy: 0.8177 - lr: 1.2500e-04\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.4053 - accuracy: 0.8236 - val_loss: 0.4071 - val_accuracy: 0.8203 - lr: 1.2500e-04\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 70s 88ms/step - loss: 0.4073 - accuracy: 0.8223 - val_loss: 0.4059 - val_accuracy: 0.8225 - lr: 1.2500e-04\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.4071 - accuracy: 0.8232 - val_loss: 0.4067 - val_accuracy: 0.8205 - lr: 1.2500e-04\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 65s 81ms/step - loss: 0.4091 - accuracy: 0.8204 - val_loss: 0.4076 - val_accuracy: 0.8183 - lr: 1.2500e-04\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 64s 81ms/step - loss: 0.4051 - accuracy: 0.8216 - val_loss: 0.4062 - val_accuracy: 0.8189 - lr: 1.2500e-04\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 65s 81ms/step - loss: 0.4093 - accuracy: 0.8201 - val_loss: 0.4043 - val_accuracy: 0.8195 - lr: 1.2500e-04\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.4082 - accuracy: 0.8200 - val_loss: 0.4083 - val_accuracy: 0.8195 - lr: 1.2500e-04\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 66s 83ms/step - loss: 0.4073 - accuracy: 0.8233 - val_loss: 0.4088 - val_accuracy: 0.8186 - lr: 1.2500e-04\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 70s 88ms/step - loss: 0.4074 - accuracy: 0.8222 - val_loss: 0.4057 - val_accuracy: 0.8197 - lr: 1.2500e-04\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 101s 127ms/step - loss: 0.4048 - accuracy: 0.8234 - val_loss: 0.4098 - val_accuracy: 0.8173 - lr: 1.2500e-04\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 108s 136ms/step - loss: 0.4050 - accuracy: 0.8244 - val_loss: 0.4053 - val_accuracy: 0.8192 - lr: 1.2500e-04\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 97s 122ms/step - loss: 0.4052 - accuracy: 0.8239 - val_loss: 0.4114 - val_accuracy: 0.8142 - lr: 1.2500e-04\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 151s 189ms/step - loss: 0.4037 - accuracy: 0.8231 - val_loss: 0.4103 - val_accuracy: 0.8173 - lr: 1.2500e-04\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 162s 203ms/step - loss: 0.4032 - accuracy: 0.8221 - val_loss: 0.4074 - val_accuracy: 0.8202 - lr: 1.2500e-04\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 137s 172ms/step - loss: 0.4059 - accuracy: 0.8240 - val_loss: 0.4075 - val_accuracy: 0.8197 - lr: 1.2500e-04\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 151s 189ms/step - loss: 0.4050 - accuracy: 0.8225 - val_loss: 0.4103 - val_accuracy: 0.8172 - lr: 1.2500e-04\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 138s 172ms/step - loss: 0.4044 - accuracy: 0.8232 - val_loss: 0.4066 - val_accuracy: 0.8197 - lr: 6.2500e-05\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 161s 201ms/step - loss: 0.4012 - accuracy: 0.8279 - val_loss: 0.4089 - val_accuracy: 0.8175 - lr: 6.2500e-05\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 160s 200ms/step - loss: 0.4016 - accuracy: 0.8253 - val_loss: 0.4092 - val_accuracy: 0.8189 - lr: 6.2500e-05\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 114s 143ms/step - loss: 0.4005 - accuracy: 0.8237 - val_loss: 0.4068 - val_accuracy: 0.8189 - lr: 6.2500e-05\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 102s 128ms/step - loss: 0.4012 - accuracy: 0.8241 - val_loss: 0.4059 - val_accuracy: 0.8200 - lr: 6.2500e-05\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 106s 132ms/step - loss: 0.4001 - accuracy: 0.8259 - val_loss: 0.4095 - val_accuracy: 0.8169 - lr: 6.2500e-05\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 94s 117ms/step - loss: 0.4002 - accuracy: 0.8268 - val_loss: 0.4097 - val_accuracy: 0.8181 - lr: 6.2500e-05\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 131s 163ms/step - loss: 0.4023 - accuracy: 0.8236 - val_loss: 0.4057 - val_accuracy: 0.8217 - lr: 6.2500e-05\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 148s 185ms/step - loss: 0.4012 - accuracy: 0.8253 - val_loss: 0.4062 - val_accuracy: 0.8178 - lr: 6.2500e-05\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 123s 154ms/step - loss: 0.4018 - accuracy: 0.8274 - val_loss: 0.4060 - val_accuracy: 0.8217 - lr: 6.2500e-05\n",
      "200/200 [==============================] - 35s 162ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/09 02:44:58 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/10/09 02:44:58 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as serving, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmprxb6eod7\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmprxb6eod7\\model\\data\\model\\assets\n",
      "2025/10/09 02:45:55 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmprxb6eod7\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/10/09 02:45:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with BERT model : Kapiche/twitter-roberta-base-sentiment\n",
      "Kapiche/twitter-roberta-base-sentiment  not available in TF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['roberta.embeddings.position_ids', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model_1 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, None                                               \n",
      "                                , 768),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model_1[0][0]']     \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.math.reduce_max (TFOpLambda  (None, 768)         0           ['tf_roberta_model_1[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpLambd  (None, 768)         0           ['tf_roberta_model_1[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 2304)         0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'tf.math.reduce_max[0][0]',    \n",
      "                                                                  'tf.math.reduce_mean[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          590080      ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_74 (Dropout)           (None, 256)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            257         ['dropout_74[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 125,235,969\n",
      "Trainable params: 590,337\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 116s 132ms/step - loss: 0.5151 - accuracy: 0.7850 - val_loss: 0.4475 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 87s 109ms/step - loss: 0.4715 - accuracy: 0.7969 - val_loss: 0.4339 - val_accuracy: 0.8117 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 100s 125ms/step - loss: 0.4635 - accuracy: 0.7977 - val_loss: 0.4269 - val_accuracy: 0.8141 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 134s 167ms/step - loss: 0.4599 - accuracy: 0.7979 - val_loss: 0.4242 - val_accuracy: 0.8152 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 130s 162ms/step - loss: 0.4544 - accuracy: 0.8008 - val_loss: 0.4236 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 134s 167ms/step - loss: 0.4554 - accuracy: 0.8018 - val_loss: 0.4258 - val_accuracy: 0.8092 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 154s 192ms/step - loss: 0.4534 - accuracy: 0.8019 - val_loss: 0.4217 - val_accuracy: 0.8144 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 165s 207ms/step - loss: 0.4509 - accuracy: 0.8021 - val_loss: 0.4250 - val_accuracy: 0.8142 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 175s 219ms/step - loss: 0.4555 - accuracy: 0.8022 - val_loss: 0.4196 - val_accuracy: 0.8144 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 253s 316ms/step - loss: 0.4493 - accuracy: 0.8027 - val_loss: 0.4252 - val_accuracy: 0.8142 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 165s 207ms/step - loss: 0.4478 - accuracy: 0.8030 - val_loss: 0.4223 - val_accuracy: 0.8147 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 155s 193ms/step - loss: 0.4474 - accuracy: 0.8035 - val_loss: 0.4177 - val_accuracy: 0.8163 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 147s 184ms/step - loss: 0.4471 - accuracy: 0.8057 - val_loss: 0.4219 - val_accuracy: 0.8161 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 153s 192ms/step - loss: 0.4483 - accuracy: 0.8051 - val_loss: 0.4181 - val_accuracy: 0.8177 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 132s 164ms/step - loss: 0.4501 - accuracy: 0.8050 - val_loss: 0.4181 - val_accuracy: 0.8163 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 127s 159ms/step - loss: 0.4484 - accuracy: 0.8052 - val_loss: 0.4196 - val_accuracy: 0.8139 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 139s 174ms/step - loss: 0.4493 - accuracy: 0.8037 - val_loss: 0.4213 - val_accuracy: 0.8158 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 122s 153ms/step - loss: 0.4490 - accuracy: 0.8046 - val_loss: 0.4208 - val_accuracy: 0.8133 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 116s 146ms/step - loss: 0.4481 - accuracy: 0.8062 - val_loss: 0.4199 - val_accuracy: 0.8156 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4500 - accuracy: 0.8052 - val_loss: 0.4169 - val_accuracy: 0.8159 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 124s 155ms/step - loss: 0.4482 - accuracy: 0.8037 - val_loss: 0.4208 - val_accuracy: 0.8183 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4486 - accuracy: 0.8043 - val_loss: 0.4204 - val_accuracy: 0.8152 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4442 - accuracy: 0.8066 - val_loss: 0.4201 - val_accuracy: 0.8145 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 124s 155ms/step - loss: 0.4505 - accuracy: 0.8053 - val_loss: 0.4201 - val_accuracy: 0.8191 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 116s 145ms/step - loss: 0.4500 - accuracy: 0.8046 - val_loss: 0.4198 - val_accuracy: 0.8177 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 116s 145ms/step - loss: 0.4487 - accuracy: 0.8042 - val_loss: 0.4261 - val_accuracy: 0.8089 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 116s 146ms/step - loss: 0.4473 - accuracy: 0.8068 - val_loss: 0.4224 - val_accuracy: 0.8106 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4483 - accuracy: 0.8046 - val_loss: 0.4202 - val_accuracy: 0.8164 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4492 - accuracy: 0.8045 - val_loss: 0.4178 - val_accuracy: 0.8183 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4461 - accuracy: 0.8064 - val_loss: 0.4159 - val_accuracy: 0.8181 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 123s 154ms/step - loss: 0.4460 - accuracy: 0.8049 - val_loss: 0.4146 - val_accuracy: 0.8195 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4473 - accuracy: 0.8044 - val_loss: 0.4205 - val_accuracy: 0.8172 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4464 - accuracy: 0.8074 - val_loss: 0.4199 - val_accuracy: 0.8184 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4498 - accuracy: 0.8051 - val_loss: 0.4220 - val_accuracy: 0.8170 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4468 - accuracy: 0.8080 - val_loss: 0.4166 - val_accuracy: 0.8158 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4469 - accuracy: 0.8030 - val_loss: 0.4211 - val_accuracy: 0.8130 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4504 - accuracy: 0.8043 - val_loss: 0.4227 - val_accuracy: 0.8148 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4476 - accuracy: 0.8046 - val_loss: 0.4215 - val_accuracy: 0.8161 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4494 - accuracy: 0.8060 - val_loss: 0.4163 - val_accuracy: 0.8177 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4462 - accuracy: 0.8044 - val_loss: 0.4264 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4472 - accuracy: 0.8070 - val_loss: 0.4263 - val_accuracy: 0.8105 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4389 - accuracy: 0.8095 - val_loss: 0.4159 - val_accuracy: 0.8189 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4347 - accuracy: 0.8108 - val_loss: 0.4155 - val_accuracy: 0.8163 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4316 - accuracy: 0.8104 - val_loss: 0.4208 - val_accuracy: 0.8153 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 116s 145ms/step - loss: 0.4349 - accuracy: 0.8088 - val_loss: 0.4157 - val_accuracy: 0.8170 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 123s 154ms/step - loss: 0.4319 - accuracy: 0.8110 - val_loss: 0.4133 - val_accuracy: 0.8203 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4339 - accuracy: 0.8106 - val_loss: 0.4148 - val_accuracy: 0.8166 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4335 - accuracy: 0.8099 - val_loss: 0.4174 - val_accuracy: 0.8138 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 116s 146ms/step - loss: 0.4334 - accuracy: 0.8111 - val_loss: 0.4176 - val_accuracy: 0.8166 - lr: 5.0000e-04\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 116s 146ms/step - loss: 0.4323 - accuracy: 0.8092 - val_loss: 0.4124 - val_accuracy: 0.8172 - lr: 5.0000e-04\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4304 - accuracy: 0.8099 - val_loss: 0.4152 - val_accuracy: 0.8134 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4294 - accuracy: 0.8098 - val_loss: 0.4127 - val_accuracy: 0.8173 - lr: 5.0000e-04\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 116s 145ms/step - loss: 0.4292 - accuracy: 0.8123 - val_loss: 0.4125 - val_accuracy: 0.8172 - lr: 5.0000e-04\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 116s 145ms/step - loss: 0.4297 - accuracy: 0.8120 - val_loss: 0.4132 - val_accuracy: 0.8197 - lr: 5.0000e-04\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 116s 146ms/step - loss: 0.4319 - accuracy: 0.8105 - val_loss: 0.4115 - val_accuracy: 0.8183 - lr: 5.0000e-04\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 116s 146ms/step - loss: 0.4307 - accuracy: 0.8127 - val_loss: 0.4145 - val_accuracy: 0.8161 - lr: 5.0000e-04\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4301 - accuracy: 0.8115 - val_loss: 0.4178 - val_accuracy: 0.8139 - lr: 5.0000e-04\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4286 - accuracy: 0.8130 - val_loss: 0.4136 - val_accuracy: 0.8202 - lr: 5.0000e-04\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4300 - accuracy: 0.8126 - val_loss: 0.4105 - val_accuracy: 0.8188 - lr: 5.0000e-04\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 116s 146ms/step - loss: 0.4307 - accuracy: 0.8108 - val_loss: 0.4153 - val_accuracy: 0.8125 - lr: 5.0000e-04\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4291 - accuracy: 0.8134 - val_loss: 0.4114 - val_accuracy: 0.8170 - lr: 5.0000e-04\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4290 - accuracy: 0.8121 - val_loss: 0.4122 - val_accuracy: 0.8203 - lr: 5.0000e-04\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 116s 146ms/step - loss: 0.4275 - accuracy: 0.8127 - val_loss: 0.4163 - val_accuracy: 0.8145 - lr: 5.0000e-04\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 116s 145ms/step - loss: 0.4295 - accuracy: 0.8115 - val_loss: 0.4189 - val_accuracy: 0.8117 - lr: 5.0000e-04\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4295 - accuracy: 0.8127 - val_loss: 0.4174 - val_accuracy: 0.8189 - lr: 5.0000e-04\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4307 - accuracy: 0.8108 - val_loss: 0.4259 - val_accuracy: 0.8091 - lr: 5.0000e-04\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 116s 145ms/step - loss: 0.4276 - accuracy: 0.8124 - val_loss: 0.4123 - val_accuracy: 0.8195 - lr: 5.0000e-04\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4263 - accuracy: 0.8126 - val_loss: 0.4111 - val_accuracy: 0.8191 - lr: 5.0000e-04\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 124s 155ms/step - loss: 0.4275 - accuracy: 0.8117 - val_loss: 0.4085 - val_accuracy: 0.8211 - lr: 5.0000e-04\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4285 - accuracy: 0.8119 - val_loss: 0.4159 - val_accuracy: 0.8191 - lr: 5.0000e-04\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 116s 145ms/step - loss: 0.4297 - accuracy: 0.8138 - val_loss: 0.4136 - val_accuracy: 0.8180 - lr: 5.0000e-04\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4297 - accuracy: 0.8128 - val_loss: 0.4082 - val_accuracy: 0.8192 - lr: 5.0000e-04\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4284 - accuracy: 0.8108 - val_loss: 0.4128 - val_accuracy: 0.8167 - lr: 5.0000e-04\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 124s 155ms/step - loss: 0.4276 - accuracy: 0.8143 - val_loss: 0.4136 - val_accuracy: 0.8213 - lr: 5.0000e-04\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4264 - accuracy: 0.8147 - val_loss: 0.4135 - val_accuracy: 0.8156 - lr: 5.0000e-04\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 116s 146ms/step - loss: 0.4286 - accuracy: 0.8105 - val_loss: 0.4094 - val_accuracy: 0.8189 - lr: 5.0000e-04\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4235 - accuracy: 0.8138 - val_loss: 0.4118 - val_accuracy: 0.8155 - lr: 5.0000e-04\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4301 - accuracy: 0.8130 - val_loss: 0.4102 - val_accuracy: 0.8188 - lr: 5.0000e-04\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4274 - accuracy: 0.8124 - val_loss: 0.4152 - val_accuracy: 0.8169 - lr: 5.0000e-04\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4274 - accuracy: 0.8130 - val_loss: 0.4123 - val_accuracy: 0.8188 - lr: 5.0000e-04\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4293 - accuracy: 0.8143 - val_loss: 0.4121 - val_accuracy: 0.8200 - lr: 5.0000e-04\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 124s 155ms/step - loss: 0.4299 - accuracy: 0.8114 - val_loss: 0.4136 - val_accuracy: 0.8234 - lr: 5.0000e-04\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4201 - accuracy: 0.8166 - val_loss: 0.4159 - val_accuracy: 0.8134 - lr: 2.5000e-04\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4192 - accuracy: 0.8171 - val_loss: 0.4106 - val_accuracy: 0.8213 - lr: 2.5000e-04\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 116s 145ms/step - loss: 0.4178 - accuracy: 0.8166 - val_loss: 0.4100 - val_accuracy: 0.8209 - lr: 2.5000e-04\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4195 - accuracy: 0.8184 - val_loss: 0.4119 - val_accuracy: 0.8178 - lr: 2.5000e-04\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4174 - accuracy: 0.8182 - val_loss: 0.4121 - val_accuracy: 0.8173 - lr: 2.5000e-04\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 116s 146ms/step - loss: 0.4177 - accuracy: 0.8182 - val_loss: 0.4127 - val_accuracy: 0.8183 - lr: 2.5000e-04\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4189 - accuracy: 0.8173 - val_loss: 0.4080 - val_accuracy: 0.8205 - lr: 2.5000e-04\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4167 - accuracy: 0.8182 - val_loss: 0.4120 - val_accuracy: 0.8164 - lr: 2.5000e-04\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4160 - accuracy: 0.8190 - val_loss: 0.4123 - val_accuracy: 0.8155 - lr: 2.5000e-04\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4152 - accuracy: 0.8190 - val_loss: 0.4132 - val_accuracy: 0.8155 - lr: 2.5000e-04\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4124 - accuracy: 0.8215 - val_loss: 0.4103 - val_accuracy: 0.8216 - lr: 2.5000e-04\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4163 - accuracy: 0.8197 - val_loss: 0.4079 - val_accuracy: 0.8214 - lr: 2.5000e-04\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 116s 146ms/step - loss: 0.4145 - accuracy: 0.8202 - val_loss: 0.4093 - val_accuracy: 0.8167 - lr: 2.5000e-04\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4125 - accuracy: 0.8198 - val_loss: 0.4124 - val_accuracy: 0.8178 - lr: 2.5000e-04\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4174 - accuracy: 0.8198 - val_loss: 0.4097 - val_accuracy: 0.8183 - lr: 2.5000e-04\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 116s 146ms/step - loss: 0.4157 - accuracy: 0.8168 - val_loss: 0.4063 - val_accuracy: 0.8192 - lr: 2.5000e-04\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 117s 146ms/step - loss: 0.4162 - accuracy: 0.8178 - val_loss: 0.4138 - val_accuracy: 0.8139 - lr: 2.5000e-04\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 116s 146ms/step - loss: 0.4142 - accuracy: 0.8188 - val_loss: 0.4142 - val_accuracy: 0.8184 - lr: 2.5000e-04\n",
      "200/200 [==============================] - 24s 109ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/09 06:09:57 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/10/09 06:09:57 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as serving, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp3y_ic57_\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp3y_ic57_\\model\\data\\model\\assets\n",
      "2025/10/09 06:10:47 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp3y_ic57_\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/10/09 06:10:47 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "for rnn_size in [64, 192, 256]:\n",
    "    tf.keras.backend.clear_session()\n",
    "    print(f\"Running test with BERT model : Kapiche/twitter-roberta-base-sentiment\")\n",
    "    test_bert_model_other_v2(bert_model_name=\"Kapiche/twitter-roberta-base-sentiment\", rnn_size=rnn_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc472cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a3603fe",
   "metadata": {},
   "source": [
    "# Last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6225eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_size = 256\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "def test_bert_model_other_v2(bert_model_name, rnn_size=rnn_size):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_input(dataset)\n",
    "        mlflow.log_params(params={\n",
    "            'rnn_size': rnn_size, \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr,\n",
    "            'bert_model_name':bert_model_name\n",
    "        })\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "        encodings_train = tokenizer(X_train.to_list(), \n",
    "                                    truncation=True, \n",
    "                                    padding=True, \n",
    "                                    max_length=64,\n",
    "                                    return_tensors=\"tf\")\n",
    "        encodings_val = tokenizer(X_val.to_list(), \n",
    "                                  truncation=True, \n",
    "                                  padding=True, \n",
    "                                  max_length=64,\n",
    "                                  return_tensors=\"tf\")\n",
    "\n",
    "        dataset_train = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                {\"input_ids\": encodings_train[\"input_ids\"], \n",
    "                 \"attention_mask\": encodings_train[\"attention_mask\"]\n",
    "                 },y_train\n",
    "                )\n",
    "                ).batch(32)\n",
    "        \n",
    "        dataset_val = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                {\"input_ids\": encodings_val[\"input_ids\"], \n",
    "                 \"attention_mask\": encodings_val[\"attention_mask\"]\n",
    "                 },y_val\n",
    "                )\n",
    "                ).batch(32)\n",
    "        # On charge le mod√®le pr√©-entrainn√©\n",
    "        try:\n",
    "            base_model = TFAutoModel.from_pretrained(bert_model_name, from_pt=False)# Forcer la version tensorflow\n",
    "            mlflow.log_param(\"from_pt\", False)\n",
    "        except:\n",
    "            print(bert_model_name, \" not available in TF\")\n",
    "            base_model = TFAutoModel.from_pretrained(bert_model_name,from_pt=True)\n",
    "            mlflow.log_param(\"from_pt\", True)\n",
    "\n",
    "            \n",
    "    \n",
    "        base_model.trainable = False # Pas de fine-tuning ou d'entrainement car impossible √† faire avec les ressources disponibles\n",
    "\n",
    "        # Construction du mod√®le keras \n",
    "        ## Une input layer pour les input ids\n",
    "        input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "        ## Une input layer pour le masque d'attention\n",
    "        attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "        ## On r√©cup√®re \n",
    "\n",
    "        outputs = base_model(input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n",
    "\n",
    "        # On prend le token [CLS] comme vecteur de phrase\n",
    "        cls_token = token_embeddings[:, 0, :]\n",
    "        max_tokens = tf.reduce_max(token_embeddings, axis=1)  #\n",
    "        mean_tokens = tf.reduce_mean(token_embeddings, axis=1)  #\n",
    "\n",
    "        all_tokens = tf.concat([cls_token, max_tokens, mean_tokens], axis=-1)  # [batch, hidden_size*2]\n",
    "    \n",
    "        x = tf.keras.layers.Dense(rnn_size, activation=\"relu\", \n",
    "                                  kernel_regularizer=regularizers.L2(1e-4),\n",
    "                                  bias_regularizer=regularizers.L2(1e-4), \n",
    "                                #   activity_regularizer=regularizers.L2(1e-4),\n",
    "                                  )(all_tokens)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        logits = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "        model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=logits)\n",
    "\n",
    "        ## Callbacks\n",
    "        model_savepath = f\"./Models/MY_{'_'.join(bert_model_name.split('/'))}_dense{rnn_size}.h5\"\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=True, mode='max')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=20)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "\n",
    "\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "        # Summary\n",
    "        model.summary()\n",
    "        # History\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(dataset_train, epochs=epochs, batch_size=64, validation_data=dataset_val, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "                # Pr√©dictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(dataset_val)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des m√©triques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du mod√®le dans MLflow\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30fbc961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with BERT model Kapiche/twitter-roberta-base-sentiment\n",
      "Kapiche/twitter-roberta-base-sentiment  not available in TF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['roberta.embeddings.position_ids', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model_6 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, None                                               \n",
      "                                , 768),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None, 768)         0           ['tf_roberta_model_6[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_3 (TFOpLamb  (None, 768)         0           ['tf_roberta_model_6[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_3 (TFOpLam  (None, 768)         0           ['tf_roberta_model_6[0][0]']     \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.concat_3 (TFOpLambda)       (None, 2304)         0           ['tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.math.reduce_max_3[0][0]',   \n",
      "                                                                  'tf.math.reduce_mean_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 256)          590080      ['tf.concat_3[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_262 (Dropout)          (None, 256)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            257         ['dropout_262[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 125,235,969\n",
      "Trainable params: 590,337\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 159s 187ms/step - loss: 0.5132 - accuracy: 0.7846 - val_loss: 0.4458 - val_accuracy: 0.8127 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 143s 178ms/step - loss: 0.4725 - accuracy: 0.7966 - val_loss: 0.4320 - val_accuracy: 0.8103 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 148s 185ms/step - loss: 0.4619 - accuracy: 0.7984 - val_loss: 0.4259 - val_accuracy: 0.8142 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 144s 180ms/step - loss: 0.4592 - accuracy: 0.7999 - val_loss: 0.4268 - val_accuracy: 0.8117 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 144s 180ms/step - loss: 0.4566 - accuracy: 0.8014 - val_loss: 0.4219 - val_accuracy: 0.8158 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 150s 187ms/step - loss: 0.4549 - accuracy: 0.8007 - val_loss: 0.4230 - val_accuracy: 0.8128 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 150s 188ms/step - loss: 0.4523 - accuracy: 0.8025 - val_loss: 0.4209 - val_accuracy: 0.8167 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 133s 166ms/step - loss: 0.4483 - accuracy: 0.8027 - val_loss: 0.4307 - val_accuracy: 0.8159 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 133s 166ms/step - loss: 0.4536 - accuracy: 0.8045 - val_loss: 0.4234 - val_accuracy: 0.8112 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 129s 162ms/step - loss: 0.4492 - accuracy: 0.8057 - val_loss: 0.4192 - val_accuracy: 0.8167 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 139s 174ms/step - loss: 0.4486 - accuracy: 0.8035 - val_loss: 0.4196 - val_accuracy: 0.8172 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 133s 166ms/step - loss: 0.4492 - accuracy: 0.8041 - val_loss: 0.4251 - val_accuracy: 0.8114 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 137s 172ms/step - loss: 0.4469 - accuracy: 0.8051 - val_loss: 0.4188 - val_accuracy: 0.8173 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 140s 176ms/step - loss: 0.4464 - accuracy: 0.8066 - val_loss: 0.4194 - val_accuracy: 0.8180 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 131s 164ms/step - loss: 0.4470 - accuracy: 0.8050 - val_loss: 0.4198 - val_accuracy: 0.8128 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 131s 164ms/step - loss: 0.4480 - accuracy: 0.8052 - val_loss: 0.4213 - val_accuracy: 0.8167 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 130s 162ms/step - loss: 0.4487 - accuracy: 0.8008 - val_loss: 0.4240 - val_accuracy: 0.8141 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 127s 159ms/step - loss: 0.4488 - accuracy: 0.8021 - val_loss: 0.4164 - val_accuracy: 0.8163 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.4505 - accuracy: 0.8051 - val_loss: 0.4208 - val_accuracy: 0.8141 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 131s 164ms/step - loss: 0.4495 - accuracy: 0.8055 - val_loss: 0.4187 - val_accuracy: 0.8172 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 134s 167ms/step - loss: 0.4503 - accuracy: 0.8037 - val_loss: 0.4215 - val_accuracy: 0.8153 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 133s 166ms/step - loss: 0.4481 - accuracy: 0.8059 - val_loss: 0.4221 - val_accuracy: 0.8175 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 153s 191ms/step - loss: 0.4473 - accuracy: 0.8045 - val_loss: 0.4178 - val_accuracy: 0.8163 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 127s 158ms/step - loss: 0.4473 - accuracy: 0.8051 - val_loss: 0.4209 - val_accuracy: 0.8169 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 130s 163ms/step - loss: 0.4487 - accuracy: 0.8052 - val_loss: 0.4190 - val_accuracy: 0.8147 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 138s 173ms/step - loss: 0.4486 - accuracy: 0.8029 - val_loss: 0.4224 - val_accuracy: 0.8175 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 151s 188ms/step - loss: 0.4497 - accuracy: 0.8047 - val_loss: 0.4197 - val_accuracy: 0.8180 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 140s 175ms/step - loss: 0.4471 - accuracy: 0.8054 - val_loss: 0.4169 - val_accuracy: 0.8163 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 145s 182ms/step - loss: 0.4384 - accuracy: 0.8095 - val_loss: 0.4195 - val_accuracy: 0.8161 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 132s 166ms/step - loss: 0.4367 - accuracy: 0.8082 - val_loss: 0.4199 - val_accuracy: 0.8178 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 132s 166ms/step - loss: 0.4335 - accuracy: 0.8118 - val_loss: 0.4130 - val_accuracy: 0.8170 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 139s 174ms/step - loss: 0.4322 - accuracy: 0.8118 - val_loss: 0.4119 - val_accuracy: 0.8177 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 140s 175ms/step - loss: 0.4319 - accuracy: 0.8109 - val_loss: 0.4134 - val_accuracy: 0.8170 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 125s 156ms/step - loss: 0.4314 - accuracy: 0.8108 - val_loss: 0.4127 - val_accuracy: 0.8206 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 121s 151ms/step - loss: 0.4298 - accuracy: 0.8125 - val_loss: 0.4167 - val_accuracy: 0.8170 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 140s 175ms/step - loss: 0.4315 - accuracy: 0.8096 - val_loss: 0.4169 - val_accuracy: 0.8184 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 98s 122ms/step - loss: 0.4329 - accuracy: 0.8104 - val_loss: 0.4138 - val_accuracy: 0.8155 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 129s 162ms/step - loss: 0.4309 - accuracy: 0.8111 - val_loss: 0.4139 - val_accuracy: 0.8153 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 124s 155ms/step - loss: 0.4298 - accuracy: 0.8103 - val_loss: 0.4144 - val_accuracy: 0.8184 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 139s 174ms/step - loss: 0.4304 - accuracy: 0.8142 - val_loss: 0.4158 - val_accuracy: 0.8159 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 143s 179ms/step - loss: 0.4310 - accuracy: 0.8093 - val_loss: 0.4111 - val_accuracy: 0.8191 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 128s 160ms/step - loss: 0.4305 - accuracy: 0.8109 - val_loss: 0.4095 - val_accuracy: 0.8202 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 109s 137ms/step - loss: 0.4288 - accuracy: 0.8116 - val_loss: 0.4190 - val_accuracy: 0.8144 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 132s 166ms/step - loss: 0.4307 - accuracy: 0.8146 - val_loss: 0.4094 - val_accuracy: 0.8188 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 185s 231ms/step - loss: 0.4306 - accuracy: 0.8100 - val_loss: 0.4082 - val_accuracy: 0.8223 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 126s 157ms/step - loss: 0.4298 - accuracy: 0.8121 - val_loss: 0.4166 - val_accuracy: 0.8145 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 141s 176ms/step - loss: 0.4304 - accuracy: 0.8133 - val_loss: 0.4064 - val_accuracy: 0.8225 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 158s 198ms/step - loss: 0.4291 - accuracy: 0.8127 - val_loss: 0.4098 - val_accuracy: 0.8241 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 145s 181ms/step - loss: 0.4311 - accuracy: 0.8110 - val_loss: 0.4129 - val_accuracy: 0.8163 - lr: 5.0000e-04\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 136s 169ms/step - loss: 0.4290 - accuracy: 0.8132 - val_loss: 0.4153 - val_accuracy: 0.8155 - lr: 5.0000e-04\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 135s 168ms/step - loss: 0.4281 - accuracy: 0.8117 - val_loss: 0.4110 - val_accuracy: 0.8189 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 135s 169ms/step - loss: 0.4255 - accuracy: 0.8115 - val_loss: 0.4120 - val_accuracy: 0.8198 - lr: 5.0000e-04\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 148s 185ms/step - loss: 0.4284 - accuracy: 0.8121 - val_loss: 0.4123 - val_accuracy: 0.8200 - lr: 5.0000e-04\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 140s 175ms/step - loss: 0.4284 - accuracy: 0.8139 - val_loss: 0.4141 - val_accuracy: 0.8189 - lr: 5.0000e-04\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 135s 168ms/step - loss: 0.4283 - accuracy: 0.8117 - val_loss: 0.4131 - val_accuracy: 0.8170 - lr: 5.0000e-04\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 126s 157ms/step - loss: 0.4298 - accuracy: 0.8129 - val_loss: 0.4089 - val_accuracy: 0.8214 - lr: 5.0000e-04\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 144s 179ms/step - loss: 0.4323 - accuracy: 0.8122 - val_loss: 0.4154 - val_accuracy: 0.8177 - lr: 5.0000e-04\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 160s 201ms/step - loss: 0.4221 - accuracy: 0.8162 - val_loss: 0.4132 - val_accuracy: 0.8156 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 142s 177ms/step - loss: 0.4215 - accuracy: 0.8134 - val_loss: 0.4125 - val_accuracy: 0.8188 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 144s 179ms/step - loss: 0.4205 - accuracy: 0.8161 - val_loss: 0.4121 - val_accuracy: 0.8141 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 131s 164ms/step - loss: 0.4177 - accuracy: 0.8176 - val_loss: 0.4100 - val_accuracy: 0.8167 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 133s 166ms/step - loss: 0.4184 - accuracy: 0.8187 - val_loss: 0.4114 - val_accuracy: 0.8136 - lr: 2.5000e-04\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 131s 164ms/step - loss: 0.4183 - accuracy: 0.8154 - val_loss: 0.4124 - val_accuracy: 0.8133 - lr: 2.5000e-04\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.4164 - accuracy: 0.8170 - val_loss: 0.4142 - val_accuracy: 0.8130 - lr: 2.5000e-04\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 130s 162ms/step - loss: 0.4175 - accuracy: 0.8176 - val_loss: 0.4075 - val_accuracy: 0.8198 - lr: 2.5000e-04\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 129s 161ms/step - loss: 0.4182 - accuracy: 0.8188 - val_loss: 0.4154 - val_accuracy: 0.8139 - lr: 2.5000e-04\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 124s 155ms/step - loss: 0.4179 - accuracy: 0.8209 - val_loss: 0.4110 - val_accuracy: 0.8192 - lr: 2.5000e-04\n",
      "200/200 [==============================] - 26s 115ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/09 20:34:47 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/10/09 20:34:48 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as serving, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpz06ietb2\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpz06ietb2\\model\\data\\model\\assets\n",
      "2025/10/09 20:35:48 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpz06ietb2\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/10/09 20:35:48 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Running test with BERT model Kapiche/twitter-roberta-base-sentiment\")\n",
    "model, tokenizer  = test_bert_model_other_v2(bert_model_name=\"Kapiche/twitter-roberta-base-sentiment\", rnn_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8049c6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x21f33466ef0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fb7041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/Best_BERT_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/Best_BERT_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"../Models/Best_BERT_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f1055a",
   "metadata": {},
   "source": [
    "# Sauvegarde du mod√®le "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "decfe8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/Model_BERT_save_with_tf2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/Model_BERT_save_with_tf2\\assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# On d√©finit une fonction de service avec les bons inputs\n",
    "@tf.function(input_signature=[\n",
    "    {\n",
    "        \"input_ids\": tf.TensorSpec([None, None], tf.int32, name=\"input_ids\"),\n",
    "        \"attention_mask\": tf.TensorSpec([None, None], tf.int32, name=\"attention_mask\"),\n",
    "    }\n",
    "])\n",
    "def serving_fn(inputs):\n",
    "    return model(inputs)\n",
    "\n",
    "# Sauvegarde au format TensorFlow\n",
    "tf.saved_model.save(model, \"../Models/Model_BERT_save_with_tf2\", signatures={\"serving_default\": serving_fn})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1b53ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/Best_BERT_model_save_keras\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/Best_BERT_model_save_keras\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"../Models/Best_BERT_model_save_keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e8e5c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/Best_BERT_model_save_keras_v2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/Best_BERT_model_save_keras_v2\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.keras.models.save_model(model, \"../Models/Best_BERT_model_save_keras_v2\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab6622ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model_6 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, None                                               \n",
      "                                , 768),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None, 768)         0           ['tf_roberta_model_6[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_3 (TFOpLamb  (None, 768)         0           ['tf_roberta_model_6[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_3 (TFOpLam  (None, 768)         0           ['tf_roberta_model_6[0][0]']     \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.concat_3 (TFOpLambda)       (None, 2304)         0           ['tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.math.reduce_max_3[0][0]',   \n",
      "                                                                  'tf.math.reduce_mean_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 256)          590080      ['tf.concat_3[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_262 (Dropout)          (None, 256)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            257         ['dropout_262[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 125,235,969\n",
      "Trainable params: 590,337\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5039d",
   "metadata": {},
   "source": [
    "# Sauvegarde du tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bea67fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Models/tokenizer_BERT_model_v1\\\\tokenizer_config.json',\n",
       " '../Models/tokenizer_BERT_model_v1\\\\special_tokens_map.json',\n",
       " '../Models/tokenizer_BERT_model_v1\\\\vocab.json',\n",
       " '../Models/tokenizer_BERT_model_v1\\\\merges.txt',\n",
       " '../Models/tokenizer_BERT_model_v1\\\\added_tokens.json',\n",
       " '../Models/tokenizer_BERT_model_v1\\\\tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"../Models/tokenizer_BERT_model_v1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2cb458",
   "metadata": {},
   "source": [
    "# Chargement du mod√®le "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load(\"../Models/Best_BERT_model2\")\n",
    "infer = loaded.signatures[\"serving_default\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22402d3",
   "metadata": {},
   "source": [
    "# Chargement du tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa14796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    }
   ],
   "source": [
    "tokenizer_loaded = AutoTokenizer.from_pretrained(\"../Models/Best_BERT_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc6e07b",
   "metadata": {},
   "source": [
    "# Texte de test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8321bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"it wasn't so bad actually  \", \"bad\", \"average\"]\n",
    "\n",
    "test_encoding = tokenizer_loaded(text, truncation=True, padding=True, return_tensors=\"tf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7f70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_0': <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
      "array([[0.5667334 ],\n",
      "       [0.01679735],\n",
      "       [0.1408657 ]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "test_output = infer(**{\n",
    "    \"input_ids\": test_encoding[\"input_ids\"],\n",
    "    \"attention_mask\": test_encoding[\"attention_mask\"],\n",
    "    \"token_type_ids\": test_encoding.get(\"token_type_ids\", tf.zeros_like(test_encoding[\"input_ids\"]))\n",
    "})\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bdb7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_output['output_0'].numpy()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47fd799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:42:04) [MSC v.1943 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d7f6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\Models\\\\Best_BERT_model2'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(\"..\",\"Models\",\"Best_BERT_model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c8b567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=({'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 128), dtype=tf.int32, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6408916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "token_type_ids\n",
      "attention_mask\n"
     ]
    }
   ],
   "source": [
    "for k in encodings_train.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a8352b",
   "metadata": {},
   "source": [
    "# Conversion tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d526af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create a Keras model from SavedModel at ../Models/Best_BERT_model2. This SavedModel was exported with `tf.saved_model.save`, and lacks the Keras metadata file. Please save your Keras model by calling `model.save` or `tf.keras.models.save_model`. Note that you can still load this SavedModel with `tf.saved_model.load`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Charger le mod√®le keras\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../Models/Best_BERT_model2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Convertir vers TFLite\u001b[39;00m\n\u001b[0;32m      7\u001b[0m converter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mTFLiteConverter\u001b[38;5;241m.\u001b[39mfrom_keras_model(model)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\keras\\saving\\saved_model\\load.py:220\u001b[0m, in \u001b[0;36m_read_legacy_metadata\u001b[1;34m(object_graph_def, metadata, path)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    215\u001b[0m     proto\u001b[38;5;241m.\u001b[39mWhichOneof(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m proto\u001b[38;5;241m.\u001b[39muser_object\u001b[38;5;241m.\u001b[39midentifier\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;129;01min\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mKERAS_OBJECT_IDENTIFIERS\n\u001b[0;32m    218\u001b[0m ):\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proto\u001b[38;5;241m.\u001b[39muser_object\u001b[38;5;241m.\u001b[39mmetadata:\n\u001b[1;32m--> 220\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    221\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create a Keras model from SavedModel at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This SavedModel was exported with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    223\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.saved_model.save`, and lacks the Keras metadata file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease save your Keras model by calling `model.save` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `tf.keras.models.save_model`. Note that \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    226\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can still load this SavedModel with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    227\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.saved_model.load`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    228\u001b[0m         )\n\u001b[0;32m    229\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    230\u001b[0m         node_id\u001b[38;5;241m=\u001b[39mnode_id,\n\u001b[0;32m    231\u001b[0m         node_path\u001b[38;5;241m=\u001b[39mnode_paths[node_id],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    236\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mproto\u001b[38;5;241m.\u001b[39muser_object\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[0;32m    237\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create a Keras model from SavedModel at ../Models/Best_BERT_model2. This SavedModel was exported with `tf.saved_model.save`, and lacks the Keras metadata file. Please save your Keras model by calling `model.save` or `tf.keras.models.save_model`. Note that you can still load this SavedModel with `tf.saved_model.load`."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Charger le mod√®le keras\n",
    "model = tf.keras.models.load_model(\"../Models/Best_BERT_model2\", compile=False)\n",
    "\n",
    "# Convertir vers TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Sauvegarde\n",
    "with open(\"bert_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bbb0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/Best_BERT_model_v3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/Best_BERT_model_v3\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"../Models/Best_BERT_model_v3\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = tf.keras.models.load_model(\"../Models/Best_BERT_model_v3\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0defe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env_P7_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
