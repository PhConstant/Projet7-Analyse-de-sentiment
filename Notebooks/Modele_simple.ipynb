{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f8d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import product\n",
    "from Source.preprocess_data import *  ## import all functions from preprocess_data.py\n",
    "from Source.postprocess_data import * ## import all functions from postprocess_data.py\n",
    "from Source.utils import *  ## import all functions from utils.py\n",
    "import nltk\n",
    "\n",
    "\n",
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db79dd0",
   "metadata": {},
   "source": [
    "# Prétraitement du dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca04b81",
   "metadata": {},
   "source": [
    "## Extraction d'un faible pourcentage de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a5f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/AI+Engineer/Project+7%C2%A0-+D%C3%A9tectez+les+Bad+Buzz+gr%C3%A2ce+au+Deep+Learning/sentiment140.zip',\n",
    "                header=None,\n",
    "                compression='zip',\n",
    "                encoding='cp1252')\n",
    "\n",
    "df.columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "sample_df, _ = train_test_split(df, test_size=0.9, random_state=42, stratify=df['target'])\n",
    "sample_df = sample_df.reset_index(drop=True)\n",
    "print(f\"Sample size: {sample_df.shape[0]} rows\")\n",
    "# On ne garde que les colonnes 'target' et 'text'\n",
    "sample_df = sample_df[['target', 'text']]\n",
    "sample_df.to_csv('Data/raw_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab922b4",
   "metadata": {},
   "source": [
    "## Normalisation du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc0f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation du texte\n",
    "sample_df['text'] = sample_df['text'].apply(lambda x: normalize_text(x))\n",
    "# Enregistrement du jeu de données pré-traité\n",
    "sample_df.to_csv('Data/normalized_data.csv', index=False)\n",
    "print(f\"Normalized {sample_df.shape[0]} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1669e515",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b9720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords\n",
    "nltk.download('stopwords')\n",
    "#Vocabulaire\n",
    "nltk.download('words')\n",
    "#Punctuation\n",
    "nltk.download('punkt_tab')\n",
    "#Wordnet\n",
    "nltk.download('wordnet')\n",
    "#POS-tagging\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6681f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.sample(10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5475c9bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5391ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df[\"target\"] = sample_df[\"target\"].apply(lambda x: 0 if x == 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c043ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd814be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='target',hue='target', data=sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3243e1",
   "metadata": {},
   "source": [
    "Dataset équilibré, pas besoin de rééquilibrer la cible.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0429d378",
   "metadata": {},
   "source": [
    "## Experimentation sur l'impact du feature engineering (tokenisation, stemming/lemmatisation, vectorisation) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11928708",
   "metadata": {},
   "source": [
    "### Mise en place de l'environnement MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide an Experiment description that will appear in the UI\n",
    "\n",
    "mlflow.set_experiment(\"Tokenization experiment\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"Tokenization experiment\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Cette experience contient les différents tests pour le <<modèle sur mesure simple>>. \"\n",
    "    \"Le but est d'évaluer un modèle simple permettant d'évaluer les sentiments dans les tweets à partir d'un modèle de régression simple.\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"simple-regression\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d05ca8",
   "metadata": {},
   "source": [
    "### Fonction de run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7f2007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import TweetTokenizer, WordPunctTokenizer, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, LancasterStemmer\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df367f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model1_experiment(\n",
    "    df: pd.DataFrame,\n",
    "    params: dict\n",
    "):\n",
    "    \"\"\"\n",
    "    Lance une expérience MLflow avec les paramètres fournis.\n",
    "\n",
    "    Paramètres attendus dans params :\n",
    "        - stop_words : liste de stopwords ou None (default=None)\n",
    "        - tokenizer : fonction de tokenisation (default=None)\n",
    "        - stem_lem_func : fonction de stemming/lemmatisation (default=None)\n",
    "        - vectorizertype : type de vectoriseur, CountVectorizer ou TfidfVectorizer (default=CountVectorizer)\n",
    "        - min_df : int, fréquence minimale des mots (default=1)\n",
    "        - ngram_range : tuple, plage des n-grammes (default=(1,1)), \n",
    "        - model_name : nom du modèle pour enregistrement dans MLflow (default=\"log_regression_v1\")\n",
    "    \"\"\"\n",
    "    # Paramètres par défaut\n",
    "    default_params = {\n",
    "\n",
    "        \"stop_words\": None,\n",
    "        \"tokenizer\": RegexpTokenizer(r'\\w+').tokenize,\n",
    "        \"stem_lem_func\": WordNetLemmatizer().lemmatize,\n",
    "        \"vectorizertype\": \"CountVectorizer\",\n",
    "        \"min_df\": 1,\n",
    "        \"ngram_range\": (1, 1), \n",
    "        }\n",
    "    # Met à jour les paramètres manquants\n",
    "    for k, v in default_params.items():\n",
    "        if k not in params:\n",
    "            params[k] = v\n",
    "\n",
    "\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        X_raw = df['text']\n",
    "        y = df['target']\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_raw, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "        # Prétraitement des textes\n",
    "        ## Jeu de données d'entraînement\n",
    "        X_train_stem = X_train.apply(lambda x:preprocess_text_simple(\n",
    "                                                                    x,\n",
    "                                                                    tokenizer=params[\"tokenizer\"],\n",
    "                                                                    stem_lem_func=params[\"stem_lem_func\"]\n",
    "                                                                    )\n",
    "                                    )\n",
    "        ## Jeu de données de validation\n",
    "        X_val_stem = X_val.apply(lambda x:preprocess_text_simple(\n",
    "                                                                x,\n",
    "                                                                tokenizer=params[\"tokenizer\"],\n",
    "                                                                stem_lem_func=params[\"stem_lem_func\"]\n",
    "                                                                )\n",
    "                                )\n",
    "\n",
    "        # Selection du vectorizer\n",
    "        match params[\"vectorizertype\"]:\n",
    "            case \"CountVectorizer\":\n",
    "                vectorizer = CountVectorizer(min_df=params[\"min_df\"], ngram_range=params[\"ngram_range\"], stop_words=params[\"stop_words\"])\n",
    "            case \"TfidfVectorizer\":\n",
    "                vectorizer = TfidfVectorizer(min_df=params[\"min_df\"], ngram_range=params[\"ngram_range\"], stop_words=params[\"stop_words\"])\n",
    "            case _:\n",
    "                raise ValueError(\"Invalid vectorizer type\")\n",
    "        # Création du pipeline\n",
    "        pipe = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', LogisticRegression(C=1.0, max_iter=1000))\n",
    "        ])\n",
    "        # Entraînement du modèle\n",
    "        pipe.fit(X_train_stem, y_train)\n",
    "        # Prédictions sur le jeu de validation\n",
    "        y_val_pred = pipe.predict(X_val_stem)\n",
    "        y_val_proba = pipe.predict_proba(X_val_stem)[:, 1]\n",
    "        # Évaluation du modèle\n",
    "        output_dict = postprocess_model_output(y_val, y_val_pred, y_val_proba) # voir postprocess_data.py\n",
    "        # Logging des métriques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_val_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.sklearn.log_model(pipe, \"model\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c4aa10",
   "metadata": {},
   "source": [
    "### Test baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578b7c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline avec des paramètres par défaut\n",
    "\n",
    "params = {}\n",
    "model1_experiment(df=sample_df, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ef426",
   "metadata": {},
   "source": [
    "### Grille de paramètres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc08c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {\n",
    "        \"stop_words\": [None, stopwords.words('english')],\n",
    "        \"tokenizer\": [RegexpTokenizer(r'\\w+').tokenize, TweetTokenizer().tokenize, WordPunctTokenizer().tokenize],\n",
    "        \"stem_lem_func\": [WordNetLemmatizer().lemmatize, LancasterStemmer().stem, PorterStemmer().stem],\n",
    "        \"vectorizertype\": [\"CountVectorizer\", \"TfidfVectorizer\"],\n",
    "        \"min_df\": [1, 5, 10],\n",
    "        \"ngram_range\": [(1, 1), (1, 2), (1, 3)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4e1956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    "param_names = list(param_grid.keys())\n",
    "\n",
    "print(f\"Total combinations to try: {len(param_combinations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f09288",
   "metadata": {},
   "source": [
    "### Lancement de la grille de paramètres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7e4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, comb in enumerate(param_combinations):\n",
    "#     params = dict(zip(param_names, comb))\n",
    "#     print(f\"Running combination {i+1}/{len(param_combinations)}: {params}\")\n",
    "#     model1_experiment(df=sample_df, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4919a1f",
   "metadata": {},
   "source": [
    "### Sélection du meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac27f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection du meilleur modèle\n",
    "# Reset client ? \n",
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")\n",
    "experiment_id = mlflow.get_experiment_by_name(\"Tokenization experiment\").experiment_id\n",
    "runs = client.search_runs(experiment_id)\n",
    "\n",
    "# Métrique pour sélectionner le meilleur modèle\n",
    "metric_to_optimize = \"Accuracy\" # liste des métriques enregistrées dans postprocess_data.py ou sur l'UI MLflow\n",
    "best_run = max(runs, key=lambda run: run.data.metrics.get(metric_to_optimize, float('-inf')))\n",
    "print(f\"Best run ID: {best_run.info.run_id} with metrics:\")\n",
    "for key, value in best_run.data.metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Best run parameters:\")\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Enregistrement du meilleur modèle\n",
    "best_model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "registered_model_name = \"log_regression_model\"\n",
    "registered_model = mlflow.register_model(best_model_uri, registered_model_name)\n",
    "# Enregistrement des paramètres sous forme de tags dans le modèle enregistré\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"Setting tag {key} = {value} in registered model\")\n",
    "    client.set_model_version_tag(\n",
    "        name=registered_model_name,\n",
    "        version=str(registered_model.version),\n",
    "        key=str(key),\n",
    "        value=str(value))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09676dc3",
   "metadata": {},
   "source": [
    "## Les paramètres de tokenization optimaux ont été établis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92249e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Logreg hyperparameter exepriment\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"Logreg hyperparameter exepriment\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Cette experience contient les différents tests pour le <<modèle sur mesure simple>>. \"\n",
    "    \"Le but est d'évaluer un modèle simple permettant d'évaluer les sentiments dans les tweets à partir d'un modèle de régression simple.\"\n",
    "    \"Ici on va chercher à optimiser les hyperparamètres de la régression logistique.\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"simple-regression\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7874f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tqdm\n",
    "\n",
    "# Data\n",
    "X_raw = sample_df['text']\n",
    "y = sample_df['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_raw, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Prétraitement des textes\n",
    "## Jeu de données d'entraînement\n",
    "print(\"Preprocessing training data...\")\n",
    "X_train_stem = X_train.apply(lambda x:preprocess_text_simple(\n",
    "                                                                    x,\n",
    "                                                                    tokenizer=TweetTokenizer().tokenize,\n",
    "                                                                    stem_lem_func=WordNetLemmatizer().lemmatize\n",
    "                                                                    )\n",
    "                                    )\n",
    "## Jeu de données de validation\n",
    "print(\"Preprocessing validation data...\")\n",
    "X_val_stem = X_val.apply(lambda x:preprocess_text_simple(\n",
    "                                                                x,\n",
    "                                                                tokenizer=TweetTokenizer().tokenize,\n",
    "                                                                stem_lem_func=WordNetLemmatizer().lemmatize\n",
    "                                                                )\n",
    "                                )\n",
    "print(\"Preprocessing done.\")\n",
    "# Définition de la fonction objective pour Optuna\n",
    "def logreg_eval(trial):\n",
    "    # Hyperparamètres\n",
    "    C = trial.suggest_float(\"C\", 1e-3, 1e2, log=True)   # numérique (régularisation)\n",
    "    max_iter = trial.suggest_int(\"max_iter\", 10, 1000, log=True)  # numérique (itérations)\n",
    "    tol = trial.suggest_float(\"tol\", 1e-5, 1e-1, log=True)  # numérique (tolérance)\n",
    "    penalty = trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\", \"elasticnet\"])  # catégoriel\n",
    "    l1_ratio = trial.suggest_float(\"l1_ratio\", 0, 1) if penalty == \"elasticnet\" else None\n",
    "    solver = trial.suggest_categorical(\"solver\", [\"liblinear\", \"saga\"])  # catégoriel\n",
    "    \n",
    "\n",
    "    # Attention : tous les solveurs ne supportent pas toutes les pénalités\n",
    "    if penalty == \"elasticnet\" and solver != \"saga\":\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    if penalty == \"l1\" and solver not in [\"liblinear\", \"saga\"]:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_params(params={\n",
    "            'C':C,               \n",
    "            'Max iterations': max_iter,\n",
    "            'tolerance': tol, \n",
    "            'penalty': penalty, \n",
    "            'l1_ratio': l1_ratio, \n",
    "            'Solver': solver \n",
    "        })\n",
    "        # Modèle\n",
    "        pipe = Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer(min_df=5, ngram_range=(1,3),stop_words=None)),\n",
    "            ('classifier', LogisticRegression(C=C, \n",
    "                                              max_iter=max_iter, \n",
    "                                              tol=tol, \n",
    "                                              penalty=penalty, \n",
    "                                              solver=solver, \n",
    "                                              l1_ratio=l1_ratio))\n",
    "        ])\n",
    "        # Entraînement du modèle\n",
    "        pipe.fit(X_train_stem, y_train)\n",
    "        # Prédictions sur le jeu de validation\n",
    "        y_val_pred = pipe.predict(X_val_stem)\n",
    "        y_val_proba = pipe.predict_proba(X_val_stem)[:, 1]\n",
    "        # Évaluation du modèle\n",
    "        output_dict = postprocess_model_output(y_val, y_val_pred, y_val_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des métriques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_val_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.sklearn.log_model(pipe, \"model\")\n",
    "        acc = output_dict[\"Accuracy\"]\n",
    "    return acc\n",
    "\n",
    "\n",
    "# Création de l'étude Optuna et optimisation\n",
    "print(\"Starting hyperparameter optimization with Optuna...\")\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"optuna_logreg_experiment\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"optuna_logreg_experiment\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Cette experience contient les différents tests pour le <<modèle sur mesure simple>>. \"\n",
    "    \"Le but est d'évaluer un modèle simple permettant d'évaluer les sentiments dans les tweets à partir d'un modèle de régression simple.\"\n",
    "    \"Ici on va chercher à optimiser les hyperparamètres de la régression logistique.\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"simple-regression\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)\n",
    "\n",
    "\n",
    "\n",
    "# Lancement de l'optimisation avec Optuna\n",
    "print(\"Starting optimization trials...\")\n",
    "with mlflow.start_run(run_name=\"optuna_logreg_optimization\"):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(logreg_eval, n_trials=50)\n",
    "\n",
    "    mlflow.log_params(study.best_params)\n",
    "    mlflow.log_metric(\"best_accuracy\", study.best_value)\n",
    "\n",
    "print(\"Optimization completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f341080",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")\n",
    "experiment_id = mlflow.get_experiment_by_name(\"optuna_logreg_experiment\").experiment_id\n",
    "runs = client.search_runs(experiment_id)\n",
    "\n",
    "# Métrique pour sélectionner le meilleur modèle\n",
    "metric_to_optimize = \"Accuracy\" # liste des métriques enregistrées dans postprocess_data.py ou sur l'UI MLflow\n",
    "best_run = max(runs, key=lambda run: run.data.metrics.get(metric_to_optimize, float('-inf')))\n",
    "print(f\"Best run ID: {best_run.info.run_id} with metrics:\")\n",
    "for key, value in best_run.data.metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Best run parameters:\")\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Enregistrement du meilleur modèle\n",
    "best_model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "registered_model_name = \"log_regression_model_opt\"\n",
    "registered_model = mlflow.register_model(best_model_uri, registered_model_name)\n",
    "# Enregistrement des paramètres sous forme de tags dans le modèle enregistré\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"Setting tag {key} = {value} in registered model\")\n",
    "    client.set_model_version_tag(\n",
    "        name=registered_model_name,\n",
    "        version=str(registered_model.version),\n",
    "        key=str(key),\n",
    "        value=str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d9b82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env_P7_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
