{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c92577",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8616ccf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "Num GPUs Available:  1\n",
      "GPUs disponibles : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Version TF : 2.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, SimpleRNN, Dense, LSTM, Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing\n",
    "from mlflow import MlflowClient\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from Source.preprocess_data import *  ## import all functions from preprocess_data.py\n",
    "from Source.postprocess_data import * ## import all functions from postprocess_data.py\n",
    "from Source.utils import *  ## import all functions from utils.py\n",
    "\n",
    "# import nltk\n",
    "# import optuna\n",
    "# ü§ó\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "# from nltk.corpus import stopwords  \n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "nw = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "\n",
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")\n",
    "os.environ[\"TF_KERAS\"]='1'\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPUs disponibles :\", tf.config.list_physical_devices(\"GPU\"))\n",
    "print(\"Version TF :\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f45b2",
   "metadata": {},
   "source": [
    "# Pr√©paration data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4e0f0a",
   "metadata": {},
   "source": [
    "## Importation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "196d7d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 16000 rows\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/AI+Engineer/Project+7%C2%A0-+D%C3%A9tectez+les+Bad+Buzz+gr%C3%A2ce+au+Deep+Learning/sentiment140.zip',\n",
    "                header=None,\n",
    "                compression='zip',\n",
    "                encoding='cp1252')\n",
    "\n",
    "df.columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "sample_df, _ = train_test_split(df, test_size=0.99, random_state=42, stratify=df['target'])\n",
    "sample_df = sample_df.reset_index(drop=True)\n",
    "print(f\"Sample size: {sample_df.shape[0]} rows\")\n",
    "# On ne garde que les colonnes 'target' et 'text'\n",
    "sample_df = sample_df[['target', 'text']]\n",
    "sample_df[\"target\"] = sample_df[\"target\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "sample_df.to_csv('Data/raw_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca7f92",
   "metadata": {},
   "source": [
    "## Train/Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c0ba358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X_raw = sample_df['text']\n",
    "y = sample_df['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_raw, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b8e27f",
   "metadata": {},
   "source": [
    "# Fonction centrale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78ad5576",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_size = 128\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "def test_bert_model(bert_model_name):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(params={\n",
    "            'rnn_size': rnn_size, \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr,\n",
    "            'bert_model_name':bert_model_name\n",
    "        })\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "        encodings_train = tokenizer(X_train.to_list(), \n",
    "                                    truncation=True, \n",
    "                                    padding=True, \n",
    "                                    max_length=64,\n",
    "                                    return_tensors=\"tf\")\n",
    "        encodings_val = tokenizer(X_val.to_list(), \n",
    "                                  truncation=True, \n",
    "                                  padding=True, \n",
    "                                  max_length = 64,\n",
    "                                  return_tensors=\"tf\")\n",
    "\n",
    "        dataset_train = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                {\"input_ids\": encodings_train[\"input_ids\"], \n",
    "                 \"attention_mask\": encodings_train[\"attention_mask\"]\n",
    "                 },y_train\n",
    "                )\n",
    "                ).batch(32)\n",
    "        \n",
    "        dataset_val = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                {\"input_ids\": encodings_val[\"input_ids\"], \n",
    "                 \"attention_mask\": encodings_val[\"attention_mask\"]\n",
    "                 },y_val\n",
    "                )\n",
    "                ).batch(32)\n",
    "        # On charge le mod√®le pr√©-entrainn√©\n",
    "        base_model = TFAutoModel.from_pretrained(bert_model_name, from_pt=True)\n",
    "        base_model.trainable = False # Pas de fine-tuning ou d'entrainement car impossible √† faire avec les ressources disponibles\n",
    "\n",
    "        # Construction du mod√®le keras \n",
    "        ## Une input layer pour les input ids\n",
    "        input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "        ## Une input layer pour le masque d'attention\n",
    "        attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "        ## On r√©cup√®re \n",
    "\n",
    "        outputs = base_model(input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n",
    "\n",
    "        # On prend le token [CLS] comme vecteur de phrase\n",
    "        cls_token = token_embeddings[:, 0, :]\n",
    "        max_tokens = tf.reduce_max(token_embeddings, axis=1)  #\n",
    "        mean_tokens = tf.reduce_mean(token_embeddings, axis=1)  #\n",
    "\n",
    "        all_tokens = tf.concat([cls_token, max_tokens, mean_tokens], axis=-1)  # [batch, hidden_size*2]\n",
    "    \n",
    "        x = tf.keras.layers.Dense(rnn_size, activation=\"relu\", \n",
    "                                  kernel_regularizer=regularizers.L2(1e-4),\n",
    "                                  bias_regularizer=regularizers.L2(1e-4), \n",
    "                                #   activity_regularizer=regularizers.L2(1e-4),\n",
    "                                  )(all_tokens)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        logits = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "        model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=logits)\n",
    "\n",
    "        ## Callbacks\n",
    "        model_savepath = f\"./Models/MY_{'_'.join(bert_model_name.split('/'))}_dense{rnn_size}.h5\"\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=True, mode='max')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=20)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "\n",
    "\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "        # Summary\n",
    "        model.summary()\n",
    "        # History\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(dataset_train, epochs=epochs, batch_size=64, validation_data=dataset_val, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "                # Pr√©dictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(dataset_val)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des m√©triques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du mod√®le dans MLflow\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45e0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88b67ee9",
   "metadata": {},
   "source": [
    "# Experiment MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b09a8d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MLflow experiment...\n"
     ]
    }
   ],
   "source": [
    "# Cr√©ation de l'√©tude Optuna et optimisation\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"BERT_models_experiment\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"BERT_models_experiment\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Comparaison de plusieurs mod√®les BERT\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"BERT_pretrained\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68246c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_list = [\n",
    "    \"prajjwal1/bert-tiny\",\n",
    "    \"prajjwal1/bert-small\",\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"roberta-base\",\n",
    "    \"distilroberta-base\",\n",
    "    \"vinai/bertweet-base\",  \n",
    "    \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c722ca",
   "metadata": {},
   "source": [
    "# Lancement experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbb5c1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with BERT model : prajjwal1/bert-tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.embeddings.position_ids', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_4 (TFBertModel)  TFBaseModelOutputWi  4385920     ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, None                                               \n",
      "                                , 128),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 128),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5 (Sl  (None, 128)         0           ['tf_bert_model_4[0][0]']        \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_2 (TFOpLamb  (None, 128)         0           ['tf_bert_model_4[0][0]']        \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_2 (TFOpLam  (None, 128)         0           ['tf_bert_model_4[0][0]']        \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.concat_2 (TFOpLambda)       (None, 384)          0           ['tf.__operators__.getitem_5[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.math.reduce_max_2[0][0]',   \n",
      "                                                                  'tf.math.reduce_mean_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 128)          49280       ['tf.concat_2[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_71 (Dropout)           (None, 128)          0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 1)            129         ['dropout_71[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,435,329\n",
      "Trainable params: 49,409\n",
      "Non-trainable params: 4,385,920\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "400/400 [==============================] - 7s 12ms/step - loss: 0.6871 - accuracy: 0.6135 - val_loss: 0.6256 - val_accuracy: 0.6700 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.6454 - accuracy: 0.6467 - val_loss: 0.6173 - val_accuracy: 0.6744 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.6395 - accuracy: 0.6498 - val_loss: 0.6096 - val_accuracy: 0.6806 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.6345 - accuracy: 0.6533 - val_loss: 0.6058 - val_accuracy: 0.6834 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 0.6319 - accuracy: 0.6580 - val_loss: 0.6050 - val_accuracy: 0.6828 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 0.6264 - accuracy: 0.6599 - val_loss: 0.6050 - val_accuracy: 0.6806 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.6261 - accuracy: 0.6609 - val_loss: 0.6089 - val_accuracy: 0.6806 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.6248 - accuracy: 0.6624 - val_loss: 0.6016 - val_accuracy: 0.6797 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.6222 - accuracy: 0.6622 - val_loss: 0.6040 - val_accuracy: 0.6831 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 0.6201 - accuracy: 0.6667 - val_loss: 0.5962 - val_accuracy: 0.6822 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.6225 - accuracy: 0.6658 - val_loss: 0.6031 - val_accuracy: 0.6853 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.6212 - accuracy: 0.6621 - val_loss: 0.6002 - val_accuracy: 0.6775 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.6194 - accuracy: 0.6581 - val_loss: 0.5946 - val_accuracy: 0.6812 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 0.6199 - accuracy: 0.6648 - val_loss: 0.5963 - val_accuracy: 0.6809 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.6201 - accuracy: 0.6605 - val_loss: 0.6020 - val_accuracy: 0.6812 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 0.6172 - accuracy: 0.6678 - val_loss: 0.5965 - val_accuracy: 0.6841 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.6197 - accuracy: 0.6664 - val_loss: 0.5960 - val_accuracy: 0.6800 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 5s 13ms/step - loss: 0.6206 - accuracy: 0.6656 - val_loss: 0.5950 - val_accuracy: 0.6872 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 0.6166 - accuracy: 0.6678 - val_loss: 0.6012 - val_accuracy: 0.6822 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.6182 - accuracy: 0.6659 - val_loss: 0.5937 - val_accuracy: 0.6919 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.6180 - accuracy: 0.6658 - val_loss: 0.5991 - val_accuracy: 0.6837 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 0.6149 - accuracy: 0.6679 - val_loss: 0.5930 - val_accuracy: 0.6831 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 0.6143 - accuracy: 0.6731 - val_loss: 0.5962 - val_accuracy: 0.6841 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 0.6151 - accuracy: 0.6659 - val_loss: 0.5960 - val_accuracy: 0.6809 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.6139 - accuracy: 0.6698 - val_loss: 0.5985 - val_accuracy: 0.6787 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.6158 - accuracy: 0.6680 - val_loss: 0.5893 - val_accuracy: 0.6897 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 0.6132 - accuracy: 0.6705 - val_loss: 0.5960 - val_accuracy: 0.6819 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.6139 - accuracy: 0.6755 - val_loss: 0.5973 - val_accuracy: 0.6856 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.6139 - accuracy: 0.6673 - val_loss: 0.5920 - val_accuracy: 0.6922 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.6126 - accuracy: 0.6739 - val_loss: 0.6055 - val_accuracy: 0.6731 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.6133 - accuracy: 0.6710 - val_loss: 0.5950 - val_accuracy: 0.6841 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.6131 - accuracy: 0.6662 - val_loss: 0.5907 - val_accuracy: 0.6844 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 5s 13ms/step - loss: 0.6154 - accuracy: 0.6696 - val_loss: 0.5892 - val_accuracy: 0.6934 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 5s 13ms/step - loss: 0.6123 - accuracy: 0.6680 - val_loss: 0.5914 - val_accuracy: 0.6941 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.6095 - accuracy: 0.6745 - val_loss: 0.5952 - val_accuracy: 0.6900 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.6127 - accuracy: 0.6706 - val_loss: 0.5874 - val_accuracy: 0.6906 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.6140 - accuracy: 0.6734 - val_loss: 0.5962 - val_accuracy: 0.6812 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.6117 - accuracy: 0.6702 - val_loss: 0.5882 - val_accuracy: 0.6972 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 7s 17ms/step - loss: 0.6078 - accuracy: 0.6747 - val_loss: 0.5893 - val_accuracy: 0.6922 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 7s 17ms/step - loss: 0.6125 - accuracy: 0.6751 - val_loss: 0.5970 - val_accuracy: 0.6816 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 7s 16ms/step - loss: 0.6123 - accuracy: 0.6738 - val_loss: 0.5861 - val_accuracy: 0.6931 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 6s 16ms/step - loss: 0.6090 - accuracy: 0.6770 - val_loss: 0.5893 - val_accuracy: 0.6894 - lr: 0.0010\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 7s 16ms/step - loss: 0.6115 - accuracy: 0.6748 - val_loss: 0.5929 - val_accuracy: 0.6859 - lr: 0.0010\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 7s 17ms/step - loss: 0.6112 - accuracy: 0.6722 - val_loss: 0.5936 - val_accuracy: 0.6881 - lr: 0.0010\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 7s 16ms/step - loss: 0.6116 - accuracy: 0.6734 - val_loss: 0.5918 - val_accuracy: 0.6878 - lr: 0.0010\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 7s 18ms/step - loss: 0.6117 - accuracy: 0.6716 - val_loss: 0.5878 - val_accuracy: 0.6988 - lr: 0.0010\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - 6s 16ms/step - loss: 0.6145 - accuracy: 0.6730 - val_loss: 0.5962 - val_accuracy: 0.6862 - lr: 0.0010\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - 6s 16ms/step - loss: 0.6088 - accuracy: 0.6735 - val_loss: 0.5926 - val_accuracy: 0.6891 - lr: 0.0010\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - 6s 16ms/step - loss: 0.6113 - accuracy: 0.6724 - val_loss: 0.6004 - val_accuracy: 0.6828 - lr: 0.0010\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - 5s 13ms/step - loss: 0.6077 - accuracy: 0.6760 - val_loss: 0.5915 - val_accuracy: 0.6913 - lr: 0.0010\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - 5s 13ms/step - loss: 0.6113 - accuracy: 0.6762 - val_loss: 0.5931 - val_accuracy: 0.6844 - lr: 0.0010\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - 5s 13ms/step - loss: 0.6082 - accuracy: 0.6789 - val_loss: 0.5860 - val_accuracy: 0.6959 - lr: 5.0000e-04\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - 5s 13ms/step - loss: 0.6051 - accuracy: 0.6827 - val_loss: 0.5855 - val_accuracy: 0.6922 - lr: 5.0000e-04\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - 7s 16ms/step - loss: 0.6039 - accuracy: 0.6820 - val_loss: 0.5854 - val_accuracy: 0.6969 - lr: 5.0000e-04\n",
      "Epoch 55/100\n",
      "400/400 [==============================] - 5s 13ms/step - loss: 0.6056 - accuracy: 0.6812 - val_loss: 0.5861 - val_accuracy: 0.6903 - lr: 5.0000e-04\n",
      "Epoch 56/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.5994 - accuracy: 0.6866 - val_loss: 0.5852 - val_accuracy: 0.6938 - lr: 5.0000e-04\n",
      "Epoch 57/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.6068 - accuracy: 0.6785 - val_loss: 0.5882 - val_accuracy: 0.6972 - lr: 5.0000e-04\n",
      "Epoch 58/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.6023 - accuracy: 0.6808 - val_loss: 0.5841 - val_accuracy: 0.6922 - lr: 5.0000e-04\n",
      "Epoch 59/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.6042 - accuracy: 0.6810 - val_loss: 0.5865 - val_accuracy: 0.6972 - lr: 5.0000e-04\n",
      "Epoch 60/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.6020 - accuracy: 0.6827 - val_loss: 0.5860 - val_accuracy: 0.6947 - lr: 5.0000e-04\n",
      "Epoch 61/100\n",
      "400/400 [==============================] - 5s 13ms/step - loss: 0.6042 - accuracy: 0.6824 - val_loss: 0.5829 - val_accuracy: 0.7000 - lr: 5.0000e-04\n",
      "Epoch 62/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 0.6034 - accuracy: 0.6819 - val_loss: 0.5895 - val_accuracy: 0.6894 - lr: 5.0000e-04\n",
      "Epoch 63/100\n",
      "400/400 [==============================] - 5s 13ms/step - loss: 0.6025 - accuracy: 0.6806 - val_loss: 0.5811 - val_accuracy: 0.7069 - lr: 5.0000e-04\n",
      "Epoch 64/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.6015 - accuracy: 0.6787 - val_loss: 0.5877 - val_accuracy: 0.6906 - lr: 5.0000e-04\n",
      "Epoch 65/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5990 - accuracy: 0.6855 - val_loss: 0.5810 - val_accuracy: 0.6975 - lr: 5.0000e-04\n",
      "Epoch 66/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5993 - accuracy: 0.6834 - val_loss: 0.5856 - val_accuracy: 0.7006 - lr: 5.0000e-04\n",
      "Epoch 67/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5992 - accuracy: 0.6860 - val_loss: 0.5822 - val_accuracy: 0.7044 - lr: 5.0000e-04\n",
      "Epoch 68/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5990 - accuracy: 0.6851 - val_loss: 0.5786 - val_accuracy: 0.7006 - lr: 5.0000e-04\n",
      "Epoch 69/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5961 - accuracy: 0.6853 - val_loss: 0.5844 - val_accuracy: 0.6994 - lr: 5.0000e-04\n",
      "Epoch 70/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.6014 - accuracy: 0.6787 - val_loss: 0.5847 - val_accuracy: 0.6963 - lr: 5.0000e-04\n",
      "Epoch 71/100\n",
      "400/400 [==============================] - 5s 11ms/step - loss: 0.5953 - accuracy: 0.6856 - val_loss: 0.5801 - val_accuracy: 0.7053 - lr: 5.0000e-04\n",
      "Epoch 72/100\n",
      "400/400 [==============================] - 5s 11ms/step - loss: 0.6037 - accuracy: 0.6789 - val_loss: 0.5830 - val_accuracy: 0.6991 - lr: 5.0000e-04\n",
      "Epoch 73/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5986 - accuracy: 0.6841 - val_loss: 0.5850 - val_accuracy: 0.6966 - lr: 5.0000e-04\n",
      "Epoch 74/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.6003 - accuracy: 0.6841 - val_loss: 0.5843 - val_accuracy: 0.6994 - lr: 5.0000e-04\n",
      "Epoch 75/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5991 - accuracy: 0.6870 - val_loss: 0.5835 - val_accuracy: 0.6997 - lr: 5.0000e-04\n",
      "Epoch 76/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.6005 - accuracy: 0.6837 - val_loss: 0.5822 - val_accuracy: 0.6978 - lr: 5.0000e-04\n",
      "Epoch 77/100\n",
      "400/400 [==============================] - 5s 11ms/step - loss: 0.5992 - accuracy: 0.6893 - val_loss: 0.5846 - val_accuracy: 0.6969 - lr: 5.0000e-04\n",
      "Epoch 78/100\n",
      "400/400 [==============================] - 5s 11ms/step - loss: 0.6002 - accuracy: 0.6849 - val_loss: 0.5846 - val_accuracy: 0.6978 - lr: 5.0000e-04\n",
      "Epoch 79/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5973 - accuracy: 0.6862 - val_loss: 0.5804 - val_accuracy: 0.6997 - lr: 2.5000e-04\n",
      "Epoch 80/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5942 - accuracy: 0.6930 - val_loss: 0.5799 - val_accuracy: 0.6997 - lr: 2.5000e-04\n",
      "Epoch 81/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5950 - accuracy: 0.6912 - val_loss: 0.5785 - val_accuracy: 0.7034 - lr: 2.5000e-04\n",
      "Epoch 82/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5966 - accuracy: 0.6848 - val_loss: 0.5796 - val_accuracy: 0.6972 - lr: 2.5000e-04\n",
      "Epoch 83/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5979 - accuracy: 0.6889 - val_loss: 0.5808 - val_accuracy: 0.7016 - lr: 2.5000e-04\n",
      "Epoch 84/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5954 - accuracy: 0.6837 - val_loss: 0.5792 - val_accuracy: 0.7000 - lr: 2.5000e-04\n",
      "Epoch 85/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5943 - accuracy: 0.6888 - val_loss: 0.5760 - val_accuracy: 0.7013 - lr: 2.5000e-04\n",
      "Epoch 86/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5934 - accuracy: 0.6889 - val_loss: 0.5774 - val_accuracy: 0.7013 - lr: 2.5000e-04\n",
      "Epoch 87/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5947 - accuracy: 0.6869 - val_loss: 0.5782 - val_accuracy: 0.6988 - lr: 2.5000e-04\n",
      "Epoch 88/100\n",
      "400/400 [==============================] - 5s 11ms/step - loss: 0.5917 - accuracy: 0.6925 - val_loss: 0.5779 - val_accuracy: 0.7006 - lr: 2.5000e-04\n",
      "Epoch 89/100\n",
      "400/400 [==============================] - 5s 11ms/step - loss: 0.5931 - accuracy: 0.6910 - val_loss: 0.5803 - val_accuracy: 0.6950 - lr: 2.5000e-04\n",
      "Epoch 90/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5942 - accuracy: 0.6906 - val_loss: 0.5776 - val_accuracy: 0.6975 - lr: 2.5000e-04\n",
      "Epoch 91/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5910 - accuracy: 0.6866 - val_loss: 0.5813 - val_accuracy: 0.6947 - lr: 2.5000e-04\n",
      "Epoch 92/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5952 - accuracy: 0.6873 - val_loss: 0.5763 - val_accuracy: 0.7006 - lr: 2.5000e-04\n",
      "Epoch 93/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.5938 - accuracy: 0.6927 - val_loss: 0.5777 - val_accuracy: 0.6975 - lr: 2.5000e-04\n",
      "Epoch 94/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.5938 - accuracy: 0.6912 - val_loss: 0.5793 - val_accuracy: 0.6991 - lr: 2.5000e-04\n",
      "Epoch 95/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.5941 - accuracy: 0.6913 - val_loss: 0.5773 - val_accuracy: 0.7025 - lr: 2.5000e-04\n",
      "Epoch 96/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 0.5890 - accuracy: 0.6914 - val_loss: 0.5760 - val_accuracy: 0.7000 - lr: 1.2500e-04\n",
      "Epoch 97/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.5914 - accuracy: 0.6951 - val_loss: 0.5773 - val_accuracy: 0.7025 - lr: 1.2500e-04\n",
      "Epoch 98/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.5922 - accuracy: 0.6891 - val_loss: 0.5757 - val_accuracy: 0.7003 - lr: 1.2500e-04\n",
      "Epoch 99/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 0.5925 - accuracy: 0.6913 - val_loss: 0.5748 - val_accuracy: 0.7044 - lr: 1.2500e-04\n",
      "Epoch 100/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 0.5896 - accuracy: 0.6928 - val_loss: 0.5727 - val_accuracy: 0.7059 - lr: 1.2500e-04\n",
      "100/100 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/23 17:21:54 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/23 17:21:54 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 81). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmphf5yol8r\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmphf5yol8r\\model\\data\\model\\assets\n",
      "2025/09/23 17:22:11 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmphf5yol8r\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/23 17:22:11 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with BERT model : prajjwal1/bert-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'bert.embeddings.position_ids', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_5 (TFBertModel)  TFBaseModelOutputWi  28763648    ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, None                                               \n",
      "                                , 512),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 512),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6 (Sl  (None, 512)         0           ['tf_bert_model_5[0][0]']        \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_3 (TFOpLamb  (None, 512)         0           ['tf_bert_model_5[0][0]']        \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_3 (TFOpLam  (None, 512)         0           ['tf_bert_model_5[0][0]']        \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.concat_3 (TFOpLambda)       (None, 1536)         0           ['tf.__operators__.getitem_6[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.math.reduce_max_3[0][0]',   \n",
      "                                                                  'tf.math.reduce_mean_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 128)          196736      ['tf.concat_3[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_85 (Dropout)           (None, 128)          0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 1)            129         ['dropout_85[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28,960,513\n",
      "Trainable params: 196,865\n",
      "Non-trainable params: 28,763,648\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "400/400 [==============================] - 13s 24ms/step - loss: 0.6418 - accuracy: 0.6584 - val_loss: 0.5711 - val_accuracy: 0.7194 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 11s 27ms/step - loss: 0.5946 - accuracy: 0.7012 - val_loss: 0.5534 - val_accuracy: 0.7284 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 0.5834 - accuracy: 0.7044 - val_loss: 0.5508 - val_accuracy: 0.7309 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 0.5765 - accuracy: 0.7078 - val_loss: 0.5436 - val_accuracy: 0.7312 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 0.5763 - accuracy: 0.7098 - val_loss: 0.5339 - val_accuracy: 0.7347 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 0.5732 - accuracy: 0.7123 - val_loss: 0.5437 - val_accuracy: 0.7356 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 9s 22ms/step - loss: 0.5675 - accuracy: 0.7135 - val_loss: 0.5324 - val_accuracy: 0.7322 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 11s 28ms/step - loss: 0.5667 - accuracy: 0.7151 - val_loss: 0.5277 - val_accuracy: 0.7441 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 9s 23ms/step - loss: 0.5681 - accuracy: 0.7152 - val_loss: 0.5289 - val_accuracy: 0.7369 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 9s 22ms/step - loss: 0.5659 - accuracy: 0.7174 - val_loss: 0.5264 - val_accuracy: 0.7369 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 9s 22ms/step - loss: 0.5597 - accuracy: 0.7239 - val_loss: 0.5327 - val_accuracy: 0.7381 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 9s 22ms/step - loss: 0.5638 - accuracy: 0.7182 - val_loss: 0.5323 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 11s 28ms/step - loss: 0.5592 - accuracy: 0.7216 - val_loss: 0.5199 - val_accuracy: 0.7459 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 9s 23ms/step - loss: 0.5585 - accuracy: 0.7178 - val_loss: 0.5316 - val_accuracy: 0.7416 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 9s 22ms/step - loss: 0.5539 - accuracy: 0.7257 - val_loss: 0.5287 - val_accuracy: 0.7375 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 10s 25ms/step - loss: 0.5550 - accuracy: 0.7241 - val_loss: 0.5253 - val_accuracy: 0.7422 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5560 - accuracy: 0.7234 - val_loss: 0.5237 - val_accuracy: 0.7441 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 0.5575 - accuracy: 0.7245 - val_loss: 0.5245 - val_accuracy: 0.7462 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 10s 25ms/step - loss: 0.5603 - accuracy: 0.7248 - val_loss: 0.5228 - val_accuracy: 0.7403 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 9s 23ms/step - loss: 0.5568 - accuracy: 0.7226 - val_loss: 0.5246 - val_accuracy: 0.7459 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 9s 23ms/step - loss: 0.5544 - accuracy: 0.7247 - val_loss: 0.5256 - val_accuracy: 0.7456 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 9s 23ms/step - loss: 0.5579 - accuracy: 0.7193 - val_loss: 0.5245 - val_accuracy: 0.7447 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 9s 24ms/step - loss: 0.5515 - accuracy: 0.7298 - val_loss: 0.5228 - val_accuracy: 0.7462 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 0.5475 - accuracy: 0.7320 - val_loss: 0.5221 - val_accuracy: 0.7522 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5442 - accuracy: 0.7355 - val_loss: 0.5264 - val_accuracy: 0.7494 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5437 - accuracy: 0.7380 - val_loss: 0.5165 - val_accuracy: 0.7491 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5409 - accuracy: 0.7380 - val_loss: 0.5197 - val_accuracy: 0.7478 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 10s 25ms/step - loss: 0.5410 - accuracy: 0.7359 - val_loss: 0.5200 - val_accuracy: 0.7466 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5428 - accuracy: 0.7330 - val_loss: 0.5199 - val_accuracy: 0.7500 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 0.5371 - accuracy: 0.7361 - val_loss: 0.5178 - val_accuracy: 0.7538 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 10s 25ms/step - loss: 0.5419 - accuracy: 0.7359 - val_loss: 0.5157 - val_accuracy: 0.7525 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 9s 23ms/step - loss: 0.5382 - accuracy: 0.7353 - val_loss: 0.5237 - val_accuracy: 0.7497 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5393 - accuracy: 0.7392 - val_loss: 0.5143 - val_accuracy: 0.7503 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5364 - accuracy: 0.7353 - val_loss: 0.5191 - val_accuracy: 0.7481 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5385 - accuracy: 0.7377 - val_loss: 0.5149 - val_accuracy: 0.7522 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5342 - accuracy: 0.7377 - val_loss: 0.5168 - val_accuracy: 0.7516 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5356 - accuracy: 0.7399 - val_loss: 0.5156 - val_accuracy: 0.7516 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 9s 24ms/step - loss: 0.5326 - accuracy: 0.7409 - val_loss: 0.5146 - val_accuracy: 0.7509 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 10s 25ms/step - loss: 0.5342 - accuracy: 0.7393 - val_loss: 0.5185 - val_accuracy: 0.7509 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 10s 25ms/step - loss: 0.5378 - accuracy: 0.7349 - val_loss: 0.5142 - val_accuracy: 0.7525 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 0.5353 - accuracy: 0.7391 - val_loss: 0.5181 - val_accuracy: 0.7578 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 10s 25ms/step - loss: 0.5383 - accuracy: 0.7370 - val_loss: 0.5186 - val_accuracy: 0.7525 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5300 - accuracy: 0.7397 - val_loss: 0.5186 - val_accuracy: 0.7487 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 9s 23ms/step - loss: 0.5271 - accuracy: 0.7456 - val_loss: 0.5112 - val_accuracy: 0.7531 - lr: 2.5000e-04\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 9s 23ms/step - loss: 0.5269 - accuracy: 0.7412 - val_loss: 0.5092 - val_accuracy: 0.7519 - lr: 2.5000e-04\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5250 - accuracy: 0.7440 - val_loss: 0.5108 - val_accuracy: 0.7538 - lr: 2.5000e-04\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - 9s 23ms/step - loss: 0.5237 - accuracy: 0.7479 - val_loss: 0.5112 - val_accuracy: 0.7550 - lr: 2.5000e-04\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - 9s 23ms/step - loss: 0.5308 - accuracy: 0.7416 - val_loss: 0.5139 - val_accuracy: 0.7544 - lr: 2.5000e-04\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5250 - accuracy: 0.7475 - val_loss: 0.5082 - val_accuracy: 0.7541 - lr: 2.5000e-04\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - 10s 25ms/step - loss: 0.5240 - accuracy: 0.7440 - val_loss: 0.5114 - val_accuracy: 0.7503 - lr: 2.5000e-04\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5168 - accuracy: 0.7474 - val_loss: 0.5107 - val_accuracy: 0.7519 - lr: 2.5000e-04\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - 9s 24ms/step - loss: 0.5198 - accuracy: 0.7484 - val_loss: 0.5082 - val_accuracy: 0.7553 - lr: 2.5000e-04\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - 9s 24ms/step - loss: 0.5241 - accuracy: 0.7469 - val_loss: 0.5117 - val_accuracy: 0.7509 - lr: 2.5000e-04\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5191 - accuracy: 0.7496 - val_loss: 0.5118 - val_accuracy: 0.7553 - lr: 2.5000e-04\n",
      "Epoch 55/100\n",
      "400/400 [==============================] - 9s 24ms/step - loss: 0.5253 - accuracy: 0.7458 - val_loss: 0.5121 - val_accuracy: 0.7528 - lr: 2.5000e-04\n",
      "Epoch 56/100\n",
      "400/400 [==============================] - 10s 25ms/step - loss: 0.5224 - accuracy: 0.7461 - val_loss: 0.5104 - val_accuracy: 0.7513 - lr: 2.5000e-04\n",
      "Epoch 57/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5201 - accuracy: 0.7470 - val_loss: 0.5085 - val_accuracy: 0.7553 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5222 - accuracy: 0.7459 - val_loss: 0.5099 - val_accuracy: 0.7538 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5192 - accuracy: 0.7486 - val_loss: 0.5115 - val_accuracy: 0.7503 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5203 - accuracy: 0.7483 - val_loss: 0.5093 - val_accuracy: 0.7519 - lr: 1.2500e-04\n",
      "Epoch 61/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5108 - accuracy: 0.7503 - val_loss: 0.5089 - val_accuracy: 0.7525 - lr: 1.2500e-04\n",
      "Epoch 62/100\n",
      "400/400 [==============================] - 9s 24ms/step - loss: 0.5104 - accuracy: 0.7534 - val_loss: 0.5093 - val_accuracy: 0.7528 - lr: 1.2500e-04\n",
      "Epoch 63/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5165 - accuracy: 0.7490 - val_loss: 0.5096 - val_accuracy: 0.7534 - lr: 1.2500e-04\n",
      "Epoch 64/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5160 - accuracy: 0.7527 - val_loss: 0.5086 - val_accuracy: 0.7494 - lr: 1.2500e-04\n",
      "Epoch 65/100\n",
      "400/400 [==============================] - 10s 25ms/step - loss: 0.5195 - accuracy: 0.7499 - val_loss: 0.5097 - val_accuracy: 0.7503 - lr: 1.2500e-04\n",
      "Epoch 66/100\n",
      "400/400 [==============================] - 9s 24ms/step - loss: 0.5157 - accuracy: 0.7530 - val_loss: 0.5089 - val_accuracy: 0.7544 - lr: 1.2500e-04\n",
      "Epoch 67/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5172 - accuracy: 0.7498 - val_loss: 0.5090 - val_accuracy: 0.7509 - lr: 1.2500e-04\n",
      "Epoch 68/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5126 - accuracy: 0.7557 - val_loss: 0.5084 - val_accuracy: 0.7525 - lr: 1.2500e-04\n",
      "Epoch 69/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5096 - accuracy: 0.7553 - val_loss: 0.5082 - val_accuracy: 0.7519 - lr: 1.2500e-04\n",
      "Epoch 70/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5125 - accuracy: 0.7539 - val_loss: 0.5085 - val_accuracy: 0.7553 - lr: 6.2500e-05\n",
      "Epoch 71/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5092 - accuracy: 0.7566 - val_loss: 0.5090 - val_accuracy: 0.7513 - lr: 6.2500e-05\n",
      "Epoch 72/100\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5078 - accuracy: 0.7591 - val_loss: 0.5083 - val_accuracy: 0.7509 - lr: 6.2500e-05\n",
      "100/100 [==============================] - 3s 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/23 17:34:17 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/23 17:34:17 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 149). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp1ppckiv_\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp1ppckiv_\\model\\data\\model\\assets\n",
      "2025/09/23 17:34:52 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp1ppckiv_\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/23 17:34:52 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with BERT model : distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_model_1 (TFDist  TFBaseModelOutput(l  66362880   ['input_ids[0][0]',              \n",
      " ilBertModel)                   ast_hidden_state=(N               'attention_mask[0][0]']         \n",
      "                                one, None, 768),                                                  \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_7 (Sl  (None, 768)         0           ['tf_distil_bert_model_1[0][0]'] \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_4 (TFOpLamb  (None, 768)         0           ['tf_distil_bert_model_1[0][0]'] \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_4 (TFOpLam  (None, 768)         0           ['tf_distil_bert_model_1[0][0]'] \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.concat_4 (TFOpLambda)       (None, 2304)         0           ['tf.__operators__.getitem_7[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.math.reduce_max_4[0][0]',   \n",
      "                                                                  'tf.math.reduce_mean_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 128)          295040      ['tf.concat_4[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_105 (Dropout)          (None, 128)          0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 1)            129         ['dropout_105[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,658,049\n",
      "Trainable params: 295,169\n",
      "Non-trainable params: 66,362,880\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "400/400 [==============================] - 31s 63ms/step - loss: 0.6102 - accuracy: 0.6889 - val_loss: 0.5277 - val_accuracy: 0.7525 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 24s 59ms/step - loss: 0.5556 - accuracy: 0.7300 - val_loss: 0.5215 - val_accuracy: 0.7622 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 19s 46ms/step - loss: 0.5477 - accuracy: 0.7359 - val_loss: 0.5203 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 18s 44ms/step - loss: 0.5447 - accuracy: 0.7361 - val_loss: 0.5076 - val_accuracy: 0.7606 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 24s 59ms/step - loss: 0.5376 - accuracy: 0.7411 - val_loss: 0.5076 - val_accuracy: 0.7675 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 24s 60ms/step - loss: 0.5320 - accuracy: 0.7402 - val_loss: 0.4985 - val_accuracy: 0.7756 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 24s 59ms/step - loss: 0.5402 - accuracy: 0.7408 - val_loss: 0.4957 - val_accuracy: 0.7797 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.5368 - accuracy: 0.7433 - val_loss: 0.4980 - val_accuracy: 0.7666 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 18s 45ms/step - loss: 0.5288 - accuracy: 0.7502 - val_loss: 0.5060 - val_accuracy: 0.7763 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 18s 45ms/step - loss: 0.5305 - accuracy: 0.7454 - val_loss: 0.4878 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 18s 44ms/step - loss: 0.5326 - accuracy: 0.7441 - val_loss: 0.4826 - val_accuracy: 0.7763 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 18s 45ms/step - loss: 0.5179 - accuracy: 0.7506 - val_loss: 0.4804 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 18s 45ms/step - loss: 0.5205 - accuracy: 0.7503 - val_loss: 0.4903 - val_accuracy: 0.7759 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 18s 45ms/step - loss: 0.5213 - accuracy: 0.7513 - val_loss: 0.4838 - val_accuracy: 0.7788 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 18s 44ms/step - loss: 0.5227 - accuracy: 0.7516 - val_loss: 0.4944 - val_accuracy: 0.7725 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 18s 44ms/step - loss: 0.5168 - accuracy: 0.7505 - val_loss: 0.4885 - val_accuracy: 0.7778 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 18s 45ms/step - loss: 0.5183 - accuracy: 0.7484 - val_loss: 0.4834 - val_accuracy: 0.7791 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 18s 44ms/step - loss: 0.5202 - accuracy: 0.7519 - val_loss: 0.4859 - val_accuracy: 0.7753 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 17s 43ms/step - loss: 0.5179 - accuracy: 0.7506 - val_loss: 0.4875 - val_accuracy: 0.7722 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.5185 - accuracy: 0.7543 - val_loss: 0.4803 - val_accuracy: 0.7753 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 18s 44ms/step - loss: 0.5191 - accuracy: 0.7519 - val_loss: 0.4922 - val_accuracy: 0.7797 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 18s 44ms/step - loss: 0.5173 - accuracy: 0.7492 - val_loss: 0.4858 - val_accuracy: 0.7741 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 18s 44ms/step - loss: 0.5175 - accuracy: 0.7527 - val_loss: 0.4966 - val_accuracy: 0.7719 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 17s 41ms/step - loss: 0.5088 - accuracy: 0.7541 - val_loss: 0.4908 - val_accuracy: 0.7747 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 17s 43ms/step - loss: 0.5158 - accuracy: 0.7555 - val_loss: 0.4838 - val_accuracy: 0.7744 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.5173 - accuracy: 0.7514 - val_loss: 0.4893 - val_accuracy: 0.7763 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.5165 - accuracy: 0.7584 - val_loss: 0.4816 - val_accuracy: 0.7763 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.5084 - accuracy: 0.7591 - val_loss: 0.4875 - val_accuracy: 0.7759 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.5110 - accuracy: 0.7553 - val_loss: 0.4895 - val_accuracy: 0.7784 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.5092 - accuracy: 0.7610 - val_loss: 0.4839 - val_accuracy: 0.7784 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.4970 - accuracy: 0.7630 - val_loss: 0.4859 - val_accuracy: 0.7781 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.5026 - accuracy: 0.7646 - val_loss: 0.4814 - val_accuracy: 0.7794 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 18s 45ms/step - loss: 0.4942 - accuracy: 0.7685 - val_loss: 0.4789 - val_accuracy: 0.7781 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 24s 60ms/step - loss: 0.4922 - accuracy: 0.7678 - val_loss: 0.4809 - val_accuracy: 0.7800 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 25s 61ms/step - loss: 0.4896 - accuracy: 0.7670 - val_loss: 0.4779 - val_accuracy: 0.7803 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 18s 45ms/step - loss: 0.4892 - accuracy: 0.7682 - val_loss: 0.4793 - val_accuracy: 0.7725 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.4863 - accuracy: 0.7751 - val_loss: 0.4771 - val_accuracy: 0.7797 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 25s 61ms/step - loss: 0.4895 - accuracy: 0.7713 - val_loss: 0.4753 - val_accuracy: 0.7837 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.4886 - accuracy: 0.7648 - val_loss: 0.4819 - val_accuracy: 0.7725 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 17s 42ms/step - loss: 0.4944 - accuracy: 0.7684 - val_loss: 0.4794 - val_accuracy: 0.7806 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.4865 - accuracy: 0.7676 - val_loss: 0.4887 - val_accuracy: 0.7769 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4900 - accuracy: 0.7687 - val_loss: 0.4763 - val_accuracy: 0.7781 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4870 - accuracy: 0.7693 - val_loss: 0.4824 - val_accuracy: 0.7781 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4894 - accuracy: 0.7682 - val_loss: 0.4813 - val_accuracy: 0.7772 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4895 - accuracy: 0.7680 - val_loss: 0.4794 - val_accuracy: 0.7806 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4875 - accuracy: 0.7702 - val_loss: 0.4770 - val_accuracy: 0.7828 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4867 - accuracy: 0.7723 - val_loss: 0.4775 - val_accuracy: 0.7809 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4858 - accuracy: 0.7694 - val_loss: 0.4807 - val_accuracy: 0.7819 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4798 - accuracy: 0.7759 - val_loss: 0.4789 - val_accuracy: 0.7713 - lr: 2.5000e-04\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - 16s 40ms/step - loss: 0.4723 - accuracy: 0.7771 - val_loss: 0.4793 - val_accuracy: 0.7781 - lr: 2.5000e-04\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - 16s 40ms/step - loss: 0.4726 - accuracy: 0.7806 - val_loss: 0.4787 - val_accuracy: 0.7800 - lr: 2.5000e-04\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4739 - accuracy: 0.7777 - val_loss: 0.4814 - val_accuracy: 0.7753 - lr: 2.5000e-04\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - 17s 41ms/step - loss: 0.4751 - accuracy: 0.7806 - val_loss: 0.4808 - val_accuracy: 0.7837 - lr: 2.5000e-04\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4760 - accuracy: 0.7809 - val_loss: 0.4748 - val_accuracy: 0.7806 - lr: 2.5000e-04\n",
      "Epoch 55/100\n",
      "400/400 [==============================] - 16s 40ms/step - loss: 0.4699 - accuracy: 0.7798 - val_loss: 0.4791 - val_accuracy: 0.7834 - lr: 2.5000e-04\n",
      "Epoch 56/100\n",
      "400/400 [==============================] - 22s 56ms/step - loss: 0.4730 - accuracy: 0.7795 - val_loss: 0.4758 - val_accuracy: 0.7841 - lr: 2.5000e-04\n",
      "Epoch 57/100\n",
      "400/400 [==============================] - 16s 40ms/step - loss: 0.4709 - accuracy: 0.7814 - val_loss: 0.4828 - val_accuracy: 0.7747 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "400/400 [==============================] - 16s 40ms/step - loss: 0.4753 - accuracy: 0.7760 - val_loss: 0.4824 - val_accuracy: 0.7728 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "400/400 [==============================] - 16s 40ms/step - loss: 0.4752 - accuracy: 0.7779 - val_loss: 0.4821 - val_accuracy: 0.7763 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "400/400 [==============================] - 16s 40ms/step - loss: 0.4717 - accuracy: 0.7780 - val_loss: 0.4821 - val_accuracy: 0.7759 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "400/400 [==============================] - 16s 40ms/step - loss: 0.4707 - accuracy: 0.7821 - val_loss: 0.4807 - val_accuracy: 0.7756 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "400/400 [==============================] - 16s 40ms/step - loss: 0.4716 - accuracy: 0.7763 - val_loss: 0.4857 - val_accuracy: 0.7738 - lr: 2.5000e-04\n",
      "Epoch 63/100\n",
      "400/400 [==============================] - 16s 40ms/step - loss: 0.4715 - accuracy: 0.7813 - val_loss: 0.4784 - val_accuracy: 0.7772 - lr: 2.5000e-04\n",
      "Epoch 64/100\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4695 - accuracy: 0.7804 - val_loss: 0.4801 - val_accuracy: 0.7766 - lr: 2.5000e-04\n",
      "Epoch 65/100\n",
      "400/400 [==============================] - 18s 45ms/step - loss: 0.4635 - accuracy: 0.7851 - val_loss: 0.4779 - val_accuracy: 0.7775 - lr: 1.2500e-04\n",
      "Epoch 66/100\n",
      "400/400 [==============================] - 18s 44ms/step - loss: 0.4617 - accuracy: 0.7848 - val_loss: 0.4769 - val_accuracy: 0.7794 - lr: 1.2500e-04\n",
      "Epoch 67/100\n",
      "400/400 [==============================] - 17s 44ms/step - loss: 0.4644 - accuracy: 0.7843 - val_loss: 0.4773 - val_accuracy: 0.7819 - lr: 1.2500e-04\n",
      "Epoch 68/100\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4612 - accuracy: 0.7834 - val_loss: 0.4776 - val_accuracy: 0.7794 - lr: 1.2500e-04\n",
      "Epoch 69/100\n",
      "400/400 [==============================] - 17s 41ms/step - loss: 0.4683 - accuracy: 0.7837 - val_loss: 0.4783 - val_accuracy: 0.7759 - lr: 1.2500e-04\n",
      "Epoch 70/100\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4620 - accuracy: 0.7895 - val_loss: 0.4779 - val_accuracy: 0.7825 - lr: 1.2500e-04\n",
      "Epoch 71/100\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4635 - accuracy: 0.7870 - val_loss: 0.4789 - val_accuracy: 0.7788 - lr: 1.2500e-04\n",
      "Epoch 72/100\n",
      "400/400 [==============================] - 16s 40ms/step - loss: 0.4622 - accuracy: 0.7864 - val_loss: 0.4786 - val_accuracy: 0.7778 - lr: 1.2500e-04\n",
      "Epoch 73/100\n",
      "400/400 [==============================] - 16s 40ms/step - loss: 0.4613 - accuracy: 0.7877 - val_loss: 0.4759 - val_accuracy: 0.7831 - lr: 1.2500e-04\n",
      "Epoch 74/100\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4609 - accuracy: 0.7879 - val_loss: 0.4765 - val_accuracy: 0.7778 - lr: 1.2500e-04\n",
      "100/100 [==============================] - 6s 30ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/23 17:57:26 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/23 17:57:26 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000026CE423F640>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000026CE423F640>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000026CE42F7C40>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000026CE42F7C40>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000026CE42F4CA0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000026CE42F4CA0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000026CED3788E0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000026CED3788E0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000026CCB6231F0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000026CCB6231F0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000026CD9FF0B50>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000026CD9FF0B50>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, transformer_layer_call_fn, transformer_layer_call_and_return_conditional_losses while saving (showing 5 of 165). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpyg4wlsik\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpyg4wlsik\\model\\data\\model\\assets\n",
      "2025/09/23 17:58:06 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpyg4wlsik\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/23 17:58:06 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with BERT model : roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " el)                            thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, None                                               \n",
      "                                , 768),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_8 (Sl  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_5 (TFOpLamb  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_5 (TFOpLam  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.concat_5 (TFOpLambda)       (None, 2304)         0           ['tf.__operators__.getitem_8[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.math.reduce_max_5[0][0]',   \n",
      "                                                                  'tf.math.reduce_mean_5[0][0]']  \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 128)          295040      ['tf.concat_5[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_143 (Dropout)          (None, 128)          0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 1)            129         ['dropout_143[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124,940,801\n",
      "Trainable params: 295,169\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "400/400 [==============================] - 47s 94ms/step - loss: 0.6619 - accuracy: 0.6259 - val_loss: 0.5922 - val_accuracy: 0.7294 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 39s 99ms/step - loss: 0.6006 - accuracy: 0.6935 - val_loss: 0.5500 - val_accuracy: 0.7538 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 39s 97ms/step - loss: 0.5690 - accuracy: 0.7166 - val_loss: 0.5122 - val_accuracy: 0.7672 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 40s 98ms/step - loss: 0.5522 - accuracy: 0.7338 - val_loss: 0.5309 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.5587 - accuracy: 0.7256 - val_loss: 0.5176 - val_accuracy: 0.7734 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 32s 81ms/step - loss: 0.5504 - accuracy: 0.7300 - val_loss: 0.4946 - val_accuracy: 0.7719 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 40s 99ms/step - loss: 0.5463 - accuracy: 0.7382 - val_loss: 0.5134 - val_accuracy: 0.7794 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 35s 87ms/step - loss: 0.5412 - accuracy: 0.7376 - val_loss: 0.4849 - val_accuracy: 0.7716 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 35s 87ms/step - loss: 0.5432 - accuracy: 0.7404 - val_loss: 0.5017 - val_accuracy: 0.7794 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 41s 102ms/step - loss: 0.5384 - accuracy: 0.7421 - val_loss: 0.4998 - val_accuracy: 0.7809 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 34s 85ms/step - loss: 0.5342 - accuracy: 0.7454 - val_loss: 0.4781 - val_accuracy: 0.7794 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 44s 110ms/step - loss: 0.5384 - accuracy: 0.7442 - val_loss: 0.5290 - val_accuracy: 0.7812 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 41s 101ms/step - loss: 0.5531 - accuracy: 0.7245 - val_loss: 0.4948 - val_accuracy: 0.7869 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 32s 80ms/step - loss: 0.5413 - accuracy: 0.7392 - val_loss: 0.4894 - val_accuracy: 0.7806 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 32s 80ms/step - loss: 0.5442 - accuracy: 0.7345 - val_loss: 0.4912 - val_accuracy: 0.7791 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 34s 85ms/step - loss: 0.5470 - accuracy: 0.7348 - val_loss: 0.5134 - val_accuracy: 0.7800 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.5398 - accuracy: 0.7409 - val_loss: 0.4822 - val_accuracy: 0.7859 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.5398 - accuracy: 0.7417 - val_loss: 0.5049 - val_accuracy: 0.7812 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 32s 81ms/step - loss: 0.5346 - accuracy: 0.7462 - val_loss: 0.4824 - val_accuracy: 0.7806 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 34s 85ms/step - loss: 0.5309 - accuracy: 0.7464 - val_loss: 0.4672 - val_accuracy: 0.7853 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 40s 100ms/step - loss: 0.5363 - accuracy: 0.7396 - val_loss: 0.4782 - val_accuracy: 0.7897 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 34s 84ms/step - loss: 0.5339 - accuracy: 0.7427 - val_loss: 0.5167 - val_accuracy: 0.7844 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 35s 87ms/step - loss: 0.5362 - accuracy: 0.7423 - val_loss: 0.5032 - val_accuracy: 0.7841 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 35s 89ms/step - loss: 0.5384 - accuracy: 0.7410 - val_loss: 0.4681 - val_accuracy: 0.7841 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5389 - accuracy: 0.7451 - val_loss: 0.4774 - val_accuracy: 0.7881 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 36s 89ms/step - loss: 0.5362 - accuracy: 0.7485 - val_loss: 0.4772 - val_accuracy: 0.7837 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5330 - accuracy: 0.7479 - val_loss: 0.4848 - val_accuracy: 0.7853 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 36s 89ms/step - loss: 0.5424 - accuracy: 0.7392 - val_loss: 0.4872 - val_accuracy: 0.7675 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 36s 89ms/step - loss: 0.5423 - accuracy: 0.7364 - val_loss: 0.4899 - val_accuracy: 0.7866 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 36s 89ms/step - loss: 0.5490 - accuracy: 0.7302 - val_loss: 0.4780 - val_accuracy: 0.7847 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 42s 104ms/step - loss: 0.5233 - accuracy: 0.7569 - val_loss: 0.4774 - val_accuracy: 0.7916 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 35s 87ms/step - loss: 0.5233 - accuracy: 0.7577 - val_loss: 0.4719 - val_accuracy: 0.7881 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.5245 - accuracy: 0.7509 - val_loss: 0.4791 - val_accuracy: 0.7844 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 36s 89ms/step - loss: 0.5222 - accuracy: 0.7538 - val_loss: 0.4706 - val_accuracy: 0.7866 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 35s 89ms/step - loss: 0.5200 - accuracy: 0.7579 - val_loss: 0.4739 - val_accuracy: 0.7903 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 36s 89ms/step - loss: 0.5194 - accuracy: 0.7592 - val_loss: 0.4660 - val_accuracy: 0.7909 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 37s 91ms/step - loss: 0.5229 - accuracy: 0.7520 - val_loss: 0.4721 - val_accuracy: 0.7847 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5205 - accuracy: 0.7582 - val_loss: 0.4642 - val_accuracy: 0.7856 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 43s 106ms/step - loss: 0.5190 - accuracy: 0.7576 - val_loss: 0.4688 - val_accuracy: 0.7925 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 36s 90ms/step - loss: 0.5178 - accuracy: 0.7570 - val_loss: 0.4678 - val_accuracy: 0.7897 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5164 - accuracy: 0.7593 - val_loss: 0.4679 - val_accuracy: 0.7891 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5121 - accuracy: 0.7673 - val_loss: 0.4644 - val_accuracy: 0.7925 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5180 - accuracy: 0.7540 - val_loss: 0.4721 - val_accuracy: 0.7881 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 36s 89ms/step - loss: 0.5212 - accuracy: 0.7560 - val_loss: 0.4617 - val_accuracy: 0.7925 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 35s 89ms/step - loss: 0.5177 - accuracy: 0.7586 - val_loss: 0.4737 - val_accuracy: 0.7859 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5171 - accuracy: 0.7537 - val_loss: 0.4783 - val_accuracy: 0.7834 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5183 - accuracy: 0.7569 - val_loss: 0.4737 - val_accuracy: 0.7922 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5128 - accuracy: 0.7606 - val_loss: 0.4640 - val_accuracy: 0.7862 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - 35s 89ms/step - loss: 0.5176 - accuracy: 0.7594 - val_loss: 0.4721 - val_accuracy: 0.7803 - lr: 5.0000e-04\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5176 - accuracy: 0.7557 - val_loss: 0.4697 - val_accuracy: 0.7853 - lr: 5.0000e-04\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - 35s 87ms/step - loss: 0.5180 - accuracy: 0.7582 - val_loss: 0.4702 - val_accuracy: 0.7828 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - 35s 87ms/step - loss: 0.5161 - accuracy: 0.7563 - val_loss: 0.4718 - val_accuracy: 0.7866 - lr: 5.0000e-04\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5186 - accuracy: 0.7579 - val_loss: 0.4659 - val_accuracy: 0.7881 - lr: 5.0000e-04\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5092 - accuracy: 0.7600 - val_loss: 0.4645 - val_accuracy: 0.7853 - lr: 5.0000e-04\n",
      "Epoch 55/100\n",
      "400/400 [==============================] - 36s 90ms/step - loss: 0.5096 - accuracy: 0.7643 - val_loss: 0.4721 - val_accuracy: 0.7759 - lr: 2.5000e-04\n",
      "Epoch 56/100\n",
      "400/400 [==============================] - 35s 87ms/step - loss: 0.5083 - accuracy: 0.7632 - val_loss: 0.4649 - val_accuracy: 0.7844 - lr: 2.5000e-04\n",
      "Epoch 57/100\n",
      "400/400 [==============================] - 36s 89ms/step - loss: 0.5059 - accuracy: 0.7646 - val_loss: 0.4620 - val_accuracy: 0.7897 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5045 - accuracy: 0.7697 - val_loss: 0.4763 - val_accuracy: 0.7850 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5073 - accuracy: 0.7618 - val_loss: 0.4648 - val_accuracy: 0.7853 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "400/400 [==============================] - 36s 91ms/step - loss: 0.5152 - accuracy: 0.7563 - val_loss: 0.4656 - val_accuracy: 0.7881 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5073 - accuracy: 0.7621 - val_loss: 0.4663 - val_accuracy: 0.7856 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "400/400 [==============================] - 36s 89ms/step - loss: 0.5035 - accuracy: 0.7699 - val_loss: 0.4606 - val_accuracy: 0.7872 - lr: 2.5000e-04\n",
      "Epoch 63/100\n",
      "400/400 [==============================] - 36s 89ms/step - loss: 0.5104 - accuracy: 0.7638 - val_loss: 0.4576 - val_accuracy: 0.7881 - lr: 2.5000e-04\n",
      "Epoch 64/100\n",
      "400/400 [==============================] - 36s 89ms/step - loss: 0.5045 - accuracy: 0.7639 - val_loss: 0.4671 - val_accuracy: 0.7837 - lr: 2.5000e-04\n",
      "Epoch 65/100\n",
      "400/400 [==============================] - 36s 91ms/step - loss: 0.5096 - accuracy: 0.7659 - val_loss: 0.4599 - val_accuracy: 0.7884 - lr: 2.5000e-04\n",
      "Epoch 66/100\n",
      "400/400 [==============================] - 36s 89ms/step - loss: 0.5076 - accuracy: 0.7671 - val_loss: 0.4629 - val_accuracy: 0.7844 - lr: 2.5000e-04\n",
      "Epoch 67/100\n",
      "400/400 [==============================] - 34s 85ms/step - loss: 0.5075 - accuracy: 0.7642 - val_loss: 0.4597 - val_accuracy: 0.7884 - lr: 2.5000e-04\n",
      "Epoch 68/100\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.5067 - accuracy: 0.7635 - val_loss: 0.4581 - val_accuracy: 0.7925 - lr: 2.5000e-04\n",
      "Epoch 69/100\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.5045 - accuracy: 0.7664 - val_loss: 0.4577 - val_accuracy: 0.7869 - lr: 2.5000e-04\n",
      "Epoch 70/100\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.5021 - accuracy: 0.7668 - val_loss: 0.4607 - val_accuracy: 0.7844 - lr: 2.5000e-04\n",
      "Epoch 71/100\n",
      "400/400 [==============================] - 32s 81ms/step - loss: 0.5046 - accuracy: 0.7615 - val_loss: 0.4576 - val_accuracy: 0.7881 - lr: 2.5000e-04\n",
      "Epoch 72/100\n",
      "400/400 [==============================] - 32s 80ms/step - loss: 0.5043 - accuracy: 0.7709 - val_loss: 0.4675 - val_accuracy: 0.7862 - lr: 2.5000e-04\n",
      "Epoch 73/100\n",
      "400/400 [==============================] - 32s 81ms/step - loss: 0.5065 - accuracy: 0.7628 - val_loss: 0.4640 - val_accuracy: 0.7841 - lr: 2.5000e-04\n",
      "Epoch 74/100\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.5006 - accuracy: 0.7728 - val_loss: 0.4619 - val_accuracy: 0.7812 - lr: 1.2500e-04\n",
      "Epoch 75/100\n",
      "400/400 [==============================] - 32s 80ms/step - loss: 0.5025 - accuracy: 0.7709 - val_loss: 0.4555 - val_accuracy: 0.7881 - lr: 1.2500e-04\n",
      "Epoch 76/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5044 - accuracy: 0.7659 - val_loss: 0.4555 - val_accuracy: 0.7900 - lr: 1.2500e-04\n",
      "Epoch 77/100\n",
      "400/400 [==============================] - 32s 81ms/step - loss: 0.5018 - accuracy: 0.7691 - val_loss: 0.4579 - val_accuracy: 0.7875 - lr: 1.2500e-04\n",
      "Epoch 78/100\n",
      "400/400 [==============================] - 35s 87ms/step - loss: 0.5013 - accuracy: 0.7676 - val_loss: 0.4581 - val_accuracy: 0.7897 - lr: 1.2500e-04\n",
      "Epoch 79/100\n",
      "400/400 [==============================] - 35s 89ms/step - loss: 0.5068 - accuracy: 0.7634 - val_loss: 0.4589 - val_accuracy: 0.7847 - lr: 1.2500e-04\n",
      "Epoch 80/100\n",
      "400/400 [==============================] - 36s 89ms/step - loss: 0.5057 - accuracy: 0.7719 - val_loss: 0.4577 - val_accuracy: 0.7903 - lr: 1.2500e-04\n",
      "Epoch 81/100\n",
      "400/400 [==============================] - 36s 89ms/step - loss: 0.5024 - accuracy: 0.7718 - val_loss: 0.4596 - val_accuracy: 0.7844 - lr: 1.2500e-04\n",
      "Epoch 82/100\n",
      "400/400 [==============================] - 36s 90ms/step - loss: 0.4989 - accuracy: 0.7657 - val_loss: 0.4617 - val_accuracy: 0.7859 - lr: 1.2500e-04\n",
      "Epoch 83/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5009 - accuracy: 0.7674 - val_loss: 0.4568 - val_accuracy: 0.7891 - lr: 1.2500e-04\n",
      "Epoch 84/100\n",
      "400/400 [==============================] - 36s 90ms/step - loss: 0.4997 - accuracy: 0.7745 - val_loss: 0.4576 - val_accuracy: 0.7909 - lr: 1.2500e-04\n",
      "Epoch 85/100\n",
      "400/400 [==============================] - 35s 89ms/step - loss: 0.5018 - accuracy: 0.7709 - val_loss: 0.4581 - val_accuracy: 0.7919 - lr: 1.2500e-04\n",
      "Epoch 86/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.4967 - accuracy: 0.7726 - val_loss: 0.4536 - val_accuracy: 0.7897 - lr: 6.2500e-05\n",
      "Epoch 87/100\n",
      "400/400 [==============================] - 36s 89ms/step - loss: 0.4990 - accuracy: 0.7712 - val_loss: 0.4534 - val_accuracy: 0.7909 - lr: 6.2500e-05\n",
      "Epoch 88/100\n",
      "400/400 [==============================] - 36s 90ms/step - loss: 0.4954 - accuracy: 0.7713 - val_loss: 0.4581 - val_accuracy: 0.7887 - lr: 6.2500e-05\n",
      "Epoch 89/100\n",
      "400/400 [==============================] - 35s 87ms/step - loss: 0.4983 - accuracy: 0.7703 - val_loss: 0.4542 - val_accuracy: 0.7894 - lr: 6.2500e-05\n",
      "Epoch 90/100\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.5017 - accuracy: 0.7736 - val_loss: 0.4549 - val_accuracy: 0.7878 - lr: 6.2500e-05\n",
      "Epoch 91/100\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.4965 - accuracy: 0.7724 - val_loss: 0.4547 - val_accuracy: 0.7900 - lr: 6.2500e-05\n",
      "Epoch 92/100\n",
      "400/400 [==============================] - 33s 84ms/step - loss: 0.4974 - accuracy: 0.7745 - val_loss: 0.4553 - val_accuracy: 0.7900 - lr: 6.2500e-05\n",
      "Epoch 93/100\n",
      "400/400 [==============================] - 36s 90ms/step - loss: 0.5000 - accuracy: 0.7711 - val_loss: 0.4546 - val_accuracy: 0.7900 - lr: 6.2500e-05\n",
      "Epoch 94/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.4983 - accuracy: 0.7729 - val_loss: 0.4592 - val_accuracy: 0.7831 - lr: 6.2500e-05\n",
      "Epoch 95/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5013 - accuracy: 0.7709 - val_loss: 0.4622 - val_accuracy: 0.7822 - lr: 6.2500e-05\n",
      "Epoch 96/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.4988 - accuracy: 0.7708 - val_loss: 0.4584 - val_accuracy: 0.7919 - lr: 6.2500e-05\n",
      "Epoch 97/100\n",
      "400/400 [==============================] - 36s 90ms/step - loss: 0.5013 - accuracy: 0.7704 - val_loss: 0.4570 - val_accuracy: 0.7887 - lr: 6.2500e-05\n",
      "Epoch 98/100\n",
      "400/400 [==============================] - 35s 89ms/step - loss: 0.4976 - accuracy: 0.7753 - val_loss: 0.4565 - val_accuracy: 0.7891 - lr: 3.1250e-05\n",
      "Epoch 99/100\n",
      "400/400 [==============================] - 36s 89ms/step - loss: 0.4927 - accuracy: 0.7749 - val_loss: 0.4558 - val_accuracy: 0.7887 - lr: 3.1250e-05\n",
      "Epoch 100/100\n",
      "400/400 [==============================] - 34s 84ms/step - loss: 0.4971 - accuracy: 0.7723 - val_loss: 0.4568 - val_accuracy: 0.7897 - lr: 3.1250e-05\n",
      "100/100 [==============================] - 10s 63ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/23 18:57:31 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/23 18:57:31 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as serving, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp6wf12ns3\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp6wf12ns3\\model\\data\\model\\assets\n",
      "2025/09/23 18:58:47 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp6wf12ns3\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/23 18:58:47 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with BERT model : distilroberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model_1 (TFRobertaM  TFBaseModelOutputWi  82118400   ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, None                                               \n",
      "                                , 768),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_9 (Sl  (None, 768)         0           ['tf_roberta_model_1[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_6 (TFOpLamb  (None, 768)         0           ['tf_roberta_model_1[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_6 (TFOpLam  (None, 768)         0           ['tf_roberta_model_1[0][0]']     \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.concat_6 (TFOpLambda)       (None, 2304)         0           ['tf.__operators__.getitem_9[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.math.reduce_max_6[0][0]',   \n",
      "                                                                  'tf.math.reduce_mean_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 128)          295040      ['tf.concat_6[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_163 (Dropout)          (None, 128)          0           ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 1)            129         ['dropout_163[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 82,413,569\n",
      "Trainable params: 295,169\n",
      "Non-trainable params: 82,118,400\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "400/400 [==============================] - 27s 52ms/step - loss: 0.6599 - accuracy: 0.6293 - val_loss: 0.5960 - val_accuracy: 0.7316 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 24s 61ms/step - loss: 0.5929 - accuracy: 0.6996 - val_loss: 0.5445 - val_accuracy: 0.7481 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 25s 63ms/step - loss: 0.5804 - accuracy: 0.7127 - val_loss: 0.5362 - val_accuracy: 0.7506 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 24s 60ms/step - loss: 0.5689 - accuracy: 0.7183 - val_loss: 0.5422 - val_accuracy: 0.7634 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 25s 62ms/step - loss: 0.5725 - accuracy: 0.7151 - val_loss: 0.5404 - val_accuracy: 0.7666 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 18s 45ms/step - loss: 0.5644 - accuracy: 0.7250 - val_loss: 0.5203 - val_accuracy: 0.7616 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 25s 63ms/step - loss: 0.5642 - accuracy: 0.7225 - val_loss: 0.5081 - val_accuracy: 0.7716 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 18s 44ms/step - loss: 0.5564 - accuracy: 0.7236 - val_loss: 0.5209 - val_accuracy: 0.7603 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 17s 43ms/step - loss: 0.5595 - accuracy: 0.7284 - val_loss: 0.5623 - val_accuracy: 0.7700 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 25s 61ms/step - loss: 0.5627 - accuracy: 0.7202 - val_loss: 0.5005 - val_accuracy: 0.7731 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 24s 60ms/step - loss: 0.5592 - accuracy: 0.7291 - val_loss: 0.5075 - val_accuracy: 0.7744 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 24s 59ms/step - loss: 0.5551 - accuracy: 0.7284 - val_loss: 0.5066 - val_accuracy: 0.7803 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 17s 42ms/step - loss: 0.5517 - accuracy: 0.7274 - val_loss: 0.4840 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 17s 43ms/step - loss: 0.5557 - accuracy: 0.7238 - val_loss: 0.4913 - val_accuracy: 0.7741 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 17s 44ms/step - loss: 0.5469 - accuracy: 0.7302 - val_loss: 0.4984 - val_accuracy: 0.7719 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 17s 42ms/step - loss: 0.5559 - accuracy: 0.7274 - val_loss: 0.4962 - val_accuracy: 0.7788 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 17s 42ms/step - loss: 0.5526 - accuracy: 0.7311 - val_loss: 0.4980 - val_accuracy: 0.7769 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 17s 42ms/step - loss: 0.5527 - accuracy: 0.7198 - val_loss: 0.4977 - val_accuracy: 0.7750 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 17s 42ms/step - loss: 0.5567 - accuracy: 0.7245 - val_loss: 0.5094 - val_accuracy: 0.7803 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 24s 60ms/step - loss: 0.5524 - accuracy: 0.7227 - val_loss: 0.5104 - val_accuracy: 0.7816 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 17s 42ms/step - loss: 0.5382 - accuracy: 0.7341 - val_loss: 0.5173 - val_accuracy: 0.7703 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.5461 - accuracy: 0.7273 - val_loss: 0.4783 - val_accuracy: 0.7812 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 25s 62ms/step - loss: 0.5481 - accuracy: 0.7291 - val_loss: 0.4998 - val_accuracy: 0.7841 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 18s 45ms/step - loss: 0.5572 - accuracy: 0.7228 - val_loss: 0.4870 - val_accuracy: 0.7825 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 18s 45ms/step - loss: 0.5566 - accuracy: 0.7291 - val_loss: 0.5029 - val_accuracy: 0.7591 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 18s 45ms/step - loss: 0.5585 - accuracy: 0.7216 - val_loss: 0.5103 - val_accuracy: 0.7837 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.5618 - accuracy: 0.7170 - val_loss: 0.4877 - val_accuracy: 0.7812 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 20s 51ms/step - loss: 0.5596 - accuracy: 0.7194 - val_loss: 0.4995 - val_accuracy: 0.7816 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 20s 49ms/step - loss: 0.5547 - accuracy: 0.7254 - val_loss: 0.4948 - val_accuracy: 0.7772 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 20s 50ms/step - loss: 0.5643 - accuracy: 0.7118 - val_loss: 0.5335 - val_accuracy: 0.7756 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 27s 66ms/step - loss: 0.5667 - accuracy: 0.7111 - val_loss: 0.4894 - val_accuracy: 0.7856 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 21s 51ms/step - loss: 0.5557 - accuracy: 0.7187 - val_loss: 0.5025 - val_accuracy: 0.7841 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 20s 50ms/step - loss: 0.5501 - accuracy: 0.7221 - val_loss: 0.4915 - val_accuracy: 0.7837 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.5485 - accuracy: 0.7268 - val_loss: 0.4818 - val_accuracy: 0.7844 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 20s 50ms/step - loss: 0.5483 - accuracy: 0.7217 - val_loss: 0.4846 - val_accuracy: 0.7834 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 20s 50ms/step - loss: 0.5430 - accuracy: 0.7301 - val_loss: 0.4814 - val_accuracy: 0.7837 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 20s 49ms/step - loss: 0.5497 - accuracy: 0.7238 - val_loss: 0.4821 - val_accuracy: 0.7828 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 26s 66ms/step - loss: 0.5488 - accuracy: 0.7221 - val_loss: 0.4774 - val_accuracy: 0.7872 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 20s 50ms/step - loss: 0.5479 - accuracy: 0.7255 - val_loss: 0.4884 - val_accuracy: 0.7803 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 18s 45ms/step - loss: 0.5446 - accuracy: 0.7275 - val_loss: 0.4897 - val_accuracy: 0.7728 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 17s 43ms/step - loss: 0.5455 - accuracy: 0.7239 - val_loss: 0.4804 - val_accuracy: 0.7847 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 18s 44ms/step - loss: 0.5466 - accuracy: 0.7256 - val_loss: 0.4956 - val_accuracy: 0.7806 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.5468 - accuracy: 0.7236 - val_loss: 0.4843 - val_accuracy: 0.7841 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 19s 49ms/step - loss: 0.5455 - accuracy: 0.7223 - val_loss: 0.4835 - val_accuracy: 0.7866 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.5453 - accuracy: 0.7247 - val_loss: 0.4868 - val_accuracy: 0.7850 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 17s 43ms/step - loss: 0.5447 - accuracy: 0.7283 - val_loss: 0.4877 - val_accuracy: 0.7822 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - 19s 49ms/step - loss: 0.5468 - accuracy: 0.7197 - val_loss: 0.4858 - val_accuracy: 0.7772 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - 20s 49ms/step - loss: 0.5440 - accuracy: 0.7271 - val_loss: 0.4776 - val_accuracy: 0.7800 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.5405 - accuracy: 0.7318 - val_loss: 0.4745 - val_accuracy: 0.7847 - lr: 2.5000e-04\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.5348 - accuracy: 0.7285 - val_loss: 0.4812 - val_accuracy: 0.7816 - lr: 2.5000e-04\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - 20s 50ms/step - loss: 0.5380 - accuracy: 0.7308 - val_loss: 0.4948 - val_accuracy: 0.7834 - lr: 2.5000e-04\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - 21s 52ms/step - loss: 0.5376 - accuracy: 0.7328 - val_loss: 0.4839 - val_accuracy: 0.7841 - lr: 2.5000e-04\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.5433 - accuracy: 0.7316 - val_loss: 0.4751 - val_accuracy: 0.7859 - lr: 2.5000e-04\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - 19s 49ms/step - loss: 0.5415 - accuracy: 0.7276 - val_loss: 0.4841 - val_accuracy: 0.7850 - lr: 2.5000e-04\n",
      "Epoch 55/100\n",
      "400/400 [==============================] - 19s 49ms/step - loss: 0.5406 - accuracy: 0.7275 - val_loss: 0.4820 - val_accuracy: 0.7853 - lr: 2.5000e-04\n",
      "Epoch 56/100\n",
      "400/400 [==============================] - 19s 49ms/step - loss: 0.5398 - accuracy: 0.7260 - val_loss: 0.4840 - val_accuracy: 0.7856 - lr: 2.5000e-04\n",
      "Epoch 57/100\n",
      "400/400 [==============================] - 20s 49ms/step - loss: 0.5358 - accuracy: 0.7340 - val_loss: 0.4827 - val_accuracy: 0.7869 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "400/400 [==============================] - 20s 49ms/step - loss: 0.5377 - accuracy: 0.7316 - val_loss: 0.4766 - val_accuracy: 0.7837 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "400/400 [==============================] - 26s 66ms/step - loss: 0.5371 - accuracy: 0.7370 - val_loss: 0.4877 - val_accuracy: 0.7887 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "400/400 [==============================] - 27s 66ms/step - loss: 0.5345 - accuracy: 0.7340 - val_loss: 0.4766 - val_accuracy: 0.7900 - lr: 1.2500e-04\n",
      "Epoch 61/100\n",
      "400/400 [==============================] - 20s 50ms/step - loss: 0.5390 - accuracy: 0.7290 - val_loss: 0.4815 - val_accuracy: 0.7869 - lr: 1.2500e-04\n",
      "Epoch 62/100\n",
      "400/400 [==============================] - 20s 49ms/step - loss: 0.5366 - accuracy: 0.7305 - val_loss: 0.4780 - val_accuracy: 0.7891 - lr: 1.2500e-04\n",
      "Epoch 63/100\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.5290 - accuracy: 0.7463 - val_loss: 0.4764 - val_accuracy: 0.7850 - lr: 1.2500e-04\n",
      "Epoch 64/100\n",
      "400/400 [==============================] - 20s 49ms/step - loss: 0.5346 - accuracy: 0.7340 - val_loss: 0.4788 - val_accuracy: 0.7878 - lr: 1.2500e-04\n",
      "Epoch 65/100\n",
      "400/400 [==============================] - 20s 50ms/step - loss: 0.5345 - accuracy: 0.7320 - val_loss: 0.4798 - val_accuracy: 0.7881 - lr: 1.2500e-04\n",
      "Epoch 66/100\n",
      "400/400 [==============================] - 20s 49ms/step - loss: 0.5325 - accuracy: 0.7372 - val_loss: 0.4733 - val_accuracy: 0.7881 - lr: 1.2500e-04\n",
      "Epoch 67/100\n",
      "400/400 [==============================] - 20s 51ms/step - loss: 0.5339 - accuracy: 0.7404 - val_loss: 0.4795 - val_accuracy: 0.7878 - lr: 1.2500e-04\n",
      "Epoch 68/100\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.5353 - accuracy: 0.7336 - val_loss: 0.4792 - val_accuracy: 0.7872 - lr: 1.2500e-04\n",
      "Epoch 69/100\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.5323 - accuracy: 0.7312 - val_loss: 0.4792 - val_accuracy: 0.7859 - lr: 1.2500e-04\n",
      "Epoch 70/100\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.5354 - accuracy: 0.7324 - val_loss: 0.4726 - val_accuracy: 0.7875 - lr: 1.2500e-04\n",
      "Epoch 71/100\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.5362 - accuracy: 0.7330 - val_loss: 0.4737 - val_accuracy: 0.7891 - lr: 1.2500e-04\n",
      "Epoch 72/100\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.5347 - accuracy: 0.7330 - val_loss: 0.4768 - val_accuracy: 0.7844 - lr: 1.2500e-04\n",
      "Epoch 73/100\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.5354 - accuracy: 0.7392 - val_loss: 0.4764 - val_accuracy: 0.7847 - lr: 1.2500e-04\n",
      "Epoch 74/100\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.5371 - accuracy: 0.7319 - val_loss: 0.4740 - val_accuracy: 0.7859 - lr: 1.2500e-04\n",
      "Epoch 75/100\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.5320 - accuracy: 0.7376 - val_loss: 0.4720 - val_accuracy: 0.7891 - lr: 1.2500e-04\n",
      "Epoch 76/100\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.5379 - accuracy: 0.7292 - val_loss: 0.4798 - val_accuracy: 0.7866 - lr: 1.2500e-04\n",
      "Epoch 77/100\n",
      "400/400 [==============================] - 19s 49ms/step - loss: 0.5331 - accuracy: 0.7335 - val_loss: 0.4730 - val_accuracy: 0.7891 - lr: 1.2500e-04\n",
      "Epoch 78/100\n",
      "400/400 [==============================] - 25s 63ms/step - loss: 0.5263 - accuracy: 0.7384 - val_loss: 0.4735 - val_accuracy: 0.7906 - lr: 1.2500e-04\n",
      "Epoch 79/100\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.5310 - accuracy: 0.7383 - val_loss: 0.4784 - val_accuracy: 0.7869 - lr: 1.2500e-04\n",
      "Epoch 80/100\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.5309 - accuracy: 0.7362 - val_loss: 0.4798 - val_accuracy: 0.7812 - lr: 1.2500e-04\n",
      "Epoch 81/100\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.5319 - accuracy: 0.7359 - val_loss: 0.4695 - val_accuracy: 0.7884 - lr: 1.2500e-04\n",
      "Epoch 82/100\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.5327 - accuracy: 0.7340 - val_loss: 0.4784 - val_accuracy: 0.7853 - lr: 1.2500e-04\n",
      "Epoch 83/100\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.5308 - accuracy: 0.7363 - val_loss: 0.4765 - val_accuracy: 0.7881 - lr: 1.2500e-04\n",
      "Epoch 84/100\n",
      "400/400 [==============================] - 18s 44ms/step - loss: 0.5283 - accuracy: 0.7327 - val_loss: 0.4737 - val_accuracy: 0.7869 - lr: 1.2500e-04\n",
      "Epoch 85/100\n",
      "400/400 [==============================] - 18s 44ms/step - loss: 0.5223 - accuracy: 0.7406 - val_loss: 0.4700 - val_accuracy: 0.7869 - lr: 1.2500e-04\n",
      "Epoch 86/100\n",
      "400/400 [==============================] - 17s 43ms/step - loss: 0.5338 - accuracy: 0.7379 - val_loss: 0.4738 - val_accuracy: 0.7884 - lr: 1.2500e-04\n",
      "Epoch 87/100\n",
      "400/400 [==============================] - 17s 44ms/step - loss: 0.5348 - accuracy: 0.7337 - val_loss: 0.4756 - val_accuracy: 0.7878 - lr: 1.2500e-04\n",
      "Epoch 88/100\n",
      "400/400 [==============================] - 18s 45ms/step - loss: 0.5328 - accuracy: 0.7379 - val_loss: 0.4730 - val_accuracy: 0.7903 - lr: 1.2500e-04\n",
      "Epoch 89/100\n",
      "400/400 [==============================] - 17s 43ms/step - loss: 0.5286 - accuracy: 0.7391 - val_loss: 0.4761 - val_accuracy: 0.7903 - lr: 1.2500e-04\n",
      "Epoch 90/100\n",
      "400/400 [==============================] - 17s 44ms/step - loss: 0.5327 - accuracy: 0.7359 - val_loss: 0.4713 - val_accuracy: 0.7859 - lr: 1.2500e-04\n",
      "Epoch 91/100\n",
      "400/400 [==============================] - 18s 44ms/step - loss: 0.5353 - accuracy: 0.7348 - val_loss: 0.4704 - val_accuracy: 0.7866 - lr: 1.2500e-04\n",
      "Epoch 92/100\n",
      "400/400 [==============================] - 18s 44ms/step - loss: 0.5301 - accuracy: 0.7391 - val_loss: 0.4705 - val_accuracy: 0.7894 - lr: 6.2500e-05\n",
      "Epoch 93/100\n",
      "400/400 [==============================] - 18s 44ms/step - loss: 0.5270 - accuracy: 0.7360 - val_loss: 0.4713 - val_accuracy: 0.7887 - lr: 6.2500e-05\n",
      "Epoch 94/100\n",
      "400/400 [==============================] - 18s 44ms/step - loss: 0.5313 - accuracy: 0.7420 - val_loss: 0.4709 - val_accuracy: 0.7853 - lr: 6.2500e-05\n",
      "Epoch 95/100\n",
      "400/400 [==============================] - 18s 44ms/step - loss: 0.5306 - accuracy: 0.7385 - val_loss: 0.4707 - val_accuracy: 0.7900 - lr: 6.2500e-05\n",
      "Epoch 96/100\n",
      "400/400 [==============================] - 17s 44ms/step - loss: 0.5288 - accuracy: 0.7368 - val_loss: 0.4717 - val_accuracy: 0.7847 - lr: 6.2500e-05\n",
      "Epoch 97/100\n",
      "400/400 [==============================] - 17s 44ms/step - loss: 0.5263 - accuracy: 0.7380 - val_loss: 0.4711 - val_accuracy: 0.7900 - lr: 6.2500e-05\n",
      "Epoch 98/100\n",
      "400/400 [==============================] - 18s 45ms/step - loss: 0.5282 - accuracy: 0.7373 - val_loss: 0.4709 - val_accuracy: 0.7847 - lr: 6.2500e-05\n",
      "Epoch 99/100\n",
      "400/400 [==============================] - 18s 45ms/step - loss: 0.5285 - accuracy: 0.7371 - val_loss: 0.4714 - val_accuracy: 0.7897 - lr: 6.2500e-05\n",
      "Epoch 100/100\n",
      "400/400 [==============================] - 17s 44ms/step - loss: 0.5291 - accuracy: 0.7366 - val_loss: 0.4720 - val_accuracy: 0.7887 - lr: 6.2500e-05\n",
      "100/100 [==============================] - 5s 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/23 19:31:49 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/23 19:31:49 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as serving, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 217). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmppgvb3xrs\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmppgvb3xrs\\model\\data\\model\\assets\n",
      "2025/09/23 19:32:30 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmppgvb3xrs\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/23 19:32:30 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with BERT model : vinai/bertweet-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.embeddings.position_ids', 'lm_head.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model_2 (TFRobertaM  TFBaseModelOutputWi  134899968  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, None                                               \n",
      "                                , 768),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_10 (S  (None, 768)         0           ['tf_roberta_model_2[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_7 (TFOpLamb  (None, 768)         0           ['tf_roberta_model_2[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_7 (TFOpLam  (None, 768)         0           ['tf_roberta_model_2[0][0]']     \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.concat_7 (TFOpLambda)       (None, 2304)         0           ['tf.__operators__.getitem_10[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.math.reduce_max_7[0][0]',   \n",
      "                                                                  'tf.math.reduce_mean_7[0][0]']  \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 128)          295040      ['tf.concat_7[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_201 (Dropout)          (None, 128)          0           ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 1)            129         ['dropout_201[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 135,195,137\n",
      "Trainable params: 295,169\n",
      "Non-trainable params: 134,899,968\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "400/400 [==============================] - 47s 92ms/step - loss: 0.6558 - accuracy: 0.6291 - val_loss: 0.5840 - val_accuracy: 0.7375 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 42s 105ms/step - loss: 0.5766 - accuracy: 0.7087 - val_loss: 0.5252 - val_accuracy: 0.7778 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 42s 104ms/step - loss: 0.5443 - accuracy: 0.7270 - val_loss: 0.5031 - val_accuracy: 0.7922 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 42s 105ms/step - loss: 0.5298 - accuracy: 0.7373 - val_loss: 0.4825 - val_accuracy: 0.8034 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 42s 105ms/step - loss: 0.5245 - accuracy: 0.7447 - val_loss: 0.4548 - val_accuracy: 0.8053 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 40s 100ms/step - loss: 0.5103 - accuracy: 0.7548 - val_loss: 0.4542 - val_accuracy: 0.8134 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.5229 - accuracy: 0.7413 - val_loss: 0.4665 - val_accuracy: 0.8122 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 32s 80ms/step - loss: 0.5245 - accuracy: 0.7313 - val_loss: 0.4592 - val_accuracy: 0.8022 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 32s 81ms/step - loss: 0.5130 - accuracy: 0.7458 - val_loss: 0.4229 - val_accuracy: 0.8134 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 32s 80ms/step - loss: 0.5127 - accuracy: 0.7457 - val_loss: 0.4400 - val_accuracy: 0.8112 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.5199 - accuracy: 0.7402 - val_loss: 0.4307 - val_accuracy: 0.8122 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 34s 84ms/step - loss: 0.5051 - accuracy: 0.7501 - val_loss: 0.4283 - val_accuracy: 0.8094 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 41s 102ms/step - loss: 0.5042 - accuracy: 0.7555 - val_loss: 0.4264 - val_accuracy: 0.8169 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 34s 85ms/step - loss: 0.4969 - accuracy: 0.7577 - val_loss: 0.4307 - val_accuracy: 0.8109 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 34s 85ms/step - loss: 0.5026 - accuracy: 0.7634 - val_loss: 0.4743 - val_accuracy: 0.8109 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 34s 85ms/step - loss: 0.5176 - accuracy: 0.7550 - val_loss: 0.4538 - val_accuracy: 0.8163 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 41s 101ms/step - loss: 0.5169 - accuracy: 0.7531 - val_loss: 0.4507 - val_accuracy: 0.8172 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 34s 85ms/step - loss: 0.5182 - accuracy: 0.7537 - val_loss: 0.4411 - val_accuracy: 0.8147 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5189 - accuracy: 0.7535 - val_loss: 0.4360 - val_accuracy: 0.8125 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5043 - accuracy: 0.7677 - val_loss: 0.4207 - val_accuracy: 0.8163 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 36s 89ms/step - loss: 0.5050 - accuracy: 0.7668 - val_loss: 0.4478 - val_accuracy: 0.8034 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.5027 - accuracy: 0.7638 - val_loss: 0.4228 - val_accuracy: 0.8156 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 35s 87ms/step - loss: 0.5056 - accuracy: 0.7625 - val_loss: 0.4259 - val_accuracy: 0.8141 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 35s 87ms/step - loss: 0.4967 - accuracy: 0.7639 - val_loss: 0.4145 - val_accuracy: 0.8159 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.4984 - accuracy: 0.7715 - val_loss: 0.4205 - val_accuracy: 0.8131 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 36s 91ms/step - loss: 0.4956 - accuracy: 0.7717 - val_loss: 0.4145 - val_accuracy: 0.8181 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 30s 73ms/step - loss: 0.4990 - accuracy: 0.7670 - val_loss: 0.4395 - val_accuracy: 0.8087 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 28s 70ms/step - loss: 0.4976 - accuracy: 0.7689 - val_loss: 0.4222 - val_accuracy: 0.8138 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 28s 70ms/step - loss: 0.4993 - accuracy: 0.7714 - val_loss: 0.4253 - val_accuracy: 0.8106 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 30s 76ms/step - loss: 0.5026 - accuracy: 0.7676 - val_loss: 0.4196 - val_accuracy: 0.8150 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 874s 2s/step - loss: 0.4971 - accuracy: 0.7660 - val_loss: 0.4213 - val_accuracy: 0.8109 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 41s 103ms/step - loss: 0.4901 - accuracy: 0.7802 - val_loss: 0.4157 - val_accuracy: 0.8147 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 44s 110ms/step - loss: 0.4885 - accuracy: 0.7737 - val_loss: 0.4245 - val_accuracy: 0.8091 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 56s 141ms/step - loss: 0.4970 - accuracy: 0.7732 - val_loss: 0.4149 - val_accuracy: 0.8184 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 43s 107ms/step - loss: 0.4908 - accuracy: 0.7719 - val_loss: 0.4205 - val_accuracy: 0.8163 - lr: 2.5000e-04\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 43s 107ms/step - loss: 0.4930 - accuracy: 0.7731 - val_loss: 0.4246 - val_accuracy: 0.8156 - lr: 2.5000e-04\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 42s 106ms/step - loss: 0.4867 - accuracy: 0.7764 - val_loss: 0.4157 - val_accuracy: 0.8172 - lr: 2.5000e-04\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 42s 106ms/step - loss: 0.4880 - accuracy: 0.7734 - val_loss: 0.4228 - val_accuracy: 0.8175 - lr: 2.5000e-04\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 43s 107ms/step - loss: 0.4872 - accuracy: 0.7735 - val_loss: 0.4160 - val_accuracy: 0.8153 - lr: 2.5000e-04\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 42s 106ms/step - loss: 0.4832 - accuracy: 0.7779 - val_loss: 0.4336 - val_accuracy: 0.8116 - lr: 2.5000e-04\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 42s 106ms/step - loss: 0.4924 - accuracy: 0.7761 - val_loss: 0.4220 - val_accuracy: 0.8138 - lr: 2.5000e-04\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 42s 106ms/step - loss: 0.4923 - accuracy: 0.7725 - val_loss: 0.4190 - val_accuracy: 0.8134 - lr: 2.5000e-04\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 42s 106ms/step - loss: 0.4946 - accuracy: 0.7665 - val_loss: 0.4222 - val_accuracy: 0.8153 - lr: 2.5000e-04\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 43s 106ms/step - loss: 0.4890 - accuracy: 0.7727 - val_loss: 0.4275 - val_accuracy: 0.8138 - lr: 2.5000e-04\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 43s 106ms/step - loss: 0.4823 - accuracy: 0.7788 - val_loss: 0.4201 - val_accuracy: 0.8163 - lr: 1.2500e-04\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 43s 107ms/step - loss: 0.4806 - accuracy: 0.7824 - val_loss: 0.4210 - val_accuracy: 0.8153 - lr: 1.2500e-04\n",
      "100/100 [==============================] - 13s 80ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/23 20:16:25 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/23 20:16:25 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as serving, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpj9wymnw0\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpj9wymnw0\\model\\data\\model\\assets\n",
      "2025/09/23 20:17:59 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpj9wymnw0\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/23 20:17:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with BERT model : finiteautomata/bertweet-base-sentiment-analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model_3 (TFRobertaM  TFBaseModelOutputWi  134899968  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, None                                               \n",
      "                                , 768),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_11 (S  (None, 768)         0           ['tf_roberta_model_3[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_8 (TFOpLamb  (None, 768)         0           ['tf_roberta_model_3[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_8 (TFOpLam  (None, 768)         0           ['tf_roberta_model_3[0][0]']     \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.concat_8 (TFOpLambda)       (None, 2304)         0           ['tf.__operators__.getitem_11[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.math.reduce_max_8[0][0]',   \n",
      "                                                                  'tf.math.reduce_mean_8[0][0]']  \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 128)          295040      ['tf.concat_8[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_239 (Dropout)          (None, 128)          0           ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 1)            129         ['dropout_239[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 135,195,137\n",
      "Trainable params: 295,169\n",
      "Non-trainable params: 134,899,968\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "400/400 [==============================] - 64s 125ms/step - loss: 0.4810 - accuracy: 0.7929 - val_loss: 0.4260 - val_accuracy: 0.8106 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 57s 141ms/step - loss: 0.4471 - accuracy: 0.8092 - val_loss: 0.4118 - val_accuracy: 0.8206 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 57s 143ms/step - loss: 0.4401 - accuracy: 0.8139 - val_loss: 0.4091 - val_accuracy: 0.8250 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 44s 109ms/step - loss: 0.4343 - accuracy: 0.8171 - val_loss: 0.4070 - val_accuracy: 0.8241 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 55s 138ms/step - loss: 0.4296 - accuracy: 0.8177 - val_loss: 0.4028 - val_accuracy: 0.8341 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 43s 107ms/step - loss: 0.4247 - accuracy: 0.8218 - val_loss: 0.3986 - val_accuracy: 0.8319 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 58s 144ms/step - loss: 0.4281 - accuracy: 0.8166 - val_loss: 0.4008 - val_accuracy: 0.8347 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 44s 109ms/step - loss: 0.4252 - accuracy: 0.8216 - val_loss: 0.3984 - val_accuracy: 0.8291 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 54s 135ms/step - loss: 0.4231 - accuracy: 0.8210 - val_loss: 0.3932 - val_accuracy: 0.8356 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 42s 106ms/step - loss: 0.4204 - accuracy: 0.8223 - val_loss: 0.3953 - val_accuracy: 0.8328 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 42s 105ms/step - loss: 0.4249 - accuracy: 0.8234 - val_loss: 0.3983 - val_accuracy: 0.8331 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 43s 108ms/step - loss: 0.4212 - accuracy: 0.8230 - val_loss: 0.3961 - val_accuracy: 0.8334 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 43s 107ms/step - loss: 0.4203 - accuracy: 0.8230 - val_loss: 0.4017 - val_accuracy: 0.8328 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 43s 107ms/step - loss: 0.4192 - accuracy: 0.8250 - val_loss: 0.3989 - val_accuracy: 0.8341 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 42s 106ms/step - loss: 0.4158 - accuracy: 0.8220 - val_loss: 0.3927 - val_accuracy: 0.8347 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 53s 133ms/step - loss: 0.4209 - accuracy: 0.8224 - val_loss: 0.3960 - val_accuracy: 0.8369 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 41s 103ms/step - loss: 0.4164 - accuracy: 0.8234 - val_loss: 0.3984 - val_accuracy: 0.8331 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 42s 104ms/step - loss: 0.4182 - accuracy: 0.8262 - val_loss: 0.4232 - val_accuracy: 0.8100 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 41s 102ms/step - loss: 0.4195 - accuracy: 0.8226 - val_loss: 0.3947 - val_accuracy: 0.8309 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 43s 107ms/step - loss: 0.4196 - accuracy: 0.8248 - val_loss: 0.3949 - val_accuracy: 0.8366 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 42s 104ms/step - loss: 0.4168 - accuracy: 0.8255 - val_loss: 0.3930 - val_accuracy: 0.8309 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 42s 106ms/step - loss: 0.4170 - accuracy: 0.8258 - val_loss: 0.3919 - val_accuracy: 0.8366 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 41s 103ms/step - loss: 0.4221 - accuracy: 0.8223 - val_loss: 0.3995 - val_accuracy: 0.8350 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 42s 104ms/step - loss: 0.4156 - accuracy: 0.8255 - val_loss: 0.3933 - val_accuracy: 0.8359 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 43s 107ms/step - loss: 0.4151 - accuracy: 0.8276 - val_loss: 0.3926 - val_accuracy: 0.8356 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 42s 106ms/step - loss: 0.4196 - accuracy: 0.8230 - val_loss: 0.4001 - val_accuracy: 0.8288 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 43s 107ms/step - loss: 0.4168 - accuracy: 0.8239 - val_loss: 0.3964 - val_accuracy: 0.8359 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 42s 106ms/step - loss: 0.4131 - accuracy: 0.8272 - val_loss: 0.4016 - val_accuracy: 0.8328 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 54s 135ms/step - loss: 0.4184 - accuracy: 0.8231 - val_loss: 0.3950 - val_accuracy: 0.8409 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 42s 105ms/step - loss: 0.4163 - accuracy: 0.8259 - val_loss: 0.3943 - val_accuracy: 0.8372 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 43s 107ms/step - loss: 0.4185 - accuracy: 0.8229 - val_loss: 0.3967 - val_accuracy: 0.8353 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 42s 106ms/step - loss: 0.4192 - accuracy: 0.8263 - val_loss: 0.3982 - val_accuracy: 0.8350 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 42s 105ms/step - loss: 0.4068 - accuracy: 0.8309 - val_loss: 0.3917 - val_accuracy: 0.8394 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 42s 105ms/step - loss: 0.4080 - accuracy: 0.8280 - val_loss: 0.3945 - val_accuracy: 0.8409 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.4045 - accuracy: 0.8314 - val_loss: 0.3905 - val_accuracy: 0.8394 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 34s 86ms/step - loss: 0.4030 - accuracy: 0.8314 - val_loss: 0.3918 - val_accuracy: 0.8422 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 27s 68ms/step - loss: 0.3991 - accuracy: 0.8330 - val_loss: 0.3893 - val_accuracy: 0.8406 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 27s 68ms/step - loss: 0.4047 - accuracy: 0.8308 - val_loss: 0.3896 - val_accuracy: 0.8388 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 27s 68ms/step - loss: 0.4004 - accuracy: 0.8300 - val_loss: 0.3918 - val_accuracy: 0.8391 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 28s 69ms/step - loss: 0.3968 - accuracy: 0.8334 - val_loss: 0.3892 - val_accuracy: 0.8400 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.4021 - accuracy: 0.8310 - val_loss: 0.3895 - val_accuracy: 0.8413 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3989 - accuracy: 0.8320 - val_loss: 0.3892 - val_accuracy: 0.8403 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3987 - accuracy: 0.8281 - val_loss: 0.3891 - val_accuracy: 0.8406 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.4001 - accuracy: 0.8312 - val_loss: 0.3901 - val_accuracy: 0.8422 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3991 - accuracy: 0.8304 - val_loss: 0.3900 - val_accuracy: 0.8400 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3969 - accuracy: 0.8335 - val_loss: 0.3894 - val_accuracy: 0.8372 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3975 - accuracy: 0.8339 - val_loss: 0.3928 - val_accuracy: 0.8406 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3977 - accuracy: 0.8333 - val_loss: 0.3910 - val_accuracy: 0.8397 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3967 - accuracy: 0.8305 - val_loss: 0.3873 - val_accuracy: 0.8406 - lr: 5.0000e-04\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3973 - accuracy: 0.8324 - val_loss: 0.3907 - val_accuracy: 0.8391 - lr: 5.0000e-04\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3967 - accuracy: 0.8355 - val_loss: 0.3891 - val_accuracy: 0.8403 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3933 - accuracy: 0.8340 - val_loss: 0.3908 - val_accuracy: 0.8394 - lr: 5.0000e-04\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3987 - accuracy: 0.8334 - val_loss: 0.3921 - val_accuracy: 0.8397 - lr: 5.0000e-04\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3936 - accuracy: 0.8326 - val_loss: 0.3946 - val_accuracy: 0.8375 - lr: 5.0000e-04\n",
      "Epoch 55/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3964 - accuracy: 0.8348 - val_loss: 0.3927 - val_accuracy: 0.8369 - lr: 5.0000e-04\n",
      "Epoch 56/100\n",
      "400/400 [==============================] - 28s 70ms/step - loss: 0.3932 - accuracy: 0.8366 - val_loss: 0.3922 - val_accuracy: 0.8353 - lr: 5.0000e-04\n",
      "Epoch 57/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3952 - accuracy: 0.8329 - val_loss: 0.3920 - val_accuracy: 0.8388 - lr: 5.0000e-04\n",
      "Epoch 58/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3974 - accuracy: 0.8338 - val_loss: 0.3920 - val_accuracy: 0.8409 - lr: 5.0000e-04\n",
      "Epoch 59/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3967 - accuracy: 0.8313 - val_loss: 0.3915 - val_accuracy: 0.8391 - lr: 5.0000e-04\n",
      "Epoch 60/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3837 - accuracy: 0.8354 - val_loss: 0.3905 - val_accuracy: 0.8394 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3874 - accuracy: 0.8367 - val_loss: 0.3922 - val_accuracy: 0.8416 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3849 - accuracy: 0.8373 - val_loss: 0.3885 - val_accuracy: 0.8391 - lr: 2.5000e-04\n",
      "Epoch 63/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3867 - accuracy: 0.8392 - val_loss: 0.3936 - val_accuracy: 0.8413 - lr: 2.5000e-04\n",
      "Epoch 64/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3819 - accuracy: 0.8379 - val_loss: 0.3924 - val_accuracy: 0.8394 - lr: 2.5000e-04\n",
      "Epoch 65/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3813 - accuracy: 0.8363 - val_loss: 0.3928 - val_accuracy: 0.8397 - lr: 2.5000e-04\n",
      "Epoch 66/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3816 - accuracy: 0.8358 - val_loss: 0.3915 - val_accuracy: 0.8381 - lr: 2.5000e-04\n",
      "Epoch 67/100\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.3838 - accuracy: 0.8351 - val_loss: 0.3915 - val_accuracy: 0.8384 - lr: 2.5000e-04\n",
      "Epoch 68/100\n",
      "400/400 [==============================] - 29s 71ms/step - loss: 0.3807 - accuracy: 0.8390 - val_loss: 0.3892 - val_accuracy: 0.8413 - lr: 2.5000e-04\n",
      "Epoch 69/100\n",
      "400/400 [==============================] - 28s 70ms/step - loss: 0.3820 - accuracy: 0.8369 - val_loss: 0.3907 - val_accuracy: 0.8391 - lr: 2.5000e-04\n",
      "100/100 [==============================] - 11s 54ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/23 21:01:02 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/23 21:01:02 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as serving, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp6apxj6iv\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp6apxj6iv\\model\\data\\model\\assets\n",
      "2025/09/23 21:01:55 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp6apxj6iv\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/23 21:01:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "for model_name in bert_model_list:\n",
    "    print(f\"Running test with BERT model : {model_name}\")\n",
    "    test_bert_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a48d89",
   "metadata": {},
   "source": [
    "# Optimisation de la t√™te du mod√®le (plus petit dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b75d69",
   "metadata": {},
   "source": [
    "## R√©duction du dataset d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c515d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 16000 rows\n"
     ]
    }
   ],
   "source": [
    "sample_df, _ = train_test_split(df, test_size=0.99, random_state=42, stratify=df['target'])\n",
    "sample_df = sample_df.reset_index(drop=True)\n",
    "print(f\"Sample size: {sample_df.shape[0]} rows\")\n",
    "# On ne garde que les colonnes 'target' et 'text'\n",
    "sample_df = sample_df[['target', 'text']]\n",
    "sample_df[\"target\"] = sample_df[\"target\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "sample_df.to_csv('Data/raw_data_mini.csv', index=False)\n",
    "\n",
    "# Data\n",
    "X_raw = sample_df['text']\n",
    "y = sample_df['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_raw, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a946834",
   "metadata": {},
   "source": [
    "Fonction de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc87c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "def test_bert_model_v2(bert_model_name, rnn_size):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(params={\n",
    "            'rnn_size': rnn_size, \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr,\n",
    "            'bert_model_name':bert_model_name\n",
    "        })\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "        encodings_train = tokenizer(X_train.to_list(), truncation=True, padding=True, return_tensors=\"tf\")\n",
    "        encodings_val = tokenizer(X_val.to_list(), truncation=True, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "        dataset_train = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                {\"input_ids\": encodings_train[\"input_ids\"], \n",
    "                 \"attention_mask\": encodings_train[\"attention_mask\"]\n",
    "                 },y_train\n",
    "                )\n",
    "                ).batch(32)\n",
    "        \n",
    "        dataset_val = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                {\"input_ids\": encodings_val[\"input_ids\"], \n",
    "                 \"attention_mask\": encodings_val[\"attention_mask\"]\n",
    "                 },y_val\n",
    "                )\n",
    "                ).batch(32)\n",
    "        # On charge le mod√®le pr√©-entrainn√©\n",
    "        base_model = TFAutoModel.from_pretrained(bert_model_name, from_pt=True)\n",
    "        base_model.trainable = False # Pas de fine-tuning ou d'entrainement car impossible √† faire avec les ressources disponibles\n",
    "\n",
    "        # Construction du mod√®le keras \n",
    "        ## Une input layer pour les input ids\n",
    "\n",
    "        input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "        ## Une input layer pour le masque d'attention\n",
    "        attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "        ## On r√©cup√®re \n",
    "\n",
    "        outputs = base_model(input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n",
    "\n",
    "        # On prend le token [CLS] comme vecteur de phrase\n",
    "        cls_token = token_embeddings[:, 0, :]\n",
    "        max_tokens = tf.reduce_max(token_embeddings, axis=1)  #\n",
    "        mean_tokens = tf.reduce_mean(token_embeddings, axis=1)  #\n",
    "\n",
    "        all_tokens = tf.concat([cls_token, max_tokens, mean_tokens], axis=-1)  # [batch, hidden_size*2]\n",
    "    \n",
    "        x = tf.keras.layers.Dense(rnn_size, activation=\"relu\", \n",
    "                                  kernel_regularizer=regularizers.L2(1e-4),\n",
    "                                  bias_regularizer=regularizers.L2(1e-4), \n",
    "                                #   activity_regularizer=regularizers.L2(1e-4),\n",
    "                                  )(all_tokens)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        logits = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "        model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=logits)\n",
    "\n",
    "\n",
    "        ## Callbacks\n",
    "        model_savepath = f\"./Models/MY_{'_'.join(bert_model_name.split('/'))}_dense{rnn_size}.h5\"\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "\n",
    "\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "        # Summary\n",
    "        model.summary()\n",
    "        # History\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(dataset_train, epochs=epochs, batch_size=64, validation_data=dataset_val, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "                # Pr√©dictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(dataset_val)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des m√©triques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du mod√®le dans MLflow\n",
    "        mlflow.tensorflow.log_model(\n",
    "            model,\n",
    "            artifact_path=\"Bert_best_Model\",\n",
    "            keras_model_kwargs={\"save_format\":\"h5\"}\n",
    "        )    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71b98277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MLflow experiment...\n"
     ]
    }
   ],
   "source": [
    "# Cr√©ation de l'√©tude Optuna et optimisation\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"BERT_models_experiment\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"BERT_models_experiment\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Optimisation de la t√™te du mod√®le bas√© sur BERT.\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"BERT_pretrained\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25d4578",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f39cadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with BERT model finiteautomata/bertweet-base-sentiment-analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model_4 (TFRobertaM  TFBaseModelOutputWi  134899968  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, None                                               \n",
      "                                , 768),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_12 (S  (None, 768)         0           ['tf_roberta_model_4[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 64)           49216       ['tf.__operators__.getitem_12[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dropout_277 (Dropout)          (None, 64)           0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 1)            65          ['dropout_277[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 134,949,249\n",
      "Trainable params: 49,281\n",
      "Non-trainable params: 134,899,968\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "400/400 [==============================] - 73s 163ms/step - loss: 0.4545 - accuracy: 0.7975 - val_loss: 0.4084 - val_accuracy: 0.8122 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "354/400 [=========================>....] - ETA: 6s - loss: 0.4284 - accuracy: 0.8089"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rnn_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m254\u001b[39m,\u001b[38;5;241m64\u001b[39m)):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning test with BERT model finiteautomata/bertweet-base-sentiment-analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtest_bert_model_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfiniteautomata/bertweet-base-sentiment-analysis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrnn_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 72\u001b[0m, in \u001b[0;36mtest_bert_model_v2\u001b[1;34m(bert_model_name, rnn_size)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# History\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 72\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(model_savepath)\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;66;03m# Pr√©dictions sur le jeu de validation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for rnn_size in list(range(64,254,64)):\n",
    "    print(f\"Running test with BERT model finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "    test_bert_model_v2(bert_model_name=\"finiteautomata/bertweet-base-sentiment-analysis\", rnn_size=rnn_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ada447",
   "metadata": {},
   "source": [
    "# Mod√®le Bert automatique depuis Huggingface \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a860e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some weights of TFRobertaForSequenceClassification were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier/out_proj/kernel:0: found shape (768, 3) in the checkpoint and (768, 2) in the model instantiated\n",
      "- classifier/out_proj/bias:0: found shape (3,) in the checkpoint and (2,) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFRobertaMainLayer  multiple                 134309376 \n",
      " )                                                               \n",
      "                                                                 \n",
      " classifier (TFRobertaClassi  multiple                 592130    \n",
      " ficationHead)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,901,506\n",
      "Trainable params: 592,130\n",
      "Non-trainable params: 134,309,376\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1614, in train_step\n        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n\n    AttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mloss, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Exemple d‚Äôentra√Ænement\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mid2label)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1213\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1212\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file6pa0f96u.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1614\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_using_dummy_loss \u001b[38;5;129;01mand\u001b[39;00m parse(tf\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m parse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.11.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1612\u001b[0m     \u001b[38;5;66;03m# Newer TF train steps leave this out\u001b[39;00m\n\u001b[0;32m   1613\u001b[0m     data \u001b[38;5;241m=\u001b[39m expand_1d(data)\n\u001b[1;32m-> 1614\u001b[0m x, y, sample_weight \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_x_y_sample_weight\u001b[49m(data)\n\u001b[0;32m   1615\u001b[0m \u001b[38;5;66;03m# If the inputs are mutable dictionaries, make a shallow copy of them because we will modify\u001b[39;00m\n\u001b[0;32m   1616\u001b[0m \u001b[38;5;66;03m# them during input/label pre-processing. This avoids surprising the user by wrecking their data.\u001b[39;00m\n\u001b[0;32m   1617\u001b[0m \u001b[38;5;66;03m# In addition, modifying mutable Python inputs makes XLA compilation impossible.\u001b[39;00m\n\u001b[0;32m   1618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1614, in train_step\n        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n\n    AttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "# Charger le tokenizer et le mod√®le automatiquement (transformers choisit RobertaTokenizer / TFRobertaForSequenceClassification)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "                                                        \"finiteautomata/bertweet-base-sentiment-analysis\", \n",
    "                                                        num_labels=2,\n",
    "                                                        ignore_mismatched_sizes=True )\n",
    "\n",
    "for layer in model.layers:\n",
    "    if layer.name != \"classifier\":   # nom de la couche finale\n",
    "        layer.trainable = False\n",
    "\n",
    "# V√©rification\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Tokenisation\n",
    "encodings_train = tokenizer(X_train.to_list(), truncation=True, padding=True, return_tensors=\"tf\")\n",
    "encodings_val = tokenizer(X_val.to_list(), truncation=True, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        {\n",
    "        \"input_ids\": encodings_train[\"input_ids\"], \n",
    "        \"attention_mask\": encodings_train[\"attention_mask\"]\n",
    "        },y_train\n",
    "    )\n",
    "    ).batch(32)\n",
    "        \n",
    "dataset_val = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        {\n",
    "        \"input_ids\": encodings_val[\"input_ids\"], \n",
    "        \"attention_mask\": encodings_val[\"attention_mask\"]\n",
    "        },y_val\n",
    "    )\n",
    "    ).batch(32)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)  # LR plus √©lev√© car seules quelques couches sont entra√Æn√©es\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "rename = '_'.join(\"finiteautomata/bertweet-base-sentiment-analysis\".split('/'))\n",
    "        ## Callbacks\n",
    "model_savepath = f\"./Models/AUTO_{rename}.h5\"\n",
    "checkpoint = ModelCheckpoint(model_savepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-5)\n",
    "callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# Exemple d‚Äôentra√Ænement\n",
    "history = model.fit(dataset_train, validation_data=dataset_val, epochs=50,callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "print(model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6225eb03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env_P7_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
