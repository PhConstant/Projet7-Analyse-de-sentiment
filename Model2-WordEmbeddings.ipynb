{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c17c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\tensorflow_hub\\__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n",
      "c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "Num GPUs Available:  1\n",
      "GPUs disponibles : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Version TF : 2.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, SimpleRNN, Dense, LSTM, Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim.downloader as gensim_downloader\n",
    "import gensim\n",
    "import multiprocessing\n",
    "from mlflow import MlflowClient\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from Source.preprocess_data import *  ## import all functions from preprocess_data.py\n",
    "from Source.postprocess_data import * ## import all functions from postprocess_data.py\n",
    "from Source.utils import *  ## import all functions from utils.py\n",
    "import nltk\n",
    "import optuna\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import TweetTokenizer, WordPunctTokenizer, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, LancasterStemmer, SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "nw = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "\n",
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")\n",
    "os.environ[\"TF_KERAS\"]='1'\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPUs disponibles :\", tf.config.list_physical_devices(\"GPU\"))\n",
    "print(\"Version TF :\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "969e2c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 160000 rows\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/AI+Engineer/Project+7%C2%A0-+D%C3%A9tectez+les+Bad+Buzz+gr%C3%A2ce+au+Deep+Learning/sentiment140.zip',\n",
    "                header=None,\n",
    "                compression='zip',\n",
    "                encoding='cp1252')\n",
    "\n",
    "df.columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "sample_df, _ = train_test_split(df, test_size=0.9, random_state=42, stratify=df['target'])\n",
    "sample_df = sample_df.reset_index(drop=True)\n",
    "print(f\"Sample size: {sample_df.shape[0]} rows\")\n",
    "# On ne garde que les colonnes 'target' et 'text'\n",
    "sample_df = sample_df[['target', 'text']]\n",
    "sample_df[\"target\"] = sample_df[\"target\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "sample_df.to_csv('Data/raw_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2583e99",
   "metadata": {},
   "source": [
    "# Séparation train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8a5c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X_raw = sample_df['text']\n",
    "y = sample_df['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_raw, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f95011c",
   "metadata": {},
   "source": [
    "# Préparation de l'experience de base (baseline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fbc2cd",
   "metadata": {},
   "source": [
    "## Pré-traitement des dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "478f1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 20000\n",
    "max_len = 10\n",
    "min_count = 3\n",
    "\n",
    "X_sentence_train, tokenizer, sentences_train = preprocess_data_embedding(X_raw=X_train, \n",
    "                                                        stem_lem_func=PorterStemmer().stem,\n",
    "                                                        tokenizer=None, \n",
    "                                                        stop_words=stopwords.words('english'), \n",
    "                                                        min_count=min_count,\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words, \n",
    "                                                        return_sentences=True) \n",
    "X_sentence_val = preprocess_data_embedding(X_raw=X_val, \n",
    "                                                        stem_lem_func=PorterStemmer().stem,\n",
    "                                                        tokenizer=tokenizer, \n",
    "                                                        stop_words=stopwords.words('english'), \n",
    "                                                        min_count=1, # mincount = 1 car on est sur le jeu de validation\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef45eb84",
   "metadata": {},
   "source": [
    "# Création d'un embedding de base custom pour notre modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12bc38bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build & train Word2Vec model ...\n",
      "Vocabulary size: 13547\n",
      "Word2Vec trained\n",
      "Coverage: 99.99%\n",
      "Mean norm: 7.0292816\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "latent_dim = 50\n",
    "print(\"Build & train Word2Vec model ...\")\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences_train, \n",
    "    vector_size=latent_dim,  # dimension de l’espace latent\n",
    "    window=5,         # taille du contexte\n",
    "    min_count=min_count,      # ignorer les mots trop rares\n",
    "    workers=4,        # parallélisme CPU\n",
    "    sg=0,              # 1 = skip-gram, 0 = CBOW\n",
    "    epochs=100\n",
    ")\n",
    "\n",
    "\n",
    "model_vectors = w2v_model.wv\n",
    "w2v_words = model_vectors.index_to_key\n",
    "print(\"Vocabulary size: %i\" % len(w2v_words))\n",
    "print(\"Word2Vec trained\")\n",
    "\n",
    "found = sum(1 for w in tokenizer.word_index if w in w2v_model.wv)\n",
    "coverage = found / len(tokenizer.word_index)\n",
    "print(f\"Coverage: {coverage*100:.2f}%\")\n",
    "\n",
    "vectors = np.array([w2v_model.wv[w] for w in tokenizer.word_index if w in w2v_model.wv])\n",
    "print(\"Mean norm:\", np.mean(np.linalg.norm(vectors, axis=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba88b04d",
   "metadata": {},
   "source": [
    "## Création de la matrice d'embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "664fbb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (13549, 50)\n",
      "Words found in pretrained embeddings: 13547/13549 (99.99%)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, vocab_size = build_embedding_matrix(tokenizer=tokenizer,\n",
    "                                          embedding_model=model_vectors, \n",
    "                                          latent_dim=latent_dim\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0441bc8",
   "metadata": {},
   "source": [
    "## Création du modèle simple avec RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec0b1894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 10, 50)            677450    \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 64)                7360      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 684,875\n",
      "Trainable params: 7,425\n",
      "Non-trainable params: 677,450\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model =  build_base_RNN(vocab_size=vocab_size, \n",
    "                        latent_dim=latent_dim,\n",
    "                        input_length=max_len, \n",
    "                        embedding_matrix=embedding_matrix,\n",
    "                        rnn_size = 64)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326f6b4a",
   "metadata": {},
   "source": [
    "## Callbacks pour l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./Models/baselineRNN.h5\", monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-5)\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "\n",
    "callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716708dd",
   "metadata": {},
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a2eabb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    history = model.fit(X_sentence_train, y_train, epochs=50, batch_size=64, validation_data=(X_sentence_val,y_val), callbacks=callbacks_list, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d23f7",
   "metadata": {},
   "source": [
    "## Post-traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29e8dacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 3s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.73446875,\n",
       " 'F1_negatif': 0.7334107238101214,\n",
       " 'F1_positif': 0.7355184113051327,\n",
       " 'Recall_negatif': 0.7305,\n",
       " 'Recall_positif': 0.7384375,\n",
       " 'Precision_negatif': 0.7363447363447363,\n",
       " 'Precision_positif': 0.7326223104111118,\n",
       " 'ROC_AUC': 0.815024501953125}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_sentence_val)\n",
    "y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d820a8",
   "metadata": {},
   "source": [
    "## Liste des hyper-paramètres à optimiser \n",
    "\n",
    "\n",
    "- Prétraitement : \n",
    "    - Stemming ou Lemmatisation\n",
    "    - Taille du vocabulaire (num_words)\n",
    "    - Nombre minimum d'occurences (min_count)\n",
    "\n",
    "- Embedding : \n",
    "    - Word2Vec/FastText/Glove (préentrainés)\n",
    "    - Word2Vec/FastText (customisés)\n",
    "    - Dimension latente de l'embedding\n",
    "- Modèle : \n",
    "    - Couche SimpleRNN ou LSTM\n",
    "    - Dimension de la couche d'entrainement\n",
    "    - Fine-tuning ou non des embeddings ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a1bea",
   "metadata": {},
   "source": [
    "# Experimentations sur les modèles de RNN classiques "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a8bd7",
   "metadata": {},
   "source": [
    "## Préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "729e9ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WordNetLemmatizer', 'PorterStemmer', 'LancasterStemmer', 'SnowballStemmer']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Stemmer_dict = {'WordNetLemmatizer': WordNetLemmatizer().lemmatize, \n",
    "                'PorterStemmer': PorterStemmer().stem, \n",
    "                'LancasterStemmer':LancasterStemmer().stem, \n",
    "                'SnowballStemmer' : SnowballStemmer(\"english\").stem\n",
    "}\n",
    "\n",
    "list(Stemmer_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c21a8",
   "metadata": {},
   "source": [
    "## Core fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbbba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction à optimiser pour optuna\n",
    "\n",
    "def embedding_eva_pre(trial):\n",
    "    # Hyperparamètres\n",
    "    ## Prétraitement\n",
    "    min_count = trial.suggest_int('min_count',1,10)\n",
    "    num_words = trial.suggest_int('num_words',5000,50000)\n",
    "    max_len   = trial.suggest_int('max_len',2,30)\n",
    "    stemmer   = trial.suggest_categorical('stemmer',list(Stemmer_dict.keys()))\n",
    "    ## Embedding\n",
    "    latent_dim = 50\n",
    "    ## Modèle\n",
    "    rnn_size = 64\n",
    "    ## Entrainement\n",
    "    epochs = 50\n",
    "    lr = 1e-3\n",
    "    ## Savepath des poids du modèle\n",
    "    model_savepath = \"./Models/baselineRNN_pre.h5\"\n",
    "\n",
    "\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_params(params={\n",
    "            'num_words':num_words,               \n",
    "            'max_len': max_len,\n",
    "            'min_count': min_count,\n",
    "            'stemmer': stemmer, \n",
    "            'latent_dim': latent_dim, \n",
    "            'rnn_size': rnn_size, \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr \n",
    "        })\n",
    "\n",
    "        # Prétraitement\n",
    "        X_sentence_train, tokenizer, sentences_train = preprocess_data_embedding(X_raw=X_train, \n",
    "                                                        stem_lem_func=Stemmer_dict[stemmer],\n",
    "                                                        tokenizer=None, \n",
    "                                                        stop_words=stopwords.words('english'), \n",
    "                                                        min_count=min_count,\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words, \n",
    "                                                        return_sentences=True) \n",
    "        X_sentence_val = preprocess_data_embedding(X_raw=X_val, \n",
    "                                                        stem_lem_func=PorterStemmer().stem,\n",
    "                                                        tokenizer=tokenizer, \n",
    "                                                        stop_words=stopwords.words('english'), \n",
    "                                                        min_count=1, # mincount = 1 car on est sur le jeu de validation\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words) \n",
    "        # Embedding(custom)\n",
    "        \n",
    "        w2v_model = Word2Vec(\n",
    "            sentences=sentences_train, \n",
    "            vector_size=latent_dim,  # dimension de l’espace latent\n",
    "            window=5,         # taille du contexte\n",
    "            min_count=min_count,      # ignorer les mots trop rares\n",
    "            workers=4,        # parallélisme CPU\n",
    "            sg=0,              # 1 = skip-gram, 0 = CBOW\n",
    "            epochs=50\n",
    "            )\n",
    "\n",
    "\n",
    "        model_vectors = w2v_model.wv\n",
    "        w2v_words = model_vectors.index_to_key\n",
    "\n",
    "        embedding_matrix, vocab_size = build_embedding_matrix(tokenizer=tokenizer,\n",
    "                                          embedding_model=model_vectors, \n",
    "                                          latent_dim=latent_dim\n",
    "                                          )\n",
    "\n",
    "        \n",
    "        # Modèle\n",
    "        model =  build_base_RNN(vocab_size=vocab_size, \n",
    "                        latent_dim=latent_dim,\n",
    "                        input_length=max_len, \n",
    "                        embedding_matrix=embedding_matrix,\n",
    "                        rnn_size = rnn_size)\n",
    "        ## Callbacks\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "        ## Compilation\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "        #Entrainement\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(X_sentence_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_sentence_val,y_val), callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "        # Prédictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(X_sentence_val)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des métriques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        acc = output_dict[\"Accuracy\"]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded01f83",
   "metadata": {},
   "source": [
    "## Définition de l'experiment MLFlow/Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cad89439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization with Optuna...\n",
      "Setting up MLflow experiment...\n"
     ]
    }
   ],
   "source": [
    "# Création de l'étude Optuna et optimisation\n",
    "print(\"Starting hyperparameter optimization with Optuna...\")\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"optuna_word_embedding_experiment_preprocessin\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"optuna_word_embedding_experiment_preprocessin\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Cette experience contient les différents tests pour le modèle RNN simple. \"\n",
    "    \"Ici on évalue simplement l'impact des différents prétraitements sur un modèle avec simpleRNN\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"simple-RNN-preprocessing\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afb2165",
   "metadata": {},
   "source": [
    "## Lancement de l'optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancement de l'optimisation avec Optuna\n",
    "print(\"Starting optimization trials...\")\n",
    "with mlflow.start_run(run_name=\"optuna_word_embedding_experiment_preprocessin\"):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(embedding_eva_pre, n_trials=50)\n",
    "\n",
    "    mlflow.log_params(study.best_params)\n",
    "    mlflow.log_metric(\"best_accuracy\", study.best_value)\n",
    "\n",
    "print(\"Optimization completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e156241",
   "metadata": {},
   "source": [
    "## Extraction meilleur modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea7a2618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run ID: d9f27d7060494334af49d01586fdb855 with metrics:\n",
      "Accuracy: 0.75290625\n",
      "F1_negatif: 0.7471135702178016\n",
      "F1_positif: 0.7584394953105429\n",
      "Precision_negatif: 0.7650487980611776\n",
      "Precision_positif: 0.7418275264447499\n",
      "Recall_negatif: 0.73\n",
      "Recall_positif: 0.7758125\n",
      "ROC_AUC: 0.83355388671875\n",
      "Best run parameters:\n",
      "epochs: 50\n",
      "latent_dim: 50\n",
      "learning_rate: 0.001\n",
      "max_len: 27\n",
      "min_count: 1\n",
      "num_words: 13006\n",
      "rnn_size: 64\n",
      "stemmer: PorterStemmer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'simple_rnn_best_pre'.\n",
      "2025/09/14 16:01:51 WARNING mlflow.tracking._model_registry.fluent: Run with id d9f27d7060494334af49d01586fdb855 has no artifacts at artifact path 'model', registering model based on models:/m-0e60288d816b42cb87f955b8ad2cd669 instead\n",
      "Created version '1' of model 'simple_rnn_best_pre'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting tag epochs = 50 in registered model\n",
      "Setting tag latent_dim = 50 in registered model\n",
      "Setting tag learning_rate = 0.001 in registered model\n",
      "Setting tag max_len = 27 in registered model\n",
      "Setting tag min_count = 1 in registered model\n",
      "Setting tag num_words = 13006 in registered model\n",
      "Setting tag rnn_size = 64 in registered model\n",
      "Setting tag stemmer = PorterStemmer in registered model\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")\n",
    "experiment_id = mlflow.get_experiment_by_name(\"optuna_word_embedding_experiment_preprocessin\").experiment_id\n",
    "runs = client.search_runs(experiment_id)\n",
    "\n",
    "# Métrique pour sélectionner le meilleur modèle\n",
    "metric_to_optimize = \"Accuracy\" # liste des métriques enregistrées dans postprocess_data.py ou sur l'UI MLflow\n",
    "best_run = max(runs, key=lambda run: run.data.metrics.get(metric_to_optimize, float('-inf')))\n",
    "print(f\"Best run ID: {best_run.info.run_id} with metrics:\")\n",
    "for key, value in best_run.data.metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Best run parameters:\")\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Enregistrement du meilleur modèle\n",
    "best_model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "registered_model_name = \"simple_rnn_best_pre\"\n",
    "registered_model = mlflow.register_model(best_model_uri, registered_model_name)\n",
    "# Enregistrement des paramètres sous forme de tags dans le modèle enregistré\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"Setting tag {key} = {value} in registered model\")\n",
    "    client.set_model_version_tag(\n",
    "        name=registered_model_name,\n",
    "        version=str(registered_model.version),\n",
    "        key=str(key),\n",
    "        value=str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a882a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d4aeae5",
   "metadata": {},
   "source": [
    "# Experimentation sur les embeddings (custom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b0f02",
   "metadata": {},
   "source": [
    "## Préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58932e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText, Word2Vec\n",
    "\n",
    "embedding_dict = {'Word2Vec':Word2Vec, \n",
    "                  'FastText':FastText}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca2b238",
   "metadata": {},
   "source": [
    "## Core fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86f78a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction à optimiser pour optuna\n",
    "\n",
    "## Prétraitement\n",
    "min_count = 5\n",
    "num_words = 20000\n",
    "max_len   = 30\n",
    "stemmer   = 'PorterStemmer'\n",
    "# Prétraitement\n",
    "X_sentence_train, tokenizer, sentences_train = preprocess_data_embedding(X_raw=X_train, \n",
    "                                                        stem_lem_func=PorterStemmer().stem,\n",
    "                                                        tokenizer=None, \n",
    "                                                        stop_words=stopwords.words('english'), \n",
    "                                                        min_count=min_count,\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words, \n",
    "                                                        return_sentences=True) \n",
    "X_sentence_val = preprocess_data_embedding(X_raw=X_val, \n",
    "                                                        stem_lem_func=PorterStemmer().stem,\n",
    "                                                        tokenizer=tokenizer, \n",
    "                                                        stop_words=stopwords.words('english'), \n",
    "                                                        min_count=1, # mincount = 1 car on est sur le jeu de validation\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words) \n",
    "\n",
    "def embedding_eval_custom_embed(trial):\n",
    "    # Hyperparamètres\n",
    "\n",
    "    ## Embedding\n",
    "    embedding_model = trial.suggest_categorical('embedding_model',list(embedding_dict.keys()))\n",
    "    latent_dim = trial.suggest_int('latent_dim', 30, 150)\n",
    "    window = trial.suggest_int(\"window\", 2, 10)\n",
    "    sg = trial.suggest_int('sg',0,1)\n",
    "\n",
    "    ## Modèle\n",
    "    rnn_size = 64\n",
    "    ## Entrainement\n",
    "    epochs = 50\n",
    "    lr = 1e-3\n",
    "    ## Savepath des poids du modèle\n",
    "    model_savepath = \"./Models/baselineRNN_pre.h5\"\n",
    "\n",
    "\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_params(params={\n",
    "            'num_words':num_words,               \n",
    "            'max_len': max_len,\n",
    "            'min_count': min_count,\n",
    "            'stemmer': stemmer, \n",
    "            'latent_dim': latent_dim, \n",
    "            'rnn_size': rnn_size, \n",
    "            'embedding_model':embedding_model,\n",
    "            'sg':sg,\n",
    "            'window':window,\n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr \n",
    "        })\n",
    "\n",
    "\n",
    "        # Embedding(custom)\n",
    "        if embedding_model=='Word2Vec':\n",
    "            embedding_model = Word2Vec(\n",
    "                sentences=sentences_train, \n",
    "                vector_size=latent_dim,  # dimension de l’espace latent\n",
    "                window=5,         # taille du contexte\n",
    "                min_count=min_count,      # ignorer les mots trop rares\n",
    "            workers=4,        # parallélisme CPU\n",
    "            sg=sg,              # 1 = skip-gram, 0 = CBOW\n",
    "            epochs=30\n",
    "            )\n",
    "        elif embedding_model=='FastText':\n",
    "            embedding_model = FastText(\n",
    "                sentences=sentences_train, \n",
    "                vector_size=latent_dim, \n",
    "                window=5, \n",
    "                min_count=min_count,\n",
    "                workers=4,\n",
    "                sg=sg,\n",
    "                epochs=30\n",
    "                )\n",
    "\n",
    "\n",
    "        model_vectors = embedding_model.wv\n",
    "        w2v_words = model_vectors.index_to_key\n",
    "\n",
    "        embedding_matrix, vocab_size = build_embedding_matrix(tokenizer=tokenizer,\n",
    "                                          embedding_model=model_vectors, \n",
    "                                          latent_dim=latent_dim\n",
    "                                          )\n",
    "\n",
    "        \n",
    "        # Modèle\n",
    "        model =  build_base_RNN(vocab_size=vocab_size, \n",
    "                        latent_dim=latent_dim,\n",
    "                        input_length=max_len, \n",
    "                        embedding_matrix=embedding_matrix,\n",
    "                        rnn_size = rnn_size)\n",
    "        ## Callbacks\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "        ## Compilation\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "        #Entrainement\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(X_sentence_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_sentence_val,y_val), callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "        # Prédictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(X_sentence_val)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des métriques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        acc = output_dict[\"Accuracy\"]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095303da",
   "metadata": {},
   "source": [
    "## Definition de l'experiment MLFlow/Optuna\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6fdcac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization with Optuna...\n",
      "Setting up MLflow experiment...\n"
     ]
    }
   ],
   "source": [
    "# Création de l'étude Optuna et optimisation\n",
    "print(\"Starting hyperparameter optimization with Optuna...\")\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"optuna_word_embedding_experiment_custom_embedding\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"optuna_word_embedding_experiment_custom_embedding\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Cette experience contient les différents tests pour le modèle RNN simple. \"\n",
    "    \"Ici on évalue l'impact du type d'embedding custom et de la dimension de l'espace latent sur un modèle avec simpleRNN\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"simple-RNN-preprocessing\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efe00de",
   "metadata": {},
   "source": [
    "## Lancement de l'optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91597411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/14 23:15:35 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://b70c20a6-0687-4b41-a51b-c3b2c379e863/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://b70c20a6-0687-4b41-a51b-c3b2c379e863/assets\n",
      "2025/09/14 23:15:42 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpkjp5weh8\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/14 23:15:42 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-14 23:15:42,662] Trial 20 finished with value: 0.75128125 and parameters: {'embedding_model': 'FastText', 'latent_dim': 105, 'window': 4, 'sg': 0}. Best is trial 20 with value: 0.75128125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9539, 107)\n",
      "Words found in pretrained embeddings: 9538/9539 (99.99%)\n",
      "1000/1000 [==============================] - 7s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/14 23:31:07 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://90e3f848-a0c6-4c46-ac94-1d010557c7c6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://90e3f848-a0c6-4c46-ac94-1d010557c7c6/assets\n",
      "2025/09/14 23:31:15 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpy8fltxia\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/14 23:31:15 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-14 23:31:15,189] Trial 21 finished with value: 0.748125 and parameters: {'embedding_model': 'FastText', 'latent_dim': 107, 'window': 4, 'sg': 0}. Best is trial 20 with value: 0.75128125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9539, 128)\n",
      "Words found in pretrained embeddings: 9538/9539 (99.99%)\n",
      "1000/1000 [==============================] - 7s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/14 23:49:30 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f3b62c82-063e-404e-b5e3-5921370a300b/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f3b62c82-063e-404e-b5e3-5921370a300b/assets\n",
      "2025/09/14 23:49:37 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpib947_97\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/14 23:49:37 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-14 23:49:37,758] Trial 22 finished with value: 0.7520625 and parameters: {'embedding_model': 'FastText', 'latent_dim': 128, 'window': 3, 'sg': 0}. Best is trial 22 with value: 0.7520625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9539, 87)\n",
      "Words found in pretrained embeddings: 9538/9539 (99.99%)\n",
      "1000/1000 [==============================] - 7s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 00:09:29 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://5de7e54b-cc75-4177-876e-aeeeb2b61353/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://5de7e54b-cc75-4177-876e-aeeeb2b61353/assets\n",
      "2025/09/15 00:09:36 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpikdd_sqz\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/15 00:09:36 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-15 00:09:37,160] Trial 23 finished with value: 0.75309375 and parameters: {'embedding_model': 'FastText', 'latent_dim': 87, 'window': 5, 'sg': 0}. Best is trial 23 with value: 0.75309375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9539, 82)\n",
      "Words found in pretrained embeddings: 9538/9539 (99.99%)\n",
      "1000/1000 [==============================] - 6s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 00:34:36 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f1fbedb6-6c7c-4daf-a013-69dda45d9a38/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f1fbedb6-6c7c-4daf-a013-69dda45d9a38/assets\n",
      "2025/09/15 00:34:43 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpvlz2sjq6\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/15 00:34:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-15 00:34:43,999] Trial 24 finished with value: 0.74728125 and parameters: {'embedding_model': 'FastText', 'latent_dim': 82, 'window': 5, 'sg': 0}. Best is trial 23 with value: 0.75309375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9539, 68)\n",
      "Words found in pretrained embeddings: 9538/9539 (99.99%)\n",
      "1000/1000 [==============================] - 4s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 00:52:39 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://0e331421-f9d2-4161-b6ad-03bfa41c4815/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://0e331421-f9d2-4161-b6ad-03bfa41c4815/assets\n",
      "2025/09/15 00:52:46 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpm4itqpjy\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/15 00:52:46 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-15 00:52:46,131] Trial 25 finished with value: 0.746 and parameters: {'embedding_model': 'FastText', 'latent_dim': 68, 'window': 6, 'sg': 0}. Best is trial 23 with value: 0.75309375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9539, 104)\n",
      "Words found in pretrained embeddings: 9537/9539 (99.98%)\n",
      "1000/1000 [==============================] - 4s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 00:59:33 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://55c7d27b-03bd-4a85-8351-997dc83de75e/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://55c7d27b-03bd-4a85-8351-997dc83de75e/assets\n",
      "2025/09/15 00:59:40 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmphe28srfq\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/15 00:59:40 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-15 00:59:40,338] Trial 26 finished with value: 0.7395 and parameters: {'embedding_model': 'Word2Vec', 'latent_dim': 104, 'window': 7, 'sg': 0}. Best is trial 23 with value: 0.75309375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9539, 97)\n",
      "Words found in pretrained embeddings: 9538/9539 (99.99%)\n",
      "1000/1000 [==============================] - 7s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 01:14:44 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://883d2e02-1fbc-480a-babd-3e54894ecd9e/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://883d2e02-1fbc-480a-babd-3e54894ecd9e/assets\n",
      "2025/09/15 01:14:51 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpr821o2rh\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/15 01:14:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-15 01:14:51,913] Trial 27 finished with value: 0.750875 and parameters: {'embedding_model': 'FastText', 'latent_dim': 97, 'window': 4, 'sg': 0}. Best is trial 23 with value: 0.75309375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9539, 87)\n",
      "Words found in pretrained embeddings: 9538/9539 (99.99%)\n",
      "1000/1000 [==============================] - 6s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 01:27:35 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://16f2a0f0-884f-4643-b782-ce6d806cb19a/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://16f2a0f0-884f-4643-b782-ce6d806cb19a/assets\n",
      "2025/09/15 01:27:42 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpz010wyok\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/15 01:27:42 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-15 01:27:43,015] Trial 28 finished with value: 0.6356875 and parameters: {'embedding_model': 'FastText', 'latent_dim': 87, 'window': 5, 'sg': 0}. Best is trial 23 with value: 0.75309375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9539, 112)\n",
      "Words found in pretrained embeddings: 9537/9539 (99.98%)\n",
      "1000/1000 [==============================] - 7s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 01:45:46 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://6f8ba309-1b71-443b-8ca1-0ff6e75d953a/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://6f8ba309-1b71-443b-8ca1-0ff6e75d953a/assets\n",
      "2025/09/15 01:45:54 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp902n5hpw\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/09/15 01:45:54 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "[I 2025-09-15 01:45:54,860] Trial 29 finished with value: 0.751 and parameters: {'embedding_model': 'Word2Vec', 'latent_dim': 112, 'window': 7, 'sg': 0}. Best is trial 23 with value: 0.75309375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization completed.\n"
     ]
    }
   ],
   "source": [
    "# Lancement de l'optimisation avec Optuna\n",
    "print(\"Starting optimization trials...\")\n",
    "with mlflow.start_run(run_name=\"optuna_word_embedding_experiment_custom_embedding\"):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(embedding_eval_custom_embed, n_trials=30)\n",
    "    mlflow.log_params(study.best_params)\n",
    "    mlflow.log_metric(\"best_accuracy\", study.best_value)\n",
    "\n",
    "print(\"Optimization completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78809adc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffd878eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Successfully registered model 'simple_rnn_best_custom_embed'.\n",
      "2025/09/15 01:52:09 WARNING mlflow.tracking._model_registry.fluent: Run with id 4461c64ebbfd43c884ba93c17bb3838a has no artifacts at artifact path 'model', registering model based on models:/m-93f5ba0e17c84bb6b7c4c24b0acc506d instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run ID: 4461c64ebbfd43c884ba93c17bb3838a with metrics:\n",
      "Accuracy: 0.75309375\n",
      "F1_negatif: 0.7489594255393512\n",
      "F1_positif: 0.7570941064346542\n",
      "Precision_negatif: 0.7617139533380728\n",
      "Precision_positif: 0.7450232952138924\n",
      "Recall_negatif: 0.736625\n",
      "Recall_positif: 0.7695625\n",
      "ROC_AUC: 0.8303465761718751\n",
      "Best run parameters:\n",
      "embedding_model: FastText\n",
      "epochs: 50\n",
      "latent_dim: 87\n",
      "learning_rate: 0.001\n",
      "max_len: 30\n",
      "min_count: 5\n",
      "num_words: 20000\n",
      "rnn_size: 64\n",
      "sg: 0\n",
      "stemmer: PorterStemmer\n",
      "window: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'simple_rnn_best_custom_embed'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting tag embedding_model = FastText in registered model\n",
      "Setting tag epochs = 50 in registered model\n",
      "Setting tag latent_dim = 87 in registered model\n",
      "Setting tag learning_rate = 0.001 in registered model\n",
      "Setting tag max_len = 30 in registered model\n",
      "Setting tag min_count = 5 in registered model\n",
      "Setting tag num_words = 20000 in registered model\n",
      "Setting tag rnn_size = 64 in registered model\n",
      "Setting tag sg = 0 in registered model\n",
      "Setting tag stemmer = PorterStemmer in registered model\n",
      "Setting tag window = 5 in registered model\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")\n",
    "experiment_id = mlflow.get_experiment_by_name(\"optuna_word_embedding_experiment_custom_embedding\").experiment_id\n",
    "runs = client.search_runs(experiment_id)\n",
    "\n",
    "# Métrique pour sélectionner le meilleur modèle\n",
    "metric_to_optimize = \"Accuracy\" # liste des métriques enregistrées dans postprocess_data.py ou sur l'UI MLflow\n",
    "best_run = max(runs, key=lambda run: run.data.metrics.get(metric_to_optimize, float('-inf')))\n",
    "print(f\"Best run ID: {best_run.info.run_id} with metrics:\")\n",
    "for key, value in best_run.data.metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Best run parameters:\")\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Enregistrement du meilleur modèle\n",
    "best_model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "registered_model_name = \"simple_rnn_best_custom_embed\"\n",
    "registered_model = mlflow.register_model(best_model_uri, registered_model_name)\n",
    "# Enregistrement des paramètres sous forme de tags dans le modèle enregistré\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"Setting tag {key} = {value} in registered model\")\n",
    "    client.set_model_version_tag(\n",
    "        name=registered_model_name,\n",
    "        version=str(registered_model.version),\n",
    "        key=str(key),\n",
    "        value=str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b7c999",
   "metadata": {},
   "source": [
    "# Experimentation sur les embeddings (préentrainés)\n",
    "\n",
    "Les embeddings pré-entrainés ont été entrainés sur un très grand nombre de tweets et prennent donc en compte un très grand nombre de situations. On peut donc se passer de la phase de stemming et garder les stopwords. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a8c3c",
   "metadata": {},
   "source": [
    "## Préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f259b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# Word2Vec Google News\n",
    "w2v_google = api.load(\"word2vec-google-news-300\")  # KeyedVectors\n",
    "\n",
    "# FastText wiki-news subwords-300\n",
    "ft_wiki = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "\n",
    "# GloVe Twitter (par exemple 200 dimensions)\n",
    "glove_tw200 = api.load(\"glove-twitter-200\")\n",
    "glove_tw100 = api.load(\"glove-twitter-100\")\n",
    "glove_tw50  = api.load(\"glove-twitter-50\")\n",
    "glove_tw25  = api.load(\"glove-twitter-25\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b959004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {'Word2Vec_google':w2v_google, \n",
    "                  'FastText_wiki':ft_wiki,\n",
    "                  'Glove_twitter_200':glove_tw200, \n",
    "                  'Glove_twitter_100':glove_tw100, \n",
    "                  'Glove_twitter_50':glove_tw50, \n",
    "                  'Glove_twitter_25':glove_tw25\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6540f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prétraitement\n",
    "min_count = 2\n",
    "num_words = 20000\n",
    "max_len   = 30\n",
    "# Prétraitement\n",
    "X_sentence_train, tokenizer, sentences_train = preprocess_data_embedding(X_raw=X_train, \n",
    "                                                        stem_lem_func=None,\n",
    "                                                        tokenizer=None, \n",
    "                                                        stop_words=None, \n",
    "                                                        min_count=min_count,\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words, \n",
    "                                                        return_sentences=True) \n",
    "X_sentence_val = preprocess_data_embedding(X_raw=X_val, \n",
    "                                                        stem_lem_func=None,\n",
    "                                                        tokenizer=tokenizer, \n",
    "                                                        stop_words=None, \n",
    "                                                        min_count=1, # mincount = 1 car on est sur le jeu de validation\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab8767c",
   "metadata": {},
   "source": [
    "## Fonction de base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6212a43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamètres\n",
    "\n",
    "## Modèle\n",
    "rnn_size = 64\n",
    "## Entrainement\n",
    "epochs = 50\n",
    "lr = 1e-3\n",
    "## Savepath des poids du modèle\n",
    "model_savepath = \"./Models/baselineRNN_pretrained_embed.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d87b166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embed_experiment(embedding_name):\n",
    "     with mlflow.start_run():\n",
    "        latent_dim = embedding_dict[embedding_name].vector_size\n",
    "        mlflow.log_params(params={\n",
    "            'num_words':num_words,               \n",
    "            'max_len': max_len,\n",
    "            'min_count': min_count,\n",
    "            'stemmer': 'None', \n",
    "            'latent_dim': latent_dim, \n",
    "            'rnn_size': rnn_size, \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr,\n",
    "            'embedding_name':embedding_name \n",
    "        })\n",
    "\n",
    "\n",
    "        model_vectors = embedding_dict[embedding_name]\n",
    "\n",
    "        embedding_matrix, vocab_size = build_embedding_matrix(tokenizer=tokenizer,\n",
    "                                          embedding_model=model_vectors, \n",
    "                                          latent_dim=latent_dim\n",
    "                                          )\n",
    "\n",
    "        # Modèle\n",
    "        model =  build_base_RNN(vocab_size=vocab_size, \n",
    "                        latent_dim=latent_dim,\n",
    "                        input_length=max_len, \n",
    "                        embedding_matrix=embedding_matrix,\n",
    "                        rnn_size = rnn_size)\n",
    "        ## Callbacks\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "        ## Compilation\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "        #Entrainement\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(X_sentence_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_sentence_val,y_val), callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "                # Prédictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(X_sentence_val)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des métriques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d50b14",
   "metadata": {},
   "source": [
    "## Définition de l'experiment dans MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "079ae708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "2025/09/15 02:48:45 INFO mlflow.tracking.fluent: Experiment with name 'word_embedding_experiment_pretrained_embedding' does not exist. Creating a new experiment.\n",
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization with Optuna...\n",
      "Setting up MLflow experiment...\n"
     ]
    }
   ],
   "source": [
    "# Création de l'étude Optuna et optimisation\n",
    "print(\"Starting hyperparameter optimization with Optuna...\")\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"word_embedding_experiment_pretrained_embedding\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"word_embedding_experiment_pretrained_embedding\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Cette experience contient les différents tests pour le modèle RNN simple. \"\n",
    "    \"Ici on teste plusieurs embeddings préentrainées sur de larges corpora, beaucoup d'attentes par rapport aux embeddings avec Glove entrainés sur des tweets\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"simple-RNN-pretrained-embeddings\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0697ce9",
   "metadata": {},
   "source": [
    "## Lancement de l'experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de3a8135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with Word2Vec_google\n",
      "Embedding matrix shape: (24711, 300)\n",
      "Words found in pretrained embeddings: 20292/24711 (82.12%)\n",
      "1000/1000 [==============================] - 7s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 03:02:53 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 03:02:53 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpft6gpi5_\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpft6gpi5_\\model\\data\\model\\assets\n",
      "2025/09/15 03:03:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with FastText_wiki\n",
      "Embedding matrix shape: (24711, 300)\n",
      "Words found in pretrained embeddings: 21410/24711 (86.64%)\n",
      "1000/1000 [==============================] - 5s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 03:17:56 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 03:17:56 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpg7xm_dcc\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpg7xm_dcc\\model\\data\\model\\assets\n",
      "2025/09/15 03:18:14 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with Glove_twitter_200\n",
      "Embedding matrix shape: (24711, 200)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n",
      "1000/1000 [==============================] - 5s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 03:28:50 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 03:28:50 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp3_kfzaqj\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp3_kfzaqj\\model\\data\\model\\assets\n",
      "2025/09/15 03:28:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with Glove_twitter_100\n",
      "Embedding matrix shape: (24711, 100)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n",
      "1000/1000 [==============================] - 5s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 03:39:24 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 03:39:24 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpbuatvcxy\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpbuatvcxy\\model\\data\\model\\assets\n",
      "2025/09/15 03:39:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with Glove_twitter_50\n",
      "Embedding matrix shape: (24711, 50)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n",
      "1000/1000 [==============================] - 5s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 03:56:37 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 03:56:38 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpkkv9oxpy\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpkkv9oxpy\\model\\data\\model\\assets\n",
      "2025/09/15 03:56:46 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with Glove_twitter_25\n",
      "Embedding matrix shape: (24711, 25)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n",
      "1000/1000 [==============================] - 5s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 04:14:15 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 04:14:15 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpwkeji3ag\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpwkeji3ag\\model\\data\\model\\assets\n",
      "2025/09/15 04:14:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "for embedding_name in list(embedding_dict.keys()):\n",
    "    \n",
    "    print(f\"Running test with {embedding_name}\")\n",
    "    pretrained_embed_experiment(embedding_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa14dca",
   "metadata": {},
   "source": [
    "## Enregistrement du meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "abb9d454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Registered model 'simple_rnn_best_pre' already exists. Creating a new version of this model...\n",
      "2025/09/15 14:33:57 WARNING mlflow.tracking._model_registry.fluent: Run with id ce06b66ec4ff48e6a994d2485c63701f has no artifacts at artifact path 'model', registering model based on models:/m-6b8da83843cc4061a7b1edb36b9eb5f4 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run ID: ce06b66ec4ff48e6a994d2485c63701f with metrics:\n",
      "Accuracy: 0.795875\n",
      "F1_negatif: 0.7963459499906467\n",
      "F1_positif: 0.7954018668170143\n",
      "Precision_negatif: 0.7945128779395296\n",
      "Precision_positif: 0.7972497802335803\n",
      "Recall_negatif: 0.7981875\n",
      "Recall_positif: 0.7935625\n",
      "ROC_AUC: 0.874631751953125\n",
      "Best run parameters:\n",
      "embedding_name: Glove_twitter_200\n",
      "epochs: 50\n",
      "latent_dim: 200\n",
      "learning_rate: 0.001\n",
      "max_len: 30\n",
      "min_count: 2\n",
      "num_words: 20000\n",
      "rnn_size: 64\n",
      "stemmer: None\n",
      "Setting tag embedding_name = Glove_twitter_200 in registered model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'simple_rnn_best_pre'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting tag epochs = 50 in registered model\n",
      "Setting tag latent_dim = 200 in registered model\n",
      "Setting tag learning_rate = 0.001 in registered model\n",
      "Setting tag max_len = 30 in registered model\n",
      "Setting tag min_count = 2 in registered model\n",
      "Setting tag num_words = 20000 in registered model\n",
      "Setting tag rnn_size = 64 in registered model\n",
      "Setting tag stemmer = None in registered model\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")\n",
    "experiment_id = mlflow.get_experiment_by_name(\"word_embedding_experiment_pretrained_embedding\").experiment_id\n",
    "runs = client.search_runs(experiment_id)\n",
    "\n",
    "# Métrique pour sélectionner le meilleur modèle\n",
    "metric_to_optimize = \"Accuracy\" # liste des métriques enregistrées dans postprocess_data.py ou sur l'UI MLflow\n",
    "best_run = max(runs, key=lambda run: run.data.metrics.get(metric_to_optimize, float('-inf')))\n",
    "print(f\"Best run ID: {best_run.info.run_id} with metrics:\")\n",
    "for key, value in best_run.data.metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Best run parameters:\")\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Enregistrement du meilleur modèle\n",
    "best_model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "registered_model_name = \"simple_rnn_best_pre\"\n",
    "registered_model = mlflow.register_model(best_model_uri, registered_model_name)\n",
    "# Enregistrement des paramètres sous forme de tags dans le modèle enregistré\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"Setting tag {key} = {value} in registered model\")\n",
    "    client.set_model_version_tag(\n",
    "        name=registered_model_name,\n",
    "        version=str(registered_model.version),\n",
    "        key=str(key),\n",
    "        value=str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579521f6",
   "metadata": {},
   "source": [
    "# Comparaison des différentes couches de notre réseau de neurones : SimpleRNN vs GRU vs LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5966ad3",
   "metadata": {},
   "source": [
    "## Préparation \n",
    "\n",
    "On reprend les paramètres d'embedding de la meilleure run sur embeddings customs et embeddings préentrainés. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45154a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (24711, 200)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n"
     ]
    }
   ],
   "source": [
    "## Prétraitement\n",
    "min_count = 2\n",
    "num_words = 20000\n",
    "max_len   = 30\n",
    "# Prétraitement\n",
    "X_sentence_train, tokenizer, sentences_train = preprocess_data_embedding(X_raw=X_train, \n",
    "                                                        stem_lem_func=None,\n",
    "                                                        tokenizer=None, \n",
    "                                                        stop_words=None, \n",
    "                                                        min_count=min_count,\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words, \n",
    "                                                        return_sentences=True) \n",
    "X_sentence_val = preprocess_data_embedding(X_raw=X_val, \n",
    "                                                        stem_lem_func=None,\n",
    "                                                        tokenizer=tokenizer, \n",
    "                                                        stop_words=None, \n",
    "                                                        min_count=1, # mincount = 1 car on est sur le jeu de validation\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words) \n",
    "\n",
    "\n",
    "model_vectors = glove_tw200\n",
    "latent_dim = glove_tw200.vector_size\n",
    "\n",
    "embedding_matrix, vocab_size = build_embedding_matrix(tokenizer=tokenizer,\n",
    "                                embedding_model=model_vectors, \n",
    "                                latent_dim=latent_dim\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80933f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Source.preprocess_data import *\n",
    "rnn_layer_name_list = ['SimpleRNN','GRU','LSTM']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a066a0f",
   "metadata": {},
   "source": [
    "## Fonction de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "765bce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modèle\n",
    "rnn_size = 64\n",
    "## Entrainement\n",
    "epochs = 50\n",
    "lr = 1e-3\n",
    "## Savepath des poids du modèle\n",
    "\n",
    "def rnn_layer_experiment(rnn_layer_name):\n",
    "     with mlflow.start_run():\n",
    "        mlflow.log_params(params={\n",
    "            'num_words':num_words,               \n",
    "            'max_len': max_len,\n",
    "            'min_count': min_count,\n",
    "            'stemmer': 'None', \n",
    "            'latent_dim': latent_dim, \n",
    "            'rnn_size': rnn_size, \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr,\n",
    "            'embedding_name':'Glove_twitter_200',\n",
    "            'rnn_layer_name':rnn_layer_name \n",
    "        })\n",
    "        model_savepath = \"./Models/\"+rnn_layer_name+\"_model_exp.h5\"\n",
    "        # Modèle\n",
    "        if rnn_layer_name == 'SimpleRNN':\n",
    "            model =  build_base_RNN(vocab_size=vocab_size, \n",
    "                            latent_dim=latent_dim,\n",
    "                            input_length=max_len, \n",
    "                            embedding_matrix=embedding_matrix,\n",
    "                            rnn_size = rnn_size)\n",
    "        elif rnn_layer_name == 'GRU':\n",
    "            model = build_gru_RNN(vocab_size=vocab_size, \n",
    "                            latent_dim=latent_dim,\n",
    "                            input_length=max_len, \n",
    "                            embedding_matrix=embedding_matrix,\n",
    "                            rnn_size = rnn_size)\n",
    "        elif rnn_layer_name=='LSTM':\n",
    "            model = build_lstm_RNN(vocab_size=vocab_size, \n",
    "                            latent_dim=latent_dim,\n",
    "                            input_length=max_len, \n",
    "                            embedding_matrix=embedding_matrix,\n",
    "                            rnn_size = rnn_size)\n",
    "        ## Callbacks\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "        ## Compilation\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "        #Entrainement\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(X_sentence_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_sentence_val,y_val), callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "                # Prédictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(X_sentence_val)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des métriques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdd9607",
   "metadata": {},
   "source": [
    "## Definition de l'experiment dans MLFLow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a9394a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MLflow experiment...\n"
     ]
    }
   ],
   "source": [
    "# Création de l'étude Optuna et optimisation\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"rnn_layer_experiment_pretrained_embedding\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"rnn_layer_experiment_pretrained_embedding\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Comparaison des impact des types de cellules RNN utilisées : SimpleRNN, GRU et LSTM \"\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"RNN_types-pretrained-embeddings\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362e75c9",
   "metadata": {},
   "source": [
    "## Lancement de l'expériment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af58cb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with SimpleRNN\n",
      "1000/1000 [==============================] - 8s 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 20:51:51 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 20:51:51 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpzg36l18g\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpzg36l18g\\model\\data\\model\\assets\n",
      "2025/09/15 20:52:03 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with GRU\n",
      "1000/1000 [==============================] - 5s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 21:00:15 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 21:00:15 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpmp__baop\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpmp__baop\\model\\data\\model\\assets\n",
      "2025/09/15 21:00:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with LSTM\n",
      "1000/1000 [==============================] - 6s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 21:09:40 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 21:09:40 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmps1u9tisp\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmps1u9tisp\\model\\data\\model\\assets\n",
      "2025/09/15 21:09:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "for rnn_layer_name in rnn_layer_name_list:\n",
    "    \n",
    "    print(f\"Running test with {rnn_layer_name}\")\n",
    "    rnn_layer_experiment(rnn_layer_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f7e94",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb8369b0",
   "metadata": {},
   "source": [
    "# Retour sur la longueur des séquences     \n",
    "\n",
    "Ici on va revenir sur la longueur des séquences utilisées car LSTM permet de garder des séquences plus longues sans pour autant avoir d'évanescence de gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb59f6",
   "metadata": {},
   "source": [
    "## Fonction de base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c298ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Modèle\n",
    "rnn_size = 64\n",
    "## Entrainement\n",
    "epochs = 50\n",
    "lr = 1e-3\n",
    "## Savepath des poids du modèle\n",
    "\n",
    "def lstm_maxlen_experiment(max_len):\n",
    "     with mlflow.start_run():\n",
    "        latent_dim = glove_tw200.vector_size\n",
    "\n",
    "        mlflow.log_params(params={\n",
    "            'num_words':num_words,               \n",
    "            'max_len': max_len,\n",
    "            'min_count': min_count,\n",
    "            'stemmer': 'None', \n",
    "            'latent_dim': latent_dim, \n",
    "            'rnn_size': rnn_size, \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr,\n",
    "            'embedding_name':'Glove_twitter_200',\n",
    "            'rnn_layer_name':'LSTM' \n",
    "        })\n",
    "\n",
    "        ## Prétraitement\n",
    "\n",
    "        # Prétraitement\n",
    "        X_sentence_train, tokenizer, sentences_train = preprocess_data_embedding(X_raw=X_train, \n",
    "                                                        stem_lem_func=None,\n",
    "                                                        tokenizer=None, \n",
    "                                                        stop_words=None, \n",
    "                                                        min_count=min_count,\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words, \n",
    "                                                        return_sentences=True) \n",
    "        X_sentence_val = preprocess_data_embedding(X_raw=X_val, \n",
    "                                                        stem_lem_func=None,\n",
    "                                                        tokenizer=tokenizer, \n",
    "                                                        stop_words=None, \n",
    "                                                        min_count=1, # mincount = 1 car on est sur le jeu de validation\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words) \n",
    "\n",
    "\n",
    "        model_vectors = glove_tw200\n",
    "\n",
    "        embedding_matrix, vocab_size = build_embedding_matrix(tokenizer=tokenizer,\n",
    "                                embedding_model=model_vectors, \n",
    "                                latent_dim=latent_dim\n",
    "                              )\n",
    "        model_savepath = f\"./Models/{rnn_layer_name}_model_exp_len{max_len}.h5\"\n",
    "        # Modèle\n",
    "\n",
    "        model = build_lstm_RNN(vocab_size=vocab_size, \n",
    "                            latent_dim=latent_dim,\n",
    "                            input_length=max_len, \n",
    "                            embedding_matrix=embedding_matrix,\n",
    "                            rnn_size = rnn_size)\n",
    "        ## Callbacks\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "        ## Compilation\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "        #Entrainement\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(X_sentence_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_sentence_val,y_val), callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "                # Prédictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(X_sentence_val)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des métriques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5404a5",
   "metadata": {},
   "source": [
    "## Experiment MLFLow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdc6830b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MLflow experiment...\n"
     ]
    }
   ],
   "source": [
    "# Création de l'étude Optuna et optimisation\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"lstm_maxlen_experiment\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"lstm_maxlen_experiment\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Comparaison des impact des types de cellules RNN utilisées : SimpleRNN, GRU et LSTM \"\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"LSTM_pretrained_embedding\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f405b051",
   "metadata": {},
   "source": [
    "## Lancement de l'experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c0c6c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 16s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 23:06:53 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/15 23:06:53 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpeobtdrra\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpeobtdrra\\model\\data\\model\\assets\n",
      "2025/09/15 23:07:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with sequence length of 75 tokens\n",
      "Embedding matrix shape: (24711, 200)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n",
      "1000/1000 [==============================] - 26s 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 00:06:53 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/16 00:06:53 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_10_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpbaxxk0ux\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpbaxxk0ux\\model\\data\\model\\assets\n",
      "2025/09/16 00:07:18 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with sequence length of 80 tokens\n",
      "Embedding matrix shape: (24711, 200)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n",
      "1000/1000 [==============================] - 19s 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 01:12:46 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/16 01:12:46 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_11_layer_call_fn, lstm_cell_11_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpvujtzpq5\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpvujtzpq5\\model\\data\\model\\assets\n",
      "2025/09/16 01:13:05 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with sequence length of 85 tokens\n",
      "Embedding matrix shape: (24711, 200)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n",
      "1000/1000 [==============================] - 19s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 02:03:47 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/16 02:03:47 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_12_layer_call_fn, lstm_cell_12_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp4kp924jp\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp4kp924jp\\model\\data\\model\\assets\n",
      "2025/09/16 02:04:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with sequence length of 90 tokens\n",
      "Embedding matrix shape: (24711, 200)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n",
      "1000/1000 [==============================] - 25s 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 03:09:26 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/16 03:09:26 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp0qlemuot\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmp0qlemuot\\model\\data\\model\\assets\n",
      "2025/09/16 03:09:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with sequence length of 95 tokens\n",
      "Embedding matrix shape: (24711, 200)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n",
      "1000/1000 [==============================] - 22s 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 04:12:22 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/16 04:12:22 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_14_layer_call_fn, lstm_cell_14_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpybz59rw1\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bassm\\AppData\\Local\\Temp\\tmpybz59rw1\\model\\data\\model\\assets\n",
      "2025/09/16 04:12:54 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "for max_len in list(range(30,100,5)):\n",
    "    \n",
    "    print(f\"Running test with sequence length of {max_len} tokens\")\n",
    "    lstm_maxlen_experiment(max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecba592",
   "metadata": {},
   "source": [
    "## Enregistrement du mailleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "406df028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 366, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 464, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1634, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1627, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\bassm\\.conda\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Successfully registered model 'lstm_maxlen_best'.\n",
      "2025/09/15 20:13:01 WARNING mlflow.tracking._model_registry.fluent: Run with id b63acc4f64914a5fb77ab6b26a974e13 has no artifacts at artifact path 'model', registering model based on models:/m-612a40cdbb1d4774b96429b02b0740f8 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run ID: b63acc4f64914a5fb77ab6b26a974e13 with metrics:\n",
      "Accuracy: 0.81365625\n",
      "F1_negatif: 0.811518159117489\n",
      "F1_positif: 0.8157463770355035\n",
      "Precision_negatif: 0.8209375199846518\n",
      "Precision_positif: 0.8066980382570433\n",
      "Recall_negatif: 0.8023125\n",
      "Recall_positif: 0.825\n",
      "ROC_AUC: 0.89485159765625\n",
      "Best run parameters:\n",
      "embedding_name: Glove_twitter_200\n",
      "epochs: 50\n",
      "latent_dim: 200\n",
      "learning_rate: 0.001\n",
      "max_len: 50\n",
      "min_count: 2\n",
      "num_words: 20000\n",
      "rnn_layer_name: LSTM\n",
      "rnn_size: 64\n",
      "stemmer: None\n",
      "Setting tag embedding_name = Glove_twitter_200 in registered model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'lstm_maxlen_best'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting tag epochs = 50 in registered model\n",
      "Setting tag latent_dim = 200 in registered model\n",
      "Setting tag learning_rate = 0.001 in registered model\n",
      "Setting tag max_len = 50 in registered model\n",
      "Setting tag min_count = 2 in registered model\n",
      "Setting tag num_words = 20000 in registered model\n",
      "Setting tag rnn_layer_name = LSTM in registered model\n",
      "Setting tag rnn_size = 64 in registered model\n",
      "Setting tag stemmer = None in registered model\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")\n",
    "experiment_id = mlflow.get_experiment_by_name(\"lstm_maxlen_experiment\").experiment_id\n",
    "runs = client.search_runs(experiment_id)\n",
    "\n",
    "# Métrique pour sélectionner le meilleur modèle\n",
    "metric_to_optimize = \"Accuracy\" # liste des métriques enregistrées dans postprocess_data.py ou sur l'UI MLflow\n",
    "best_run = max(runs, key=lambda run: run.data.metrics.get(metric_to_optimize, float('-inf')))\n",
    "print(f\"Best run ID: {best_run.info.run_id} with metrics:\")\n",
    "for key, value in best_run.data.metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Best run parameters:\")\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Enregistrement du meilleur modèle\n",
    "best_model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "registered_model_name = \"lstm_maxlen_best\"\n",
    "registered_model = mlflow.register_model(best_model_uri, registered_model_name)\n",
    "# Enregistrement des paramètres sous forme de tags dans le modèle enregistré\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"Setting tag {key} = {value} in registered model\")\n",
    "    client.set_model_version_tag(\n",
    "        name=registered_model_name,\n",
    "        version=str(registered_model.version),\n",
    "        key=str(key),\n",
    "        value=str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3d788b",
   "metadata": {},
   "source": [
    "Quand on regarde les améliorations obtenues en augmentant la longueur des séquences, il n'est pas réellement pertinent d'augmenter la longueur des séquences au delà de 50 tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c8dba",
   "metadata": {},
   "source": [
    "# Essai avec architecture Bidirectionnal-LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901638b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# GloVe Twitter (par exemple 200 dimensions)\n",
    "glove_tw200 = api.load(\"glove-twitter-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab7afc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (24711, 200)\n",
      "Words found in pretrained embeddings: 23716/24711 (95.97%)\n"
     ]
    }
   ],
   "source": [
    "## Prétraitement\n",
    "min_count = 2\n",
    "num_words = 30000\n",
    "max_len   = 50\n",
    "# Prétraitement\n",
    "X_sentence_train, tokenizer, sentences_train = preprocess_data_embedding(X_raw=X_train, \n",
    "                                                        stem_lem_func=None,\n",
    "                                                        tokenizer=None, \n",
    "                                                        stop_words=None, \n",
    "                                                        min_count=min_count,\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words, \n",
    "                                                        return_sentences=True) \n",
    "X_sentence_val = preprocess_data_embedding(X_raw=X_val, \n",
    "                                                        stem_lem_func=None,\n",
    "                                                        tokenizer=tokenizer, \n",
    "                                                        stop_words=None, \n",
    "                                                        min_count=1, # mincount = 1 car on est sur le jeu de validation\n",
    "                                                        max_len = max_len, \n",
    "                                                        num_words=num_words) \n",
    "\n",
    "\n",
    "model_vectors = glove_tw200\n",
    "latent_dim = glove_tw200.vector_size\n",
    "\n",
    "embedding_matrix, vocab_size = build_embedding_matrix(tokenizer=tokenizer,\n",
    "                                embedding_model=model_vectors, \n",
    "                                latent_dim=latent_dim\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d63fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modèle\n",
    "rnn_size = 128\n",
    "## Entrainement\n",
    "epochs = 50\n",
    "lr = 1e-3\n",
    "max_len = 50\n",
    "## Savepath des poids du modèle\n",
    "\n",
    "def rnn_layer_experiment_bi(rnn_layer_name):\n",
    "     with mlflow.start_run():\n",
    "        mlflow.log_params(params={\n",
    "            'num_words':num_words,               \n",
    "            'max_len': max_len,\n",
    "            'min_count': min_count,\n",
    "            'stemmer': 'None', \n",
    "            'latent_dim': latent_dim, \n",
    "            'rnn_size': rnn_size, \n",
    "            'epochs': epochs, \n",
    "            'learning_rate': lr,\n",
    "            'embedding_name':'Glove_twitter_200',\n",
    "            'rnn_layer_name':rnn_layer_name \n",
    "        })\n",
    "        model_savepath = \"./Models/\"+rnn_layer_name+\"_model_exp.h5\"\n",
    "        # Modèle\n",
    "\n",
    "        if rnn_layer_name=='LSTM':\n",
    "            model = build_lstm_RNN(vocab_size=vocab_size, \n",
    "                            latent_dim=latent_dim,\n",
    "                            input_length=max_len, \n",
    "                            embedding_matrix=embedding_matrix,\n",
    "                            rnn_size = rnn_size)\n",
    "        elif rnn_layer_name=='Bi-LSTM':\n",
    "            model = build_bilstm_RNN(vocab_size=vocab_size, \n",
    "                            latent_dim=latent_dim,\n",
    "                            input_length=max_len, \n",
    "                            embedding_matrix=embedding_matrix,\n",
    "                            rnn_size = rnn_size)\n",
    "\n",
    "        ## Callbacks\n",
    "        checkpoint = ModelCheckpoint(model_savepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-5)\n",
    "        callbacks_list = [checkpoint, es, lr_scheduler]\n",
    "        ## Compilation\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "        #Entrainement\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            history = model.fit(X_sentence_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_sentence_val,y_val), callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "        model.load_weights(model_savepath)\n",
    "\n",
    "                # Prédictions sur le jeu de validation\n",
    "        y_pred_proba = model.predict(X_sentence_val)\n",
    "        y_pred = (y_pred_proba>0.5)\n",
    "\n",
    "\n",
    "        output_dict = postprocess_model_output(y_val, y_pred, y_pred_proba) # voir postprocess_data.py\n",
    "\n",
    "        # Logging des métriques dans MLflow\n",
    "        mlflow.log_metrics(output_dict)\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_val, y_pred, normalize='pred')\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=ax, )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix - Validation Set\")\n",
    "        fig.savefig(\"confusion_matrix.png\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        #\n",
    "        fig2 = plot_training_history(history,show=False)\n",
    "        fig2.savefig(\"learning_path.png\")\n",
    "        plt.close(fig2)\n",
    "        mlflow.log_artifact(\"learning_path.png\")\n",
    "\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21bb2138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "2025/09/23 01:22:11 INFO mlflow.tracking.fluent: Experiment with name 'bilstm_experiment' does not exist. Creating a new experiment.\n",
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment '952162272163485199'. Detailed error Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 367, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 465, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1635, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1628, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI_env_P7_gpu\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Formation AI Engineer - OpenClassrooms\\Projets\\Projet 7 - Analyse de sentiments\\mlruns\\952162272163485199\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MLflow experiment...\n"
     ]
    }
   ],
   "source": [
    "# Création de l'étude Optuna et optimisation\n",
    "print(\"Setting up MLflow experiment...\")\n",
    "mlflow.set_experiment(\"bilstm_experiment\")\n",
    "exp_id = mlflow.get_experiment_by_name(\"bilstm_experiment\").experiment_id\n",
    "\n",
    "experiment_description = (\n",
    "    \"Essai Bidirectionnal LSTM \"\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Sentiment analysis modelling\",\n",
    "    \"model_type\": \"RNN_types-pretrained-embeddings\",\n",
    "    \"team\": \"Ph. Constant\",\n",
    "    \"project_quarter\": \"Q3-2025\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "for key, value in experiment_tags.items():\n",
    "    client.set_experiment_tag(exp_id, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a349bfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with Bi-LSTM\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 50, 200)           4942200   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 50, 256)          336896    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,295,609\n",
      "Trainable params: 353,409\n",
      "Non-trainable params: 4,942,200\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1065/2000 [==============>...............] - ETA: 4:07 - loss: 0.5006 - accuracy: 0.7533"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Running test with Bi-LSTM\")\n",
    "rnn_layer_experiment_bi(\"Bi-LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ea798",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient(tracking_uri=\"http://localhost:8080\")\n",
    "experiment_id = mlflow.get_experiment_by_name(\"bilstm_experiment\").experiment_id\n",
    "runs = client.search_runs(experiment_id)\n",
    "\n",
    "# Métrique pour sélectionner le meilleur modèle\n",
    "metric_to_optimize = \"Accuracy\" # liste des métriques enregistrées dans postprocess_data.py ou sur l'UI MLflow\n",
    "best_run = max(runs, key=lambda run: run.data.metrics.get(metric_to_optimize, float('-inf')))\n",
    "print(f\"Best run ID: {best_run.info.run_id} with metrics:\")\n",
    "for key, value in best_run.data.metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Best run parameters:\")\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Enregistrement du meilleur modèle\n",
    "best_model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "registered_model_name = \"bilstm_best\"\n",
    "registered_model = mlflow.register_model(best_model_uri, registered_model_name)\n",
    "# Enregistrement des paramètres sous forme de tags dans le modèle enregistré\n",
    "for key, value in best_run.data.params.items():\n",
    "    print(f\"Setting tag {key} = {value} in registered model\")\n",
    "    client.set_model_version_tag(\n",
    "        name=registered_model_name,\n",
    "        version=str(registered_model.version),\n",
    "        key=str(key),\n",
    "        value=str(value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env_P7_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
